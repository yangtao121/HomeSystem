
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional
from HomeSystem.workflow.task import Task
from HomeSystem.utility.arxiv.arxiv import ArxivTool, ArxivResult, ArxivData, ArxivSearchMode
from HomeSystem.workflow.paper_gather_task.llm_config import AbstractAnalysisLLM, AbstractAnalysisResult, FullPaperAnalysisLLM, FullAnalysisResult
from HomeSystem.integrations.database import DatabaseOperations, ArxivPaperModel
from loguru import logger


class PaperGatherTaskConfig:
    """è®ºæ–‡æ”¶é›†ä»»åŠ¡é…ç½®ç±»"""
    
    def __init__(self, 
                 interval_seconds: int = 3600,
                 search_query: str = "machine learning",
                 max_papers_per_search: int = 20,
                 user_requirements: str = "å¯»æ‰¾æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„æœ€æ–°ç ”ç©¶è®ºæ–‡",
                 llm_model_name: str = "ollama.Qwen3_30B",
                 abstract_analysis_model: Optional[str] = None,
                 full_paper_analysis_model: Optional[str] = None,
                 relevance_threshold: float = 0.7,
                 max_papers_in_response: int = 50,
                 max_relevant_papers_in_response: int = 10,
                 # æ·±åº¦åˆ†æç›¸å…³å‚æ•°
                 enable_deep_analysis: bool = True,
                 deep_analysis_threshold: float = 0.8,
                 deep_analysis_model: str = "deepseek.DeepSeek_V3",
                 vision_model: str = "ollama.Qwen2_5_VL_7B",
                 ocr_char_limit_for_analysis: int = 10000,
                 # ç”¨æˆ·æç¤ºè¯å‚æ•°
                 enable_user_prompt: bool = False,
                 user_prompt: Optional[str] = None,
                 # æœç´¢æ¨¡å¼ç›¸å…³å‚æ•°
                 search_mode: ArxivSearchMode = ArxivSearchMode.LATEST,
                 start_year: Optional[int] = None,
                 end_year: Optional[int] = None,
                 after_year: Optional[int] = None,
                 # è¿œç¨‹OCRç›¸å…³å‚æ•°
                 enable_remote_ocr: bool = False,
                 remote_ocr_endpoint: str = 'http://localhost:5001',
                 remote_ocr_timeout: int = 300,
                 # è§†é¢‘åˆ†æç›¸å…³å‚æ•°
                 enable_video_analysis: bool = False,
                 video_analysis_model: Optional[str] = None,
                 # ä»»åŠ¡è¿½è¸ªç›¸å…³å‚æ•°
                 task_name: Optional[str] = None,
                 task_id: Optional[str] = None,
                 custom_settings: Optional[Dict[str, Any]] = None):
        
        self.interval_seconds = interval_seconds
        self.search_query = search_query
        self.max_papers_per_search = max_papers_per_search
        self.user_requirements = user_requirements
        self.llm_model_name = llm_model_name

        if not abstract_analysis_model:
            abstract_analysis_model = llm_model_name
        self.abstract_analysis_model = abstract_analysis_model
        if not full_paper_analysis_model:
            full_paper_analysis_model = llm_model_name
        self.full_paper_analysis_model = full_paper_analysis_model
        
        self.relevance_threshold = relevance_threshold
        self.max_papers_in_response = max_papers_in_response
        self.max_relevant_papers_in_response = max_relevant_papers_in_response
        
        # æ·±åº¦åˆ†æç›¸å…³é…ç½®
        self.enable_deep_analysis = enable_deep_analysis
        self.deep_analysis_threshold = deep_analysis_threshold
        self.deep_analysis_model = deep_analysis_model
        self.vision_model = vision_model
        self.ocr_char_limit_for_analysis = ocr_char_limit_for_analysis
        # ç”¨æˆ·æç¤ºè¯é…ç½®
        self.enable_user_prompt = enable_user_prompt
        self.user_prompt = user_prompt
        # è¿œç¨‹OCRé…ç½®
        self.enable_remote_ocr = enable_remote_ocr
        self.remote_ocr_endpoint = remote_ocr_endpoint
        self.remote_ocr_timeout = remote_ocr_timeout
        # è§†é¢‘åˆ†æé…ç½®
        self.enable_video_analysis = enable_video_analysis
        if not video_analysis_model:
            video_analysis_model = llm_model_name  # é»˜è®¤ä½¿ç”¨ä¸»LLMæ¨¡å‹
        self.video_analysis_model = video_analysis_model
        # æ–°å¢æœç´¢æ¨¡å¼ç›¸å…³å±æ€§
        self.search_mode = search_mode
        self.start_year = start_year
        self.end_year = end_year
        self.after_year = after_year
        self.custom_settings = custom_settings or {}
        
        # ä»»åŠ¡è¿½è¸ªå‚æ•°
        self.task_name = task_name or "paper_gather"  # é»˜è®¤ä»»åŠ¡åç§°
        self.task_id = task_id  # å¦‚æœæœªæä¾›å°†åœ¨å®é™…æ‰§è¡Œæ—¶ç”Ÿæˆ
        
        # éªŒè¯æœç´¢æ¨¡å¼å‚æ•°
        self._validate_search_mode_params()
        
        logger.info(f"è®ºæ–‡æ”¶é›†ä»»åŠ¡é…ç½®åˆå§‹åŒ–å®Œæˆ: "
                   f"é—´éš”={interval_seconds}ç§’, "
                   f"æŸ¥è¯¢='{search_query}', "
                   f"æœç´¢æ¨¡å¼={search_mode.value}, "
                   f"æœ€å¤§è®ºæ–‡æ•°={max_papers_per_search}, "
                   f"å¯ç”¨æ·±åº¦åˆ†æ={enable_deep_analysis}, "
                   f"æ·±åº¦åˆ†æé˜ˆå€¼={deep_analysis_threshold}")
    
    def _validate_search_mode_params(self):
        """éªŒè¯æœç´¢æ¨¡å¼å‚æ•°çš„åˆæ³•æ€§"""
        if self.search_mode == ArxivSearchMode.DATE_RANGE:
            if self.start_year is None or self.end_year is None:
                raise ValueError("DATE_RANGEæœç´¢æ¨¡å¼éœ€è¦æä¾›start_yearå’Œend_yearå‚æ•°")
            if self.start_year > self.end_year:
                raise ValueError("start_yearä¸èƒ½å¤§äºend_year")
            
        elif self.search_mode == ArxivSearchMode.AFTER_YEAR:
            if self.after_year is None:
                raise ValueError("AFTER_YEARæœç´¢æ¨¡å¼éœ€è¦æä¾›after_yearå‚æ•°")
            from datetime import datetime
            current_year = datetime.now().year
            if self.after_year > current_year:
                logger.warning(f"after_year ({self.after_year}) å¤§äºå½“å‰å¹´ä»½ ({current_year})")
    
    def update_config(self, **kwargs):
        """æ›´æ–°é…ç½®å‚æ•°"""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
                logger.info(f"é…ç½®æ›´æ–°: {key} = {value}")
            else:
                logger.warning(f"æœªçŸ¥é…ç½®å‚æ•°: {key}")
    
    def get_config_dict(self) -> Dict[str, Any]:
        """è·å–é…ç½®å­—å…¸"""
        return {
            'interval_seconds': self.interval_seconds,
            'search_query': self.search_query,
            'max_papers_per_search': self.max_papers_per_search,
            'user_requirements': self.user_requirements,
            'llm_model_name': self.llm_model_name,
            'abstract_analysis_model': self.abstract_analysis_model,
            'full_paper_analysis_model': self.full_paper_analysis_model,
            'relevance_threshold': self.relevance_threshold,
            'max_papers_in_response': self.max_papers_in_response,
            'max_relevant_papers_in_response': self.max_relevant_papers_in_response,
            # æ·±åº¦åˆ†æç›¸å…³é…ç½®
            'enable_deep_analysis': self.enable_deep_analysis,
            'deep_analysis_threshold': self.deep_analysis_threshold,
            'deep_analysis_model': self.deep_analysis_model,
            'vision_model': self.vision_model,
            'ocr_char_limit_for_analysis': self.ocr_char_limit_for_analysis,
            # æœç´¢æ¨¡å¼ç›¸å…³é…ç½®
            'search_mode': self.search_mode.value,
            'start_year': self.start_year,
            'end_year': self.end_year,
            'after_year': self.after_year,
            # ä»»åŠ¡è¿½è¸ªç›¸å…³é…ç½®
            'task_name': self.task_name,
            'task_id': self.task_id,
            'custom_settings': self.custom_settings
        }


class PaperGatherTask(Task):
    """è®ºæ–‡æ”¶é›†ä»»åŠ¡ - é€šè¿‡ArXivæœç´¢è®ºæ–‡å¹¶ä½¿ç”¨LLMè¿›è¡Œåˆ†æ"""
    
    def __init__(self, config: Optional[PaperGatherTaskConfig] = None, delay_first_run: bool = True):
        """
        åˆå§‹åŒ–è®ºæ–‡æ”¶é›†ä»»åŠ¡
        
        Args:
            config: è®ºæ–‡æ”¶é›†ä»»åŠ¡é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            delay_first_run: æ˜¯å¦å»¶è¿Ÿé¦–æ¬¡è¿è¡Œï¼Œé»˜è®¤ä¸ºTrueï¼ˆç”¨äºå®šæ—¶ä»»åŠ¡ï¼‰
        """
        # ä½¿ç”¨é…ç½®æˆ–é»˜è®¤é…ç½®
        self.config = config or PaperGatherTaskConfig()
        
        super().__init__("paper_gather", self.config.interval_seconds, delay_first_run=delay_first_run)
        
        # åˆå§‹åŒ–å·¥å…·
        self.arxiv_tool = ArxivTool()
        self.llm_analyzer = AbstractAnalysisLLM(
            model_name=self.config.abstract_analysis_model 
        )
        self.full_paper_analyzer = FullPaperAnalysisLLM(
            model_name=self.config.full_paper_analysis_model
        )
        
        # åˆå§‹åŒ–æ•°æ®åº“æ“ä½œ
        self.db_ops = DatabaseOperations()
        
        logger.info(f"åˆå§‹åŒ–è®ºæ–‡æ”¶é›†ä»»åŠ¡ï¼Œé…ç½®: {self.config.get_config_dict()}")
        
    def update_config(self, **kwargs):
        """æ›´æ–°ä»»åŠ¡é…ç½®"""
        self.config.update_config(**kwargs)
        
        # å¦‚æœæ›´æ–°äº†ä»»ä½•æ¨¡å‹é…ç½®ï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ–ç›¸åº”çš„LLMåˆ†æå™¨
        model_related_keys = [
            'llm_model_name', 'abstract_analysis_model', 
            'full_paper_analysis_model'
        ]
        
        if any(key in kwargs for key in model_related_keys):
            # é‡æ–°åˆå§‹åŒ–æ‘˜è¦åˆ†æå™¨
            if 'llm_model_name' in kwargs or 'abstract_analysis_model' in kwargs:
                self.llm_analyzer = AbstractAnalysisLLM(
                    model_name=self.config.abstract_analysis_model or self.config.llm_model_name
                )
                logger.info(f"é‡æ–°åˆå§‹åŒ–æ‘˜è¦åˆ†æå™¨: {self.llm_analyzer.model_name}")
            
            # é‡æ–°åˆå§‹åŒ–å®Œæ•´è®ºæ–‡åˆ†æå™¨
            if 'llm_model_name' in kwargs or 'full_paper_analysis_model' in kwargs:
                self.full_paper_analyzer = FullPaperAnalysisLLM(
                    model_name=self.config.full_paper_analysis_model or self.config.llm_model_name
                )
                logger.info(f"é‡æ–°åˆå§‹åŒ–å®Œæ•´è®ºæ–‡åˆ†æå™¨: {self.full_paper_analyzer.model_name}")
    
    def get_config(self) -> PaperGatherTaskConfig:
        """è·å–å½“å‰é…ç½®"""
        return self.config
    
    
    async def check_paper_in_database(self, arxiv_id: str) -> Optional[ArxivPaperModel]:
        """
        æ£€æŸ¥è®ºæ–‡æ˜¯å¦å·²åœ¨æ•°æ®åº“ä¸­
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            
        Returns:
            ArxivPaperModel: å¦‚æœå­˜åœ¨è¿”å›è®ºæ–‡æ¨¡å‹ï¼Œå¦åˆ™è¿”å›None
        """
        try:
            existing_paper = self.db_ops.get_by_field(ArxivPaperModel, 'arxiv_id', arxiv_id)
            if existing_paper:
                logger.debug(f"è®ºæ–‡å·²å­˜åœ¨äºæ•°æ®åº“ä¸­: {arxiv_id}")
                return existing_paper
            else:
                logger.debug(f"è®ºæ–‡ä¸å­˜åœ¨äºæ•°æ®åº“ä¸­: {arxiv_id}")
                return None
        except Exception as e:
            logger.error(f"æ£€æŸ¥è®ºæ–‡æ•°æ®åº“çŠ¶æ€å¤±è´¥: {arxiv_id}, é”™è¯¯: {e}")
            return None
    
    def _get_required_field(self, paper: ArxivData, source_field: str, target_field: str):
        """
        è·å–å¿…éœ€å­—æ®µï¼Œå¦‚æœä¸å­˜åœ¨æˆ–ä¸ºç©ºåˆ™æŠ›å‡ºå¼‚å¸¸
        
        Args:
            paper: ArxivDataå¯¹è±¡
            source_field: æºå­—æ®µå
            target_field: ç›®æ ‡å­—æ®µåï¼ˆç”¨äºé”™è¯¯æ¶ˆæ¯ï¼‰
            
        Returns:
            å­—æ®µå€¼
            
        Raises:
            ValueError: å¦‚æœå­—æ®µä¸å­˜åœ¨ã€ä¸ºNoneæˆ–ä¸ºç©ºå­—ç¬¦ä¸²
        """
        value = getattr(paper, source_field, None)
        if value is None:
            raise ValueError(f"Required field '{target_field}' is missing (source: '{source_field}') in paper {paper.arxiv_id}")
        if isinstance(value, str) and not value.strip():
            raise ValueError(f"Required field '{target_field}' is empty (source: '{source_field}') in paper {paper.arxiv_id}")
        return value
    
    async def save_paper_to_database(self, paper: ArxivData) -> bool:
        """
        ä¿å­˜è®ºæ–‡åˆ°æ•°æ®åº“
        
        Args:
            paper: ArXivè®ºæ–‡æ•°æ®
            
        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        # æ£€æŸ¥æ·±åº¦åˆ†ææ˜¯å¦æˆåŠŸï¼ˆå¯é€‰ï¼Œå¤±è´¥ä¹Ÿå¯ä»¥ä¿å­˜åŸºç¡€ä¿¡æ¯ï¼‰
        deep_analysis_success = getattr(paper, 'deep_analysis_success', True)
        
        # æ·±åº¦åˆ†æå¤±è´¥ä¸é˜»æ­¢æ•°æ®åº“ä¿å­˜ï¼Œåªè®°å½•æ—¥å¿—
        if not deep_analysis_success:
            logger.warning(f"è®ºæ–‡æ·±åº¦åˆ†æå¤±è´¥ï¼Œä½†ä»ä¼šä¿å­˜åŸºç¡€ä¿¡æ¯åˆ°æ•°æ®åº“: {paper.arxiv_id} - {paper.title[:50]}...")
            
        try:
            # å‡†å¤‡æ·±åº¦åˆ†æå­—æ®µ
            deep_analysis_result = getattr(paper, 'deep_analysis_result', None)
            deep_analysis_completed = getattr(paper, 'deep_analysis_completed', False)
            deep_analysis_status = None
            
            # æ ¹æ®æ·±åº¦åˆ†ææƒ…å†µè®¾ç½®çŠ¶æ€
            if deep_analysis_result and deep_analysis_completed:
                deep_analysis_status = 'completed'
            elif not deep_analysis_success:
                deep_analysis_status = 'failed'
            
            # åˆ›å»ºArxivPaperModelå®ä¾‹
            paper_model = ArxivPaperModel(
                arxiv_id=paper.arxiv_id,
                title=paper.title,
                authors=getattr(paper, 'authors', ''),  # ä½¿ç”¨getattrå®‰å…¨è·å–authorså±æ€§
                abstract=paper.snippet,  # ArxivDataä¸­ä½¿ç”¨snippetä½œä¸ºabstract
                categories=paper.categories,
                published_date=getattr(paper, 'published_date', ''),  # ä½¿ç”¨published_dateè€Œä¸æ˜¯published
                pdf_url=getattr(paper, 'pdf_link', paper.link.replace("abs", "pdf") if paper.link else ''),  # ä½¿ç”¨pdf_linkå±æ€§
                processing_status='completed',  # å¤„ç†å®Œæˆåè®¾ç½®ä¸ºcompleted
                tags=[],  # åˆå§‹ä¸ºç©ºï¼Œå¯ä»¥åç»­æ·»åŠ 
                metadata={
                    'search_query': getattr(paper, 'search_query', ''),
                    'final_relevance_score': getattr(paper, 'final_relevance_score', 0.0),
                    'abstract_relevance_score': getattr(paper, 'abstract_relevance_score', 0.0),
                    'full_paper_relevance_score': getattr(paper, 'full_paper_relevance_score', 0.0)
                },
                # ä»»åŠ¡è¿½è¸ªå­—æ®µ
                task_name=self.config.task_name,
                task_id=self.config.task_id,
                # å®Œæ•´è®ºæ–‡ç›¸å…³æ€§è¯„åˆ†å­—æ®µï¼ˆä¿ç•™åŸºç¡€åˆ†æç»“æœï¼‰
                full_paper_relevance_score=getattr(paper, 'full_paper_relevance_score', None),
                full_paper_relevance_justification=getattr(paper, 'full_paper_analysis_justification', 
                                                           getattr(paper, 'abstract_analysis_justification', None)),
                # æ·±åº¦åˆ†æå­—æ®µ
                deep_analysis_result=deep_analysis_result,
                deep_analysis_status=deep_analysis_status,
                deep_analysis_created_at=datetime.now() if deep_analysis_status else None,
                deep_analysis_updated_at=datetime.now() if deep_analysis_status else None
            )
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            success = self.db_ops.create(paper_model)
            if success:
                logger.info(f"è®ºæ–‡æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“: {paper.arxiv_id} - {paper.title[:50]}...")
                return True
            else:
                logger.error(f"è®ºæ–‡ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {paper.arxiv_id}")
                return False
        except Exception as e:
            logger.error(f"ä¿å­˜è®ºæ–‡åˆ°æ•°æ®åº“æ—¶å‘ç”Ÿå¼‚å¸¸: {paper.arxiv_id}, é”™è¯¯: {e}")
            return False
        
    async def search_papers(self, query: str, num_results: int = 10) -> ArxivResult:
        """
        æ ¹æ®é…ç½®çš„æœç´¢æ¨¡å¼æœç´¢è®ºæ–‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            num_results: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            ArxivResult: æœç´¢ç»“æœ
        """
        try:
            logger.info(f"ä½¿ç”¨æœç´¢æ¨¡å¼ {self.config.search_mode.value} æœç´¢è®ºæ–‡: {query}")
            
            # æ ¹æ®é…ç½®çš„æœç´¢æ¨¡å¼é€‰æ‹©æœç´¢æ–¹æ³•
            results = self.arxiv_tool.searchPapersByMode(
                query=query,
                mode=self.config.search_mode,
                num_results=num_results,
                start_year=self.config.start_year,
                end_year=self.config.end_year,
                after_year=self.config.after_year
            )
            
            logger.info(f"æ‰¾åˆ° {results.num_results} ç¯‡è®ºæ–‡")
            return results
        except Exception as e:
            logger.error(f"æœç´¢è®ºæ–‡å¤±è´¥: {e}")
            return ArxivResult([])
    
    async def analyze_paper_relevance(self, paper: ArxivData) -> AbstractAnalysisResult:
        """
        åˆ†æè®ºæ–‡ç›¸å…³æ€§
        
        Args:
            paper: è®ºæ–‡æ•°æ®
            
        Returns:
            AbstractAnalysisResult: åˆ†æç»“æœ
        """
        try:
            logger.debug(f"åˆ†æè®ºæ–‡ç›¸å…³æ€§: {paper.title[:50]}...")
            result = self.llm_analyzer.analyze_abstract(
                abstract=paper.snippet,
                user_requirements=self.config.user_requirements
            )

            logger.debug(f"abstract justification: {result.justification}")
            return result
        except Exception as e:
            logger.error(f"è®ºæ–‡åˆ†æå¤±è´¥: {e}")
            return AbstractAnalysisResult(
                is_relevant=False,
                relevance_score=0.0,
                justification=f"åˆ†æé”™è¯¯: {str(e)}"
            )
    
    async def analyze_full_paper(self, paper: ArxivData) -> Optional[FullAnalysisResult]:
        """
        åˆ†æå®Œæ•´è®ºæ–‡çš„ç›¸å…³æ€§
        
        Args:
            paper: è®ºæ–‡æ•°æ®
            
        Returns:
            FullAnalysisResult: å®Œæ•´è®ºæ–‡åˆ†æç»“æœï¼Œå¦‚æœåˆ†æå¤±è´¥è¿”å›None
        """
        try:
            logger.info(f"å¼€å§‹å®Œæ•´è®ºæ–‡åˆ†æ: {paper.title[:50]}...")
            
            # å‡†å¤‡ç»Ÿä¸€çš„è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
            import os
            from pathlib import Path
            project_root = Path(__file__).parent.parent.parent.parent  # å›åˆ°é¡¹ç›®æ ¹ç›®å½•
            paper_folder = project_root / "data" / "paper_analyze" / paper.arxiv_id
            paper_folder.mkdir(parents=True, exist_ok=True)
            paper_folder_str = str(paper_folder)
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰OCRç»“æœ
            ocr_result = getattr(paper, 'ocr_result', None)
            
            if not ocr_result or len(ocr_result.strip()) < 500:
                # è®¾ç½®è¿œç¨‹OCRç¯å¢ƒå˜é‡ï¼ˆåœ¨PDFä¸‹è½½ä¹‹å‰ï¼Œç¡®ä¿å…ƒæ•°æ®æå–ä¹Ÿä½¿ç”¨æ­£ç¡®çš„OCRï¼‰
                if self.config.enable_remote_ocr:
                    import os
                    os.environ['REMOTE_OCR_ENDPOINT'] = self.config.remote_ocr_endpoint
                    os.environ['REMOTE_OCR_TIMEOUT'] = str(self.config.remote_ocr_timeout)
                    logger.info(f"ğŸŒ ä½¿ç”¨è¿œç¨‹OCRæœåŠ¡: {self.config.remote_ocr_endpoint} (è¶…æ—¶: {self.config.remote_ocr_timeout}ç§’)")
                else:
                    logger.debug("ğŸ” ä½¿ç”¨æœ¬åœ°PaddleOCRå¤„ç†")
                
                # ä¸‹è½½PDFï¼ˆè¿™å¯èƒ½ä¼šè§¦å‘å…ƒæ•°æ®æå–ï¼‰
                logger.debug("ä¸‹è½½PDFä¸­...")
                paper.downloadPdf()
                
                # æ‰§è¡ŒOCRå¹¶ä¿å­˜åˆ°æ ‡å‡†è·¯å¾„
                logger.debug("æ‰§è¡ŒOCRè¯†åˆ«...")
                ocr_result, status_info = paper.performOCR(
                    use_paddleocr=True,
                    use_remote_ocr=self.config.enable_remote_ocr,
                    auto_save=True,
                    save_path=paper_folder_str,
                    max_pages=25
                )
                
                # ç¡®ä¿OCRç»“æœä¿å­˜åˆ°paperå¯¹è±¡
                paper.ocr_result = ocr_result
                paper.ocr_status_info = status_info
                
                if not ocr_result or len(ocr_result.strip()) < 500:
                    logger.warning(f"OCRç»“æœè¿‡çŸ­æˆ–ä¸ºç©ºï¼Œè·³è¿‡å®Œæ•´åˆ†æ: {len(ocr_result) if ocr_result else 0} å­—ç¬¦")
                    return None
                
                logger.info(f"PaddleOCRæˆåŠŸï¼Œæå–äº† {status_info['char_count']} å­—ç¬¦ï¼Œå¤„ç†äº† {status_info['processed_pages']}/{status_info['total_pages']} é¡µ")
                logger.info(f"OCRç»“æœå·²ä¿å­˜åˆ°: {paper_folder_str}/{paper.arxiv_id}_paddleocr.md")
                if status_info['is_oversized']:
                    logger.info("æ£€æµ‹åˆ°è¶…é•¿æ–‡æ¡£ï¼Œå¯èƒ½æ˜¯æ¯•ä¸šè®ºæ–‡æˆ–ä¹¦ç±")
            else:
                logger.debug(f"ä½¿ç”¨ç°æœ‰OCRç»“æœè¿›è¡Œå®Œæ•´è®ºæ–‡åˆ†æ: {len(ocr_result)} å­—ç¬¦")
            
            # é™åˆ¶ç”¨äºLLMåˆ†æçš„å­—ç¬¦æ•°ï¼ˆé»˜è®¤10000å­—ç¬¦ï¼‰
            analysis_char_limit = getattr(self.config, 'ocr_char_limit_for_analysis', 10000)
            limited_ocr_result = ocr_result[:analysis_char_limit] if len(ocr_result) > analysis_char_limit else ocr_result
            
            if len(ocr_result) > analysis_char_limit:
                logger.info(f"OCRç»“æœè¿‡é•¿({len(ocr_result)}å­—ç¬¦)ï¼Œé™åˆ¶ä¸º{analysis_char_limit}å­—ç¬¦ç”¨äºç›¸å…³æ€§åˆ†æ")
            
            # ä½¿ç”¨FullPaperAnalysisLLMè¿›è¡Œåˆ†æ
            logger.debug("å¼€å§‹LLMåˆ†æå®Œæ•´è®ºæ–‡...")
            full_analysis = self.full_paper_analyzer.analyze_full_paper(
                paper_content=limited_ocr_result,
                user_requirements=self.config.user_requirements
            )

            logger.debug(f"å®Œæ•´è®ºæ–‡åˆ†æï¼Œjustification: {full_analysis.justification}")
            
            logger.info(f"å®Œæ•´è®ºæ–‡åˆ†æå®Œæˆ (è¯„åˆ†: {full_analysis.relevance_score:.2f}): {paper.title[:50]}...")
            return full_analysis
            
        except Exception as e:
            logger.error(f"å®Œæ•´è®ºæ–‡åˆ†æå¤±è´¥: {e}")
            return None
    
    async def perform_deep_analysis(self, paper: ArxivData, paper_folder_str: str) -> bool:
        """
        æ‰§è¡Œæ·±åº¦è®ºæ–‡åˆ†æï¼Œæ›¿ä»£åŸæ¥çš„ summarize_paper åŠŸèƒ½
        
        Args:
            paper: è®ºæ–‡æ•°æ®å¯¹è±¡
            paper_folder_str: è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            bool: æ·±åº¦åˆ†ææ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ¤– å¼€å§‹æ·±åº¦è®ºæ–‡åˆ†æ: {paper.title[:50]}...")
            
            # å¯¼å…¥ç»Ÿä¸€çš„åˆ†ææœåŠ¡
            from HomeSystem.integrations.paper_analysis import PaperAnalysisService
            
            # åˆ›å»ºåˆ†ææœåŠ¡å®ä¾‹
            analysis_config = {
                'analysis_model': self.config.deep_analysis_model,
                'vision_model': self.config.vision_model,
                'enable_user_prompt': self.config.enable_user_prompt,
                'user_prompt': self.config.user_prompt,
                'timeout': 600
            }
            
            # è®°å½•ç”¨æˆ·æç¤ºè¯ä½¿ç”¨æƒ…å†µ
            if self.config.enable_user_prompt and self.config.user_prompt:
                logger.info(f"ğŸ“ ä½¿ç”¨ç”¨æˆ·æç¤ºè¯è¿›è¡Œæ·±åº¦åˆ†æ")
                logger.debug(f"ç”¨æˆ·æç¤ºè¯: {self.config.user_prompt[:100]}...")
            
            analysis_service = PaperAnalysisService(default_config=analysis_config)
            
            # å‡†å¤‡è®ºæ–‡æ•°æ®ï¼ˆç”¨äºPDFä¸‹è½½ï¼Œå¦‚æœéœ€è¦çš„è¯ï¼‰
            paper_data = {
                'title': paper.title,
                'link': f"https://arxiv.org/abs/{paper.arxiv_id}",
                'snippet': getattr(paper, 'abstract', ''),
                'categories': getattr(paper, 'categories', ''),
                'arxiv_id': paper.arxiv_id
            }
            
            # æ‰§è¡Œæ·±åº¦åˆ†æ
            result = analysis_service.perform_deep_analysis(
                arxiv_id=paper.arxiv_id,
                paper_folder_path=paper_folder_str,
                config=analysis_config,
                paper_data=paper_data
            )
            
            if result['success']:
                # æ·»åŠ å‘è¡¨æ—¶é—´åˆ°åˆ†æç»“æœ
                publication_date = getattr(paper, 'published_date', 'æœªçŸ¥')
                final_content = analysis_service.add_analysis_footer(
                    result['analysis_result'], 
                    publication_date=publication_date
                )
                
                # é‡æ–°ä¿å­˜å¸¦é¡µè„šçš„å†…å®¹
                with open(result['analysis_file_path'], 'w', encoding='utf-8') as f:
                    f.write(final_content)
                
                # å°†æ·±åº¦åˆ†æç»“æœä¿å­˜åˆ°paperå¯¹è±¡ä¸­
                paper.deep_analysis_result = final_content
                paper.deep_analysis_completed = True
                paper.deep_analysis_file_path = result['analysis_file_path']
                
                logger.info(f"æ·±åº¦åˆ†æå®Œæˆ: {paper.arxiv_id}, ä¿å­˜äº† {len(final_content)} å­—ç¬¦")
                logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {result['analysis_file_path']}")
                
                return True
            else:
                logger.error(f"æ·±åº¦åˆ†æå¤±è´¥ {paper.arxiv_id}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æ·±åº¦åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ {paper.arxiv_id}: {e}")
            return False
    
    
    async def process_papers(self, papers: ArxivResult) -> List[ArxivData]:
        """
        å¤„ç†è®ºæ–‡æ•°æ®ï¼ŒåŒ…æ‹¬æ‘˜è¦ç›¸å…³æ€§åˆ†æå’Œå®Œæ•´è®ºæ–‡åˆ†æ
        
        Args:
            papers: æœç´¢åˆ°çš„è®ºæ–‡ç»“æœ
            
        Returns:
            List[ArxivData]: å¤„ç†åçš„è®ºæ–‡å¯¹è±¡åˆ—è¡¨
        """
        processed_papers = []
        
        for paper in papers:
            logger.info(f"å¼€å§‹å¤„ç†è®ºæ–‡: {paper.arxiv_id} - {paper.title[:50]}...")
            
            # åˆå§‹åŒ–è®ºæ–‡å¤„ç†æ ‡è®°
            setattr(paper, 'saved_to_database', False)
            setattr(paper, 'full_paper_analyzed', False)
            setattr(paper, 'deep_analysis_completed', False)  # æ·±åº¦åˆ†ææ˜¯å¦å®Œæˆ
            setattr(paper, 'deep_analysis_success', True)  # é»˜è®¤æ·±åº¦åˆ†ææˆåŠŸï¼ˆå¦‚æœä¸æ‰§è¡Œæ·±åº¦åˆ†æï¼‰
            
            # ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥è®ºæ–‡æ˜¯å¦å·²åœ¨æ•°æ®åº“ä¸­
            existing_paper = await self.check_paper_in_database(paper.arxiv_id)
            
            if existing_paper:
                logger.info(f"è®ºæ–‡å·²åœ¨æ•°æ®åº“ä¸­ï¼Œè·³è¿‡å¤„ç†: {paper.arxiv_id}")
                # ä»æ•°æ®åº“ä¸­çš„æ•°æ®åˆ›å»ºArxivDataå¯¹è±¡ï¼Œä¿æŒä¸€è‡´æ€§
                paper.final_is_relevant = existing_paper.processing_status == 'completed'
                paper.final_relevance_score = existing_paper.metadata.get('final_relevance_score', 0.0) if existing_paper.metadata else 0.0
                setattr(paper, 'saved_to_database', True)  # å·²åœ¨æ•°æ®åº“ä¸­çš„è®ºæ–‡æ ‡è®°ä¸ºå·²ä¿å­˜
                processed_papers.append(paper)
                continue
            
            # ç¬¬äºŒæ­¥ï¼šå¦‚æœè®ºæ–‡ä¸åœ¨æ•°æ®åº“ä¸­ï¼Œè¿›è¡Œæ‘˜è¦ç›¸å…³æ€§åˆ†æ
            logger.debug(f"è®ºæ–‡ä¸åœ¨æ•°æ®åº“ä¸­ï¼Œå¼€å§‹åˆ†æ: {paper.arxiv_id}")
            abstract_analysis = await self.analyze_paper_relevance(paper)
            
            # å°†åˆ†æç»“æœç›´æ¥èµ‹å€¼ç»™ArxivDataå¯¹è±¡
            paper.abstract_is_relevant = abstract_analysis.is_relevant
            paper.abstract_relevance_score = abstract_analysis.relevance_score
            paper.abstract_analysis_justification = abstract_analysis.justification
            paper.final_is_relevant = abstract_analysis.is_relevant
            paper.final_relevance_score = abstract_analysis.relevance_score
            
            # ç¬¬ä¸‰æ­¥ï¼šå¦‚æœæ‘˜è¦ç›¸å…³æ€§è¶³å¤Ÿé«˜ï¼Œè¿›è¡Œå®Œæ•´è®ºæ–‡åˆ†æ
            if abstract_analysis.is_relevant and abstract_analysis.relevance_score >= self.config.relevance_threshold:
                logger.info(f"æ‘˜è¦ç›¸å…³æ€§é«˜ ({abstract_analysis.relevance_score:.2f})ï¼Œå¼€å§‹å®Œæ•´è®ºæ–‡åˆ†æ: {paper.title[:50]}...")
                
                # æ‰§è¡Œå®Œæ•´è®ºæ–‡åˆ†æï¼ˆåŒ…å«PDFä¸‹è½½å’ŒOCRï¼‰
                full_analysis = await self.analyze_full_paper(paper)
                
                if full_analysis:
                    paper.full_paper_analyzed = True
                    paper.full_paper_is_relevant = full_analysis.is_relevant
                    paper.full_paper_relevance_score = full_analysis.relevance_score
                    paper.full_paper_analysis_justification = full_analysis.justification
                    paper.final_is_relevant = full_analysis.is_relevant
                    paper.final_relevance_score = full_analysis.relevance_score
                    
                    if full_analysis.is_relevant:
                        logger.info(f"å®Œæ•´è®ºæ–‡åˆ†æç¡®è®¤ç›¸å…³ (è¯„åˆ†: {full_analysis.relevance_score:.2f}): {paper.title}")
                        
                        # ç¬¬å››æ­¥ï¼šå¦‚æœå¯ç”¨äº†æ·±åº¦åˆ†æä¸”ç›¸å…³æ€§è¯„åˆ†è¶³å¤Ÿé«˜ï¼Œåˆ™è¿›è¡Œæ·±åº¦åˆ†æ
                        # æ­¤æ—¶OCRç»“æœå·²ç»åœ¨analyze_full_paperä¸­å‡†å¤‡å¥½ï¼Œä¸ä¼šé‡å¤æ‰§è¡Œ
                        deep_analysis_enabled = getattr(self.config, 'enable_deep_analysis', True)
                        deep_analysis_threshold = getattr(self.config, 'deep_analysis_threshold', 0.8)
                        
                        if (deep_analysis_enabled and 
                            full_analysis.relevance_score >= deep_analysis_threshold):
                            
                            logger.info(f"ç›¸å…³æ€§è¯„åˆ†è¶³å¤Ÿé«˜ ({full_analysis.relevance_score:.2f})ï¼Œå¼€å§‹æ·±åº¦åˆ†æ: {paper.title[:50]}...")
                            
                            # é‡æ–°è®¡ç®—è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
                            from pathlib import Path
                            # For Docker: /app/HomeSystem/workflow/paper_gather_task/paper_gather_task.py -> /app (4 parents) -> /app/data
                            # For local: workflow/paper_gather_task/paper_gather_task.py -> project root (5 parents) -> project_root/data
                            project_root = Path(__file__).parent.parent.parent.parent
                            if not (project_root / "data").exists():
                                project_root = Path(__file__).parent.parent.parent.parent.parent
                            paper_folder = project_root / "data" / "paper_analyze" / paper.arxiv_id
                            paper_folder_str = str(paper_folder)
                            
                            deep_analysis_success = await self.perform_deep_analysis(paper, paper_folder_str)
                            setattr(paper, 'deep_analysis_success', deep_analysis_success)
                            
                            if deep_analysis_success:
                                logger.info(f"æ·±åº¦åˆ†æå®Œæˆ: {paper.title[:50]}...")
                            else:
                                logger.warning(f"æ·±åº¦åˆ†æå¤±è´¥: {paper.title[:50]}...")
                        else:
                            if not deep_analysis_enabled:
                                logger.debug(f"æ·±åº¦åˆ†æåŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡æ·±åº¦åˆ†æ: {paper.title[:50]}...")
                            elif full_analysis.relevance_score < deep_analysis_threshold:
                                logger.debug(f"ç›¸å…³æ€§è¯„åˆ†ä¸è¶³æ·±åº¦åˆ†æé˜ˆå€¼ ({full_analysis.relevance_score:.2f} < {deep_analysis_threshold})ï¼Œè·³è¿‡æ·±åº¦åˆ†æ: {paper.title[:50]}...")
                    else:
                        logger.info(f"å®Œæ•´è®ºæ–‡åˆ†æåˆ¤å®šä¸ç›¸å…³ (è¯„åˆ†: {full_analysis.relevance_score:.2f}): {paper.title}")
                else:
                    logger.warning(f"å®Œæ•´è®ºæ–‡åˆ†æå¤±è´¥ï¼Œç”±äºOCRä¸å¯ç”¨ï¼Œæ ‡è®°ä¸ºä¸ç›¸å…³: {paper.title[:50]}...")
                    # OCRå¤±è´¥æ—¶ï¼Œé‡‡ç”¨ä¿å®ˆç­–ç•¥ï¼šæ ‡è®°ä¸ºä¸ç›¸å…³ï¼Œé¿å…é”™è¯¯ä¿å­˜
                    paper.final_is_relevant = False
                    paper.final_relevance_score = 0.0
                    paper.full_paper_analysis_justification = f"Full paper analysis failed due to OCR service unavailability (abstract score was {abstract_analysis.relevance_score:.2f})"
                    logger.info(f"OCRå¤±è´¥ä¿æŠ¤ï¼šè®ºæ–‡ {paper.arxiv_id} è¢«æ ‡è®°ä¸ºä¸ç›¸å…³ï¼Œå°†ä¸ä¼šä¿å­˜åˆ°æ•°æ®åº“")
            else:
                logger.debug(f"æ‘˜è¦ç›¸å…³æ€§ä½ ({abstract_analysis.relevance_score:.2f})ï¼Œè·³è¿‡å®Œæ•´åˆ†æ: {paper.title[:50]}...")
                # å½“è·³è¿‡å®Œæ•´åˆ†ææ—¶ï¼Œä¹Ÿè®¾ç½®é»˜è®¤çš„justificationå­—æ®µ
                paper.full_paper_analysis_justification = getattr(paper, 'abstract_analysis_justification', 
                                                                 f"Full paper analysis skipped due to low abstract relevance score ({abstract_analysis.relevance_score:.2f})")
            
            # ç¬¬å…­æ­¥ï¼šå¦‚æœè®ºæ–‡ç¬¦åˆè¦æ±‚ï¼ˆç›¸å…³æ€§è¾¾æ ‡ï¼‰ï¼Œä¿å­˜åˆ°æ•°æ®åº“
            if paper.final_is_relevant and paper.final_relevance_score >= self.config.relevance_threshold:
                logger.info(f"è®ºæ–‡ç¬¦åˆè¦æ±‚ï¼Œä¿å­˜åˆ°æ•°æ®åº“: {paper.arxiv_id} (è¯„åˆ†: {paper.final_relevance_score:.2f})")
                save_success = await self.save_paper_to_database(paper)
                if save_success:
                    # è®¾ç½®ä¿å­˜æ ‡è®°ï¼Œç”¨äºåç»­ç»Ÿè®¡
                    setattr(paper, 'saved_to_database', True)
                else:
                    logger.warning(f"è®ºæ–‡ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†: {paper.arxiv_id}")
                    setattr(paper, 'saved_to_database', False)
            else:
                logger.debug(f"è®ºæ–‡ä¸ç¬¦åˆè¦æ±‚ï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“: {paper.arxiv_id} (ç›¸å…³æ€§: {paper.final_is_relevant}, è¯„åˆ†: {paper.final_relevance_score:.2f})")
                setattr(paper, 'saved_to_database', False)
            
            # ç¬¬ä¸ƒæ­¥ï¼šå¤„ç†å®Œæ¯•é‡Šæ”¾å†…å­˜
            # æ¸…ç†PDFå’ŒOCRç»“æœï¼Œé‡Šæ”¾å†…å­˜
            if hasattr(paper, 'clearPdf'):
                paper.clearPdf()
            if hasattr(paper, 'clearOcrResult'):
                paper.clearOcrResult()
                
            processed_papers.append(paper)
        
        return processed_papers
        
    async def run(self) -> Dict[str, Any]:
        """æ‰§è¡Œè®ºæ–‡æ”¶é›†é€»è¾‘"""
        logger.info("å¼€å§‹æ‰§è¡Œè®ºæ–‡æ”¶é›†ä»»åŠ¡")
        
        all_papers = []
        total_relevant_papers = 0
        total_saved_papers = 0
        
        try:
            # å¤„ç†æœç´¢æŸ¥è¯¢
            logger.info(f"å¤„ç†æœç´¢æŸ¥è¯¢: {self.config.search_query}")
            
            # æœç´¢è®ºæ–‡
            search_results = await self.search_papers(
                self.config.search_query, 
                num_results=self.config.max_papers_per_search
            )
            
            if search_results.num_results == 0:
                logger.warning(f"æŸ¥è¯¢ '{self.config.search_query}' æœªæ‰¾åˆ°è®ºæ–‡")
                return {
                    "message": "æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡",
                    "total_papers": 0,
                    "relevant_papers": 0,
                    "saved_papers": 0,
                    "search_query": self.config.search_query,
                    "user_requirements": self.config.user_requirements,
                    "config": self.config.get_config_dict()
                }
            
            # å¤„ç†å’Œåˆ†æè®ºæ–‡
            processed_papers = await self.process_papers(search_results)
            
            # ç»Ÿè®¡ç›¸å…³è®ºæ–‡ï¼ˆæ ¹æ®é…ç½®çš„é˜ˆå€¼è¿‡æ»¤ï¼Œä½¿ç”¨æœ€ç»ˆåˆ¤æ–­ç»“æœï¼‰
            relevant_papers = [p for p in processed_papers 
                             if p.final_is_relevant and p.final_relevance_score >= self.config.relevance_threshold]
            total_relevant_papers = len(relevant_papers)
            
            # ç»Ÿè®¡ä¿å­˜åˆ°æ•°æ®åº“çš„è®ºæ–‡æ•°é‡
            saved_papers = [p for p in processed_papers 
                          if hasattr(p, 'saved_to_database') and getattr(p, 'saved_to_database', False)]
            total_saved_papers = len(saved_papers)
            
            # æ·»åŠ æŸ¥è¯¢æ ‡è¯†
            for paper in processed_papers:
                paper.search_query = self.config.search_query
            
            all_papers = processed_papers
            
            logger.info(f"æŸ¥è¯¢ '{self.config.search_query}' å®Œæˆ: æ‰¾åˆ° {len(processed_papers)} ç¯‡è®ºæ–‡ï¼Œ"
                       f"å…¶ä¸­ {len(relevant_papers)} ç¯‡ç›¸å…³ï¼ˆé˜ˆå€¼: {self.config.relevance_threshold}ï¼‰ï¼Œ"
                       f"ä¿å­˜äº† {total_saved_papers} ç¯‡åˆ°æ•°æ®åº“")
            
            # æŒ‰æœ€ç»ˆç›¸å…³æ€§è¯„åˆ†æ’åº
            all_papers.sort(key=lambda x: x.final_relevance_score, reverse=True)
            
            logger.info(f"è®ºæ–‡æ”¶é›†ä»»åŠ¡å®Œæˆ: æ€»å…±å¤„ç† {len(all_papers)} ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ {total_relevant_papers} ç¯‡ç›¸å…³ï¼Œ{total_saved_papers} ç¯‡å·²ä¿å­˜")
            
            return {
                "message": "è®ºæ–‡æ”¶é›†ä»»åŠ¡æ‰§è¡Œå®Œæˆ",
                "total_papers": len(all_papers),
                "relevant_papers": total_relevant_papers,
                "saved_papers": total_saved_papers,
                "analyzed_papers": len([p for p in processed_papers if hasattr(p, 'full_paper_analyzed') and p.full_paper_analyzed]),
                "search_query": self.config.search_query,
                "user_requirements": self.config.user_requirements,
                "config": self.config.get_config_dict(),
                "papers": all_papers[:self.config.max_papers_in_response],  # è¿”å›é…ç½®æ•°é‡çš„ArxivDataå¯¹è±¡
                "top_relevant_papers": relevant_papers[:self.config.max_relevant_papers_in_response]  # è¿”å›é…ç½®æ•°é‡çš„ç›¸å…³ArxivDataå¯¹è±¡
            }
            
        except Exception as e:
            logger.error(f"è®ºæ–‡æ”¶é›†ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "message": f"è®ºæ–‡æ”¶é›†ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}",
                "total_papers": 0,
                "relevant_papers": 0,
                "saved_papers": 0,
                "error": str(e)
            }
