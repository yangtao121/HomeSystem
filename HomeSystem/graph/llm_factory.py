"""
ç®€æ´çš„LLMå·¥å‚ - ä»YAMLé…ç½®æ–‡ä»¶è¯»å–å¹¶åˆ›å»ºlanggraphç›´æ¥å¯ç”¨çš„å®ä¾‹
"""

import os
import yaml
from typing import Optional, Dict, List
from pathlib import Path
from loguru import logger
from dotenv import load_dotenv

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.embeddings import Embeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_ollama import ChatOllama, OllamaEmbeddings


class LLMFactory:
    """LLMå·¥å‚ - ä»YAMLé…ç½®è¯»å–å¯ç”¨æ¨¡å‹å¹¶åˆ›å»ºå®ä¾‹"""
    
    def __init__(self, config_path: Optional[str] = None):
        load_dotenv()
        
        if config_path is None:
            config_path = Path(__file__).parent / "llm_providers.yaml"
        
        self.config = self._load_config(config_path)
        self.available_llm_models = self._detect_available_llm_models()
        self.available_embedding_models = self._detect_available_embedding_models()
        
        logger.info(f"æ£€æµ‹åˆ° {len(self.available_llm_models)} ä¸ªLLMæ¨¡å‹, {len(self.available_embedding_models)} ä¸ªEmbeddingæ¨¡å‹")
    
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _is_provider_available(self, provider_config: Dict) -> bool:
        """æ£€æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨"""
        api_key_env = provider_config.get('api_key_env')
        
        # Ollamaä¸éœ€è¦API Key
        if not api_key_env:
            return True
        
        api_key = os.getenv(api_key_env)
        return api_key and not api_key.startswith('your_')
    
    def _detect_available_llm_models(self) -> Dict[str, Dict]:
        """æ£€æµ‹å¯ç”¨çš„LLMæ¨¡å‹"""
        available = {}
        
        for provider_key, provider_config in self.config.get('providers', {}).items():
            if self._is_provider_available(provider_config):
                for model in provider_config.get('models', []):
                    # ä½¿ç”¨æ–°çš„keyæ ¼å¼ï¼ˆprovider.modelï¼‰
                    model_key = model.get('key', f"{provider_key}.{model['name'].replace(' ', '_').replace('-', '_').replace(':', '_').replace('/', '_')}")
                    available[model_key] = {
                        'provider': provider_key,
                        'model_name': model['name'],
                        'display_name': model['display_name'],
                        'type': provider_config['type'],
                        'api_key_env': provider_config.get('api_key_env'),
                        'base_url_env': provider_config.get('base_url_env'),
                        'base_url': provider_config.get('base_url'),
                        'description': model.get('description', ''),
                        'max_tokens': model.get('max_tokens'),
                        'supports_functions': model.get('supports_functions', False)
                    }
        
        return available
    
    def _detect_available_embedding_models(self) -> Dict[str, Dict]:
        """æ£€æµ‹å¯ç”¨çš„Embeddingæ¨¡å‹"""
        available = {}
        
        for provider_key, provider_config in self.config.get('embedding_providers', {}).items():
            if self._is_provider_available(provider_config):
                for model in provider_config.get('models', []):
                    # ä½¿ç”¨æ–°çš„keyæ ¼å¼ï¼ˆprovider.modelï¼‰
                    model_key = model.get('key', f"{provider_key}.{model['name'].replace(' ', '_').replace('-', '_').replace(':', '_').replace('/', '_')}")
                    available[model_key] = {
                        'provider': provider_key,
                        'model_name': model['name'],
                        'display_name': model['display_name'],
                        'type': provider_config['type'],
                        'api_key_env': provider_config.get('api_key_env'),
                        'base_url_env': provider_config.get('base_url_env'),
                        'base_url': provider_config.get('base_url'),
                        'description': model.get('description', ''),
                        'dimensions': model.get('dimensions'),
                        'max_input_length': model.get('max_input_length')
                    }
        
        return available
    
    def get_available_llm_models(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨LLMæ¨¡å‹åˆ—è¡¨"""
        return list(self.available_llm_models.keys())
    
    def get_available_embedding_models(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨Embeddingæ¨¡å‹åˆ—è¡¨"""
        return list(self.available_embedding_models.keys())
    
    def create_llm(self, model_name: Optional[str] = None, **kwargs) -> BaseChatModel:
        """
        åˆ›å»ºLLMå®ä¾‹ï¼Œç›´æ¥ç”¨äºlanggraph
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            **kwargs: ä¼ é€’ç»™æ¨¡å‹çš„å‚æ•°
            
        Returns:
            BaseChatModel: å¯ç›´æ¥ç”¨äºlanggraphçš„LLMå®ä¾‹
        """
        # ä½¿ç”¨é»˜è®¤æ¨¡å‹
        if model_name is None:
            default_config = self.config.get('defaults', {}).get('llm', {})
            model_name = default_config.get('model_key', 'deepseek.DeepSeek_V3')
        
        if model_name not in self.available_llm_models:
            available = ', '.join(self.available_llm_models.keys())
            raise ValueError(f"æ¨¡å‹ '{model_name}' ä¸å¯ç”¨ã€‚å¯ç”¨æ¨¡å‹: {available}")
        
        config = self.available_llm_models[model_name]
        logger.info(f"åˆ›å»ºLLM: {model_name} ({config['display_name']})")
        
        # è®¾ç½®é»˜è®¤å‚æ•°
        defaults = self.config.get('defaults', {}).get('llm', {})
        params = {
            'temperature': kwargs.get('temperature', defaults.get('temperature', 0.7)),
            'max_tokens': kwargs.get('max_tokens', config.get('max_tokens', defaults.get('max_tokens', 4000))),
            **{k: v for k, v in kwargs.items() if k not in ['temperature', 'max_tokens']}
        }
        
        if config['type'] == 'ollama':
            base_url = os.getenv(config['base_url_env'], config['base_url'])
            return ChatOllama(
                model=config['model_name'],
                base_url=base_url,
                num_predict=params.pop('max_tokens'),
                **params
            )
        else:  # openai_compatible
            api_key = os.getenv(config['api_key_env'])
            base_url = os.getenv(config['base_url_env'], config['base_url'])
            return ChatOpenAI(
                model=config['model_name'],
                api_key=api_key,
                base_url=base_url,
                **params
            )
    
    def create_embedding(self, model_name: Optional[str] = None, **kwargs) -> Embeddings:
        """
        åˆ›å»ºEmbeddingå®ä¾‹
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            **kwargs: ä¼ é€’ç»™æ¨¡å‹çš„å‚æ•°
            
        Returns:
            Embeddings: Embeddingå®ä¾‹
        """
        # ä½¿ç”¨é»˜è®¤æ¨¡å‹
        if model_name is None:
            default_config = self.config.get('defaults', {}).get('embedding', {})
            model_name = default_config.get('model_key', 'ollama.BGE_M3')
        
        if model_name not in self.available_embedding_models:
            available = ', '.join(self.available_embedding_models.keys())
            raise ValueError(f"Embeddingæ¨¡å‹ '{model_name}' ä¸å¯ç”¨ã€‚å¯ç”¨æ¨¡å‹: {available}")
        
        config = self.available_embedding_models[model_name]
        logger.info(f"åˆ›å»ºEmbedding: {model_name} ({config['display_name']})")
        
        if config['type'] == 'ollama_embedding':
            base_url = os.getenv(config['base_url_env'], config['base_url'])
            return OllamaEmbeddings(
                model=config['model_name'],
                base_url=base_url,
                **kwargs
            )
        elif config['type'] == 'openai_embedding':
            api_key = os.getenv(config['api_key_env'])
            base_url = os.getenv(config['base_url_env'], config['base_url'])
            return OpenAIEmbeddings(
                model=config['model_name'],
                api_key=api_key,
                base_url=base_url,
                **kwargs
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„embeddingç±»å‹: {config['type']}")
    
    def list_models(self) -> None:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        logger.info("=" * 80)
        logger.info("å¯ç”¨æ¨¡å‹åˆ—è¡¨")
        logger.info("=" * 80)
        
        logger.info("\nğŸ“ LLMæ¨¡å‹:")
        logger.info("-" * 60)
        for model_name, config in self.available_llm_models.items():
            logger.info(f"âœ… {model_name:35} | {config['display_name']}")
        
        logger.info("\nğŸ” Embeddingæ¨¡å‹:")
        logger.info("-" * 60)
        for model_name, config in self.available_embedding_models.items():
            dims = f"({config['dimensions']}ç»´)" if config.get('dimensions') else ""
            logger.info(f"âœ… {model_name:35} | {config['display_name']} {dims}")
        
        logger.info("=" * 80)
        logger.info(f"æ€»è®¡: {len(self.available_llm_models)} ä¸ªLLMæ¨¡å‹, {len(self.available_embedding_models)} ä¸ªEmbeddingæ¨¡å‹")


# å…¨å±€å·¥å‚å®ä¾‹
llm_factory = LLMFactory()


def get_llm(model_name: Optional[str] = None, **kwargs) -> BaseChatModel:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºLLMå®ä¾‹"""
    return llm_factory.create_llm(model_name, **kwargs)


def get_embedding(model_name: Optional[str] = None, **kwargs) -> Embeddings:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºEmbeddingå®ä¾‹"""
    return llm_factory.create_embedding(model_name, **kwargs)


def list_available_llm_models() -> List[str]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–å¯ç”¨LLMæ¨¡å‹åˆ—è¡¨"""
    return llm_factory.get_available_llm_models()


def list_available_embedding_models() -> List[str]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–å¯ç”¨Embeddingæ¨¡å‹åˆ—è¡¨"""
    return llm_factory.get_available_embedding_models()


if __name__ == "__main__":
    # æµ‹è¯•
    factory = LLMFactory()
    factory.list_models()
    
    # æµ‹è¯•åˆ›å»ºæ¨¡å‹
    try:
        llm = factory.create_llm()
        logger.info(f"âœ… é»˜è®¤LLMåˆ›å»ºæˆåŠŸ: {type(llm).__name__}")
        
        embedding = factory.create_embedding()
        logger.info(f"âœ… é»˜è®¤Embeddingåˆ›å»ºæˆåŠŸ: {type(embedding).__name__}")
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")

    
    # åˆ›å»ºdeepseekæ¨¡å‹
    deepseek_llm = factory.create_llm(model_name='deepseek.DeepSeek_V3')
    logger.info(f"âœ… DeepSeek LLMåˆ›å»ºæˆåŠŸ: {type(deepseek_llm).__name__}")

    response = deepseek_llm.invoke("ä½ å¥½ï¼ŒDeepSeekï¼")
    logger.info(f"âœ… DeepSeek LLMå“åº”æˆåŠŸ: {response}")

    # åˆ›å»ºæœ¬åœ°Ollamaæ¨¡å‹
    ollama_llm = factory.create_llm(model_name='ollama.Qwen3_30B')
    logger.info(f"âœ… Ollama LLMåˆ›å»ºæˆåŠŸ: {type(ollama_llm).__name__}")
    response = ollama_llm.invoke("ä½ å¥½ï¼ŒOllamaï¼")
    logger.info(f"âœ… Ollama LLMå“åº”æˆåŠŸ: {response}")