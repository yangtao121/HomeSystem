"""
ç®€æ´çš„LLMå·¥å‚ - ä»YAMLé…ç½®æ–‡ä»¶è¯»å–å¹¶åˆ›å»ºlanggraphç›´æ¥å¯ç”¨çš„å®ä¾‹
"""

import os
import yaml
from typing import Optional, Dict, List
from pathlib import Path
from loguru import logger
from dotenv import load_dotenv

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.embeddings import Embeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_deepseek import ChatDeepSeek
# from langchain_community.chat_models import ChatZhipuAI
from pydantic import SecretStr


class LLMFactory:
    """LLMå·¥å‚ - ä»YAMLé…ç½®è¯»å–å¯ç”¨æ¨¡å‹å¹¶åˆ›å»ºå®ä¾‹"""
    
    def __init__(self, config_path: Optional[str] = None):
        load_dotenv()
        
        if config_path is None:
            config_path = str(Path(__file__).parent / "config" / "llm_providers.yaml")
        
        self.config = self._load_config(config_path)
        self.available_llm_models = self._detect_available_llm_models()
        self.available_embedding_models = self._detect_available_embedding_models()
        
        logger.info(f"æ£€æµ‹åˆ° {len(self.available_llm_models)} ä¸ªLLMæ¨¡å‹, {len(self.available_embedding_models)} ä¸ªEmbeddingæ¨¡å‹")
    
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _is_provider_available(self, provider_config: Dict) -> bool:
        """æ£€æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨"""
        api_key_env = provider_config.get('api_key_env')
        
        # Ollamaä¸éœ€è¦API Key
        if not api_key_env:
            return True
        
        api_key = os.getenv(api_key_env)
        return api_key and not api_key.startswith('your_')
    
    def _detect_available_llm_models(self) -> Dict[str, Dict]:
        """æ£€æµ‹å¯ç”¨çš„LLMæ¨¡å‹"""
        available = {}
        
        for provider_key, provider_config in self.config.get('providers', {}).items():
            if self._is_provider_available(provider_config):
                for model in provider_config.get('models', []):
                    # ä½¿ç”¨æ–°çš„keyæ ¼å¼ï¼ˆprovider.modelï¼‰
                    model_key = model.get('key', f"{provider_key}.{model['name'].replace(' ', '_').replace('-', '_').replace(':', '_').replace('/', '_')}")
                    available[model_key] = {
                        'provider': provider_key,
                        'model_name': model['name'],
                        'display_name': model['display_name'],
                        'type': provider_config['type'],
                        'api_key_env': provider_config.get('api_key_env'),
                        'base_url_env': provider_config.get('base_url_env'),
                        'base_url': provider_config.get('base_url'),
                        'description': model.get('description', ''),
                        'max_tokens': model.get('max_tokens'),
                        'context_length': model.get('context_length'),
                        'supports_functions': model.get('supports_functions', False),
                        'supports_vision': model.get('supports_vision', False),
                        'supports_thinking': model.get('supports_thinking', False),
                        'thinking_max_length': model.get('thinking_max_length')
                    }
        
        return available
    
    def _detect_available_embedding_models(self) -> Dict[str, Dict]:
        """æ£€æµ‹å¯ç”¨çš„Embeddingæ¨¡å‹"""
        available = {}
        
        for provider_key, provider_config in self.config.get('embedding_providers', {}).items():
            if self._is_provider_available(provider_config):
                for model in provider_config.get('models', []):
                    # ä½¿ç”¨æ–°çš„keyæ ¼å¼ï¼ˆprovider.modelï¼‰
                    model_key = model.get('key', f"{provider_key}.{model['name'].replace(' ', '_').replace('-', '_').replace(':', '_').replace('/', '_')}")
                    available[model_key] = {
                        'provider': provider_key,
                        'model_name': model['name'],
                        'display_name': model['display_name'],
                        'type': provider_config['type'],
                        'api_key_env': provider_config.get('api_key_env'),
                        'base_url_env': provider_config.get('base_url_env'),
                        'base_url': provider_config.get('base_url'),
                        'description': model.get('description', ''),
                        'dimensions': model.get('dimensions'),
                        'max_input_length': model.get('max_input_length')
                    }
        
        return available
    
    def get_available_llm_models(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨LLMæ¨¡å‹åˆ—è¡¨"""
        return list(self.available_llm_models.keys())
    
    def get_available_embedding_models(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨Embeddingæ¨¡å‹åˆ—è¡¨"""
        return list(self.available_embedding_models.keys())
    
    def get_available_vision_models(self) -> List[str]:
        """è·å–æ‰€æœ‰æ”¯æŒè§†è§‰çš„æ¨¡å‹åˆ—è¡¨"""
        return [model_key for model_key, config in self.available_llm_models.items() 
                if config.get('supports_vision', False)]
    
    def supports_vision(self, model_name: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦æ”¯æŒè§†è§‰åŠŸèƒ½"""
        if model_name not in self.available_llm_models:
            return False
        return self.available_llm_models[model_name].get('supports_vision', False)
    
    def is_local_model(self, model_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°æ¨¡å‹"""
        if model_name not in self.available_llm_models:
            return False
        return self.available_llm_models[model_name]['type'] == 'ollama'
    
    def supports_thinking(self, model_name: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦æ”¯æŒæ€è€ƒæ¨¡å¼"""
        if model_name not in self.available_llm_models:
            return False
        return self.available_llm_models[model_name].get('supports_thinking', False)
    
    def get_available_thinking_models(self) -> List[str]:
        """è·å–æ‰€æœ‰æ”¯æŒæ€è€ƒæ¨¡å¼çš„æ¨¡å‹åˆ—è¡¨"""
        return [model_key for model_key, config in self.available_llm_models.items() 
                if config.get('supports_thinking', False)]
    
    def create_llm(self, model_name: Optional[str] = None, **kwargs) -> BaseChatModel:
        """
        åˆ›å»ºLLMå®ä¾‹ï¼Œç›´æ¥ç”¨äºlanggraph
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            **kwargs: ä¼ é€’ç»™æ¨¡å‹çš„å‚æ•°
            
        Returns:
            BaseChatModel: å¯ç›´æ¥ç”¨äºlanggraphçš„LLMå®ä¾‹
        """
        # ä½¿ç”¨é»˜è®¤æ¨¡å‹
        if model_name is None:
            default_config = self.config.get('defaults', {}).get('llm', {})
            model_name = default_config.get('model_key', 'deepseek.DeepSeek_V3')
        
        if model_name not in self.available_llm_models:
            available = ', '.join(self.available_llm_models.keys())
            raise ValueError(f"æ¨¡å‹ '{model_name}' ä¸å¯ç”¨ã€‚å¯ç”¨æ¨¡å‹: {available}")
        
        config = self.available_llm_models[model_name]
        logger.info(f"åˆ›å»ºLLM: {model_name} ({config['display_name']})")
        
        # åˆ†ç¦»æ€è€ƒæ¨¡å¼å‚æ•°å’Œæ™®é€šå‚æ•°
        thinking_params = {}
        if 'enable_thinking' in kwargs:
            thinking_params['enable_thinking'] = kwargs.pop('enable_thinking')
        if 'thinking_budget' in kwargs:
            thinking_params['thinking_budget'] = kwargs.pop('thinking_budget')
        
        # è®¾ç½®é»˜è®¤å‚æ•°
        defaults = self.config.get('defaults', {}).get('llm', {})
        params = {
            'temperature': kwargs.get('temperature', defaults.get('temperature', 0.7)),
            'max_tokens': kwargs.get('max_tokens', config.get('max_tokens', defaults.get('max_tokens', 4000))),
            **{k: v for k, v in kwargs.items() if k not in ['temperature', 'max_tokens']}
        }
        
        if config['type'] == 'ollama':
            base_url = os.getenv(config['base_url_env'], config['base_url'])
            return ChatOllama(
                model=config['model_name'],
                base_url=base_url,
                num_predict=params.pop('max_tokens'),
                **params
            )
        elif config['provider'] == 'deepseek':  # Use native ChatDeepSeek for DeepSeek models
            api_key = os.getenv(config['api_key_env'])
            # DeepSeek has max_tokens limit of 8192
            if 'max_tokens' in params and params['max_tokens'] > 8192:
                params['max_tokens'] = 8192
            return ChatDeepSeek(
                model=config['model_name'],
                api_key=SecretStr(api_key) if api_key else None,
                **params
            )
        elif config['provider'] == 'zhipuai':  # Use OpenAI compatible for ZhipuAI models
            api_key = os.getenv(config['api_key_env'])
            base_url = os.getenv(config['base_url_env'], config['base_url'])
            return ChatOpenAI(
                model=config['model_name'],
                api_key=SecretStr(api_key) if api_key else None,
                base_url=base_url,
                **params
            )
        else:  # openai_compatible
            api_key = os.getenv(config['api_key_env'])
            base_url = os.getenv(config['base_url_env'], config['base_url'])
            
            # å¤„ç†é˜¿é‡Œäº‘æ€è€ƒæ¨¡å¼æ¨¡å‹çš„ç‰¹æ®Šå‚æ•°
            if config.get('provider') == 'alibaba' and config.get('supports_thinking', False) and thinking_params:
                return ChatOpenAI(
                    model=config['model_name'],
                    api_key=SecretStr(api_key) if api_key else None,
                    base_url=base_url,
                    model_kwargs=thinking_params,
                    **params
                )
            else:
                return ChatOpenAI(
                    model=config['model_name'],
                    api_key=SecretStr(api_key) if api_key else None,
                    base_url=base_url,
                    **params
                )
    
    def create_vision_llm(self, model_name: Optional[str] = None, **kwargs) -> BaseChatModel:
        """
        åˆ›å»ºæ”¯æŒè§†è§‰çš„LLMå®ä¾‹
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©æ”¯æŒè§†è§‰çš„æ¨¡å‹
            **kwargs: ä¼ é€’ç»™æ¨¡å‹çš„å‚æ•°
            
        Returns:
            BaseChatModel: æ”¯æŒè§†è§‰çš„LLMå®ä¾‹
            
        Raises:
            ValueError: å¦‚æœæŒ‡å®šçš„æ¨¡å‹ä¸æ”¯æŒè§†è§‰åŠŸèƒ½æˆ–ä¸ºäº‘ç«¯æ¨¡å‹
        """
        # å¦‚æœæœªæŒ‡å®šæ¨¡å‹ï¼Œé€‰æ‹©é»˜è®¤çš„è§†è§‰æ¨¡å‹
        if model_name is None:
            vision_models = self.get_available_vision_models()
            if not vision_models:
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„è§†è§‰æ¨¡å‹")
            model_name = vision_models[0]  # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„è§†è§‰æ¨¡å‹
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒè§†è§‰
        if not self.supports_vision(model_name):
            raise ValueError(f"æ¨¡å‹ '{model_name}' ä¸æ”¯æŒè§†è§‰åŠŸèƒ½")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°æ¨¡å‹ï¼ˆåªæœ‰æœ¬åœ°æ¨¡å‹æ”¯æŒè§†è§‰ï¼‰
        if not self.is_local_model(model_name):
            available_vision = ', '.join(self.get_available_vision_models())
            raise ValueError(f"äº‘ç«¯æ¨¡å‹ '{model_name}' ä¸æ”¯æŒè§†è§‰åŠŸèƒ½ã€‚è¯·ä½¿ç”¨æœ¬åœ°è§†è§‰æ¨¡å‹: {available_vision}")
        
        # åˆ›å»ºæ”¯æŒè§†è§‰çš„LLMå®ä¾‹
        logger.info(f"åˆ›å»ºè§†è§‰LLM: {model_name}")
        return self.create_llm(model_name, **kwargs)
    
    def create_thinking_llm(self, model_name: Optional[str] = None, **kwargs) -> BaseChatModel:
        """
        åˆ›å»ºæ”¯æŒæ€è€ƒæ¨¡å¼çš„LLMå®ä¾‹
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©æ”¯æŒæ€è€ƒæ¨¡å¼çš„æ¨¡å‹
            **kwargs: ä¼ é€’ç»™æ¨¡å‹çš„å‚æ•°ï¼Œå¯åŒ…å«enable_thinkingå’Œthinking_budget
            
        Returns:
            BaseChatModel: æ”¯æŒæ€è€ƒæ¨¡å¼çš„LLMå®ä¾‹
            
        Raises:
            ValueError: å¦‚æœæŒ‡å®šçš„æ¨¡å‹ä¸æ”¯æŒæ€è€ƒæ¨¡å¼
        """
        # å¦‚æœæœªæŒ‡å®šæ¨¡å‹ï¼Œé€‰æ‹©é»˜è®¤çš„æ€è€ƒæ¨¡å¼æ¨¡å‹
        if model_name is None:
            thinking_models = self.get_available_thinking_models()
            if not thinking_models:
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ€è€ƒæ¨¡å¼æ¨¡å‹")
            model_name = thinking_models[0]  # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ€è€ƒæ¨¡å¼æ¨¡å‹
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæ€è€ƒæ¨¡å¼
        if not self.supports_thinking(model_name):
            raise ValueError(f"æ¨¡å‹ '{model_name}' ä¸æ”¯æŒæ€è€ƒæ¨¡å¼")
        
        # è®¾ç½®é»˜è®¤çš„æ€è€ƒæ¨¡å¼å‚æ•°
        if 'enable_thinking' not in kwargs:
            kwargs['enable_thinking'] = True
        if 'thinking_budget' not in kwargs:
            kwargs['thinking_budget'] = 1  # é»˜è®¤æ€è€ƒé¢„ç®—
        
        # åˆ›å»ºæ”¯æŒæ€è€ƒæ¨¡å¼çš„LLMå®ä¾‹
        logger.info(f"åˆ›å»ºæ€è€ƒæ¨¡å¼LLM: {model_name}")
        return self.create_llm(model_name, **kwargs)
    
    def validate_vision_input(self, model_name: str) -> None:
        """
        éªŒè¯æ¨¡å‹æ˜¯å¦å¯ä»¥æ¥å—è§†è§‰è¾“å…¥
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Raises:
            ValueError: å¦‚æœæ¨¡å‹ä¸æ”¯æŒè§†è§‰æˆ–ä¸ºäº‘ç«¯æ¨¡å‹
        """
        if not self.supports_vision(model_name):
            if self.is_local_model(model_name):
                available_vision = ', '.join(self.get_available_vision_models())
                raise ValueError(f"æœ¬åœ°æ¨¡å‹ '{model_name}' ä¸æ”¯æŒè§†è§‰åŠŸèƒ½ã€‚è¯·ä½¿ç”¨: {available_vision}")
            else:
                raise ValueError(f"äº‘ç«¯æ¨¡å‹ '{model_name}' ä»…æ”¯æŒçº¯æ–‡æœ¬è¾“å…¥ï¼Œä¸æ”¯æŒå›¾ç‰‡å¤„ç†")
    
    def create_embedding(self, model_name: Optional[str] = None, **kwargs) -> Embeddings:
        """
        åˆ›å»ºEmbeddingå®ä¾‹
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            **kwargs: ä¼ é€’ç»™æ¨¡å‹çš„å‚æ•°
            
        Returns:
            Embeddings: Embeddingå®ä¾‹
        """
        # ä½¿ç”¨é»˜è®¤æ¨¡å‹
        if model_name is None:
            default_config = self.config.get('defaults', {}).get('embedding', {})
            model_name = default_config.get('model_key', 'ollama.BGE_M3')
        
        if model_name not in self.available_embedding_models:
            available = ', '.join(self.available_embedding_models.keys())
            raise ValueError(f"Embeddingæ¨¡å‹ '{model_name}' ä¸å¯ç”¨ã€‚å¯ç”¨æ¨¡å‹: {available}")
        
        config = self.available_embedding_models[model_name]
        logger.info(f"åˆ›å»ºEmbedding: {model_name} ({config['display_name']})")
        
        if config['type'] == 'ollama_embedding':
            base_url = os.getenv(config['base_url_env'], config['base_url'])
            return OllamaEmbeddings(
                model=config['model_name'],
                base_url=base_url,
                **kwargs
            )
        elif config['type'] == 'openai_embedding':
            api_key = os.getenv(config['api_key_env'])
            base_url = os.getenv(config['base_url_env'], config['base_url'])
            return OpenAIEmbeddings(
                model=config['model_name'],
                api_key=SecretStr(api_key) if api_key else None,
                base_url=base_url,
                **kwargs
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„embeddingç±»å‹: {config['type']}")
    
    def list_models(self) -> None:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        logger.info("=" * 80)
        logger.info("å¯ç”¨æ¨¡å‹åˆ—è¡¨")
        logger.info("=" * 80)
        
        logger.info("\nğŸ“ LLMæ¨¡å‹:")
        logger.info("-" * 60)
        for model_name, config in self.available_llm_models.items():
            vision_mark = "ğŸ‘ï¸" if config.get('supports_vision', False) else "ğŸ“"
            thinking_mark = "ğŸ§ " if config.get('supports_thinking', False) else ""
            local_mark = "ğŸ " if config['type'] == 'ollama' else "â˜ï¸"
            logger.info(f"âœ… {model_name:35} | {vision_mark}{thinking_mark}{local_mark} {config['display_name']}")
        
        logger.info("\nğŸ” Embeddingæ¨¡å‹:")
        logger.info("-" * 60)
        for model_name, config in self.available_embedding_models.items():
            dims = f"({config['dimensions']}ç»´)" if config.get('dimensions') else ""
            logger.info(f"âœ… {model_name:35} | {config['display_name']} {dims}")
        
        logger.info("=" * 80)
        logger.info(f"æ€»è®¡: {len(self.available_llm_models)} ä¸ªLLMæ¨¡å‹, {len(self.available_embedding_models)} ä¸ªEmbeddingæ¨¡å‹")


# å…¨å±€å·¥å‚å®ä¾‹
llm_factory = LLMFactory()


def get_llm(model_name: Optional[str] = None, **kwargs) -> BaseChatModel:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºLLMå®ä¾‹"""
    return llm_factory.create_llm(model_name, **kwargs)


def get_embedding(model_name: Optional[str] = None, **kwargs) -> Embeddings:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºEmbeddingå®ä¾‹"""
    return llm_factory.create_embedding(model_name, **kwargs)


def list_available_llm_models() -> List[str]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–å¯ç”¨LLMæ¨¡å‹åˆ—è¡¨"""
    return llm_factory.get_available_llm_models()


def list_available_embedding_models() -> List[str]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–å¯ç”¨Embeddingæ¨¡å‹åˆ—è¡¨"""
    return llm_factory.get_available_embedding_models()


def get_vision_llm(model_name: Optional[str] = None, **kwargs) -> BaseChatModel:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºæ”¯æŒè§†è§‰çš„LLMå®ä¾‹"""
    return llm_factory.create_vision_llm(model_name, **kwargs)


def list_available_vision_models() -> List[str]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–å¯ç”¨è§†è§‰æ¨¡å‹åˆ—è¡¨"""
    return llm_factory.get_available_vision_models()


def check_vision_support(model_name: str) -> bool:
    """ä¾¿æ·å‡½æ•°ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒè§†è§‰"""
    return llm_factory.supports_vision(model_name)


def validate_vision_input(model_name: str) -> None:
    """ä¾¿æ·å‡½æ•°ï¼šéªŒè¯æ¨¡å‹è§†è§‰è¾“å…¥èƒ½åŠ›"""
    return llm_factory.validate_vision_input(model_name)


def get_thinking_llm(model_name: Optional[str] = None, **kwargs) -> BaseChatModel:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºæ”¯æŒæ€è€ƒæ¨¡å¼çš„LLMå®ä¾‹"""
    return llm_factory.create_thinking_llm(model_name, **kwargs)


def list_available_thinking_models() -> List[str]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–å¯ç”¨æ€è€ƒæ¨¡å¼æ¨¡å‹åˆ—è¡¨"""
    return llm_factory.get_available_thinking_models()


def check_thinking_support(model_name: str) -> bool:
    """ä¾¿æ·å‡½æ•°ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæ€è€ƒæ¨¡å¼"""
    return llm_factory.supports_thinking(model_name)


if __name__ == "__main__":
    # æµ‹è¯•
    factory = LLMFactory()
    factory.list_models()
    
    # æµ‹è¯•åˆ›å»ºæ¨¡å‹
    try:
        llm = factory.create_llm()
        logger.info(f"âœ… é»˜è®¤LLMåˆ›å»ºæˆåŠŸ: {type(llm).__name__}")
        
        embedding = factory.create_embedding()
        logger.info(f"âœ… é»˜è®¤Embeddingåˆ›å»ºæˆåŠŸ: {type(embedding).__name__}")
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")

    
    # åˆ›å»ºdeepseekæ¨¡å‹
    deepseek_llm = factory.create_llm(model_name='deepseek.DeepSeek_V3')
    logger.info(f"âœ… DeepSeek LLMåˆ›å»ºæˆåŠŸ: {type(deepseek_llm).__name__}")

    response = deepseek_llm.invoke("ä½ å¥½ï¼ŒDeepSeekï¼")
    logger.info(f"âœ… DeepSeek LLMå“åº”æˆåŠŸ: {response}")

    # åˆ›å»ºæœ¬åœ°Ollamaæ¨¡å‹
    ollama_llm = factory.create_llm(model_name='ollama.Qwen3_30B')
    logger.info(f"âœ… Ollama LLMåˆ›å»ºæˆåŠŸ: {type(ollama_llm).__name__}")
    response = ollama_llm.invoke("ä½ å¥½ï¼ŒOllamaï¼")
    logger.info(f"âœ… Ollama LLMå“åº”æˆåŠŸ: {response}")

    # åˆ›å»ºkimiæ¨¡å‹
    # kimi_llm = factory.create_llm(model_name='moonshot.Kimi_K2')
    # logger.info(f"âœ… Kimi LLMåˆ›å»ºæˆåŠŸ: {type(kimi_llm).__name__}")
    # response = kimi_llm.invoke("ä½ å¥½ï¼ŒKimiï¼")

    # åˆ›å»ºç¡…åŸºæµåŠ¨æ¨¡å‹
    siliconflow_llm = factory.create_llm(model_name='siliconflow.DeepSeek_V3')
    logger.info(f"âœ… SiliconFlow LLMåˆ›å»ºæˆåŠŸ: {type(siliconflow_llm).__name__}")
    response = siliconflow_llm.invoke("ä½ å¥½ï¼ŒSiliconFlowï¼")
    logger.info(f"âœ… SiliconFlow LLMå“åº”æˆåŠŸ: {response}")

    # åˆ›å»ºæ™ºè°±AIæ¨¡å‹ï¼ˆå¦‚æœAPI Keyå¯ç”¨ï¼‰
    try:
        zhipuai_llm = factory.create_llm(model_name='zhipuai.GLM_4_5')
        logger.info(f"âœ… ZhipuAI LLMåˆ›å»ºæˆåŠŸ: {type(zhipuai_llm).__name__}")
        response = zhipuai_llm.invoke("ä½ å¥½ï¼Œæ™ºè°±AIï¼")
        logger.info(f"âœ… ZhipuAI LLMå“åº”æˆåŠŸ: {response}")
    except Exception as e:
        logger.warning(f"âš ï¸  ZhipuAIæ¨¡å‹åˆ›å»ºå¤±è´¥ï¼ˆå¯èƒ½ç¼ºå°‘API Keyï¼‰: {e}")
