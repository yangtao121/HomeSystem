"""
æ·±åº¦è®ºæ–‡åˆ†æAgent

åŸºäºLangGraphçš„è®ºæ–‡æ·±åº¦åˆ†ææ™ºèƒ½ä½“ï¼Œä½¿ç”¨æ ‡å‡†å·¥å…·è°ƒç”¨æ¨¡å¼ï¼š
1. äº‘ç«¯LLMä¸»å¯¼åˆ†æï¼Œè‡ªåŠ¨å†³ç­–ä½•æ—¶è°ƒç”¨å›¾ç‰‡åˆ†æå·¥å…·
2. ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆå®Œæ•´çš„åˆ†æç»“æœ
3. æ”¯æŒåŒè¯­åˆ†æç»“æœè¾“å‡º

é‡‡ç”¨æ ‡å‡†LangGraphå·¥å…·è°ƒç”¨æ¶æ„ï¼ŒLLMè‡ªä¸»å†³ç­–å·¥å…·ä½¿ç”¨ã€‚
"""

import json
import os
import re
import weakref
from concurrent.futures import ThreadPoolExecutor
from typing import Annotated, Any, Dict, List, Optional, Union
from typing_extensions import TypedDict

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import AIMessage, SystemMessage, ToolMessage, BaseMessage
from loguru import logger

from .base_graph import BaseGraph
from .llm_factory import get_llm
from .tool.image_analysis_tool import create_image_analysis_tool
from .tool.video_resource_processor import VideoResourceProcessor
from .parser.paper_folder_parser import create_paper_folder_parser
from .formatter.markdown_formatter import create_markdown_formatter


class DeepPaperAnalysisState(TypedDict):
    """æ·±åº¦è®ºæ–‡åˆ†æçŠ¶æ€"""
    # è¾“å…¥æ•°æ®
    base_folder_path: str                           # è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
    paper_text: str                                 # è®ºæ–‡markdownæ–‡æœ¬
    available_images: List[str]                     # å¯ç”¨å›¾ç‰‡åˆ—è¡¨
    image_mappings: Dict[str, str]                  # å›¾ç‰‡è·¯å¾„æ˜ å°„
    
    # LangGraphæ¶ˆæ¯å†å²
    messages: Annotated[list, add_messages]         # å¯¹è¯å†å²
    
    # åˆ†æç»“æœ
    analysis_result: Optional[str]                  # æœ€ç»ˆåˆ†æç»“æœ
    
    # æ‰§è¡ŒçŠ¶æ€
    is_complete: bool                               # æ˜¯å¦å®Œæˆåˆ†æ
    
    # ç”¨æˆ·æç¤ºè¯
    user_prompt: Optional[str]                      # ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯


class DeepPaperAnalysisConfig:
    """æ·±åº¦è®ºæ–‡åˆ†æé…ç½®ç±»"""
    
    def __init__(self,
                 analysis_model: str = "deepseek.DeepSeek_V3",
                 vision_model: str = "ollama.Qwen2_5_VL_7B", 
                 memory_enabled: bool = True,
                 # æ–°å¢è§†é¢‘åˆ†æç›¸å…³é…ç½®
                 enable_video_analysis: bool = False,  # é»˜è®¤å…³é—­
                 video_analysis_model: str = "ollama.Qwen3_30B",  # è§†é¢‘åˆ†ææ¨¡å‹
                 # æ–°å¢ç”¨æˆ·æç¤ºè¯é…ç½®
                 enable_user_prompt: bool = False,  # é»˜è®¤å…³é—­
                 user_prompt: Optional[str] = None,  # ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯
                 user_prompt_position: str = "before_analysis",  # æç¤ºè¯ä½ç½®: before_analysis, after_tools, custom
                 custom_settings: Optional[Dict[str, Any]] = None):
        
        self.analysis_model = analysis_model          # ä¸»åˆ†æLLM
        self.vision_model = vision_model              # å›¾ç‰‡ç†è§£VLM
        self.memory_enabled = memory_enabled
        # è§†é¢‘åˆ†æé…ç½®
        self.enable_video_analysis = enable_video_analysis
        self.video_analysis_model = video_analysis_model
        # ç”¨æˆ·æç¤ºè¯é…ç½®
        self.enable_user_prompt = enable_user_prompt
        self.user_prompt = user_prompt
        self.user_prompt_position = user_prompt_position
        self.custom_settings = custom_settings or {}
    
    @classmethod
    def load_from_file(cls, config_path: str) -> "DeepPaperAnalysisConfig":
        """ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            return cls(**config_data)
        except Exception as e:
            logger.warning(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
            return cls()


class DeepPaperAnalysisAgent(BaseGraph):
    """æ·±åº¦è®ºæ–‡åˆ†ææ™ºèƒ½ä½“
    
    åŠŸèƒ½ï¼š
    1. ä½¿ç”¨æ ‡å‡†LangGraphå·¥å…·è°ƒç”¨æ¨¡å¼
    2. äº‘ç«¯LLMè‡ªä¸»å†³ç­–å·¥å…·ä½¿ç”¨
    3. ç»“æ„åŒ–è¾“å‡ºå’ŒåŒè¯­æ”¯æŒ
    """
    
    def __init__(self,
                 config: Optional[DeepPaperAnalysisConfig] = None,
                 config_path: Optional[str] = None,
                 **kwargs):
        
        super().__init__(**kwargs)
        
        # åŠ è½½é…ç½®
        if config_path:
            self.config = DeepPaperAnalysisConfig.load_from_file(config_path)
        elif config:
            self.config = config
        else:
            self.config = DeepPaperAnalysisConfig()
        
        logger.info(f"åˆå§‹åŒ–æ·±åº¦è®ºæ–‡åˆ†ææ™ºèƒ½ä½“")
        logger.info(f"åˆ†ææ¨¡å‹: {self.config.analysis_model}")
        logger.info(f"è§†è§‰æ¨¡å‹: {self.config.vision_model}")
        logger.info(f"è§†é¢‘åˆ†æåŠŸèƒ½: {'å¯ç”¨' if self.config.enable_video_analysis else 'ç¦ç”¨'}")
        if self.config.enable_video_analysis:
            logger.info(f"è§†é¢‘åˆ†ææ¨¡å‹: {self.config.video_analysis_model}")
        
        # åˆ›å»ºä¸»åˆ†æLLM
        self.analysis_llm = get_llm(self.config.analysis_model)
        
        # ç§»é™¤äº†ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½ï¼Œç®€åŒ–ä¸ºç›´æ¥æ–‡æœ¬è¾“å‡º
        
        # è®¾ç½®å†…å­˜ç®¡ç† - ä½¿ç”¨ç‹¬ç«‹çš„çº¿ç¨‹æ± é…ç½®
        self._custom_executor = None
        if self.config.memory_enabled:
            try:
                # åˆ›å»ºä¸“ç”¨çš„çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œé¿å…ä¸ç³»ç»Ÿé»˜è®¤æ‰§è¡Œå™¨å†²çª
                self._custom_executor = ThreadPoolExecutor(
                    max_workers=2, 
                    thread_name_prefix="deep_analysis_checkpointer"
                )
                self.memory = MemorySaver()
                # æ³¨å†Œæ¸…ç†å‡½æ•°ï¼Œç¡®ä¿èµ„æºé‡Šæ”¾
                weakref.finalize(self, self._cleanup_executor, self._custom_executor)
                logger.info("âœ… å†…å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨ç‹¬ç«‹çº¿ç¨‹æ± ")
            except Exception as e:
                logger.warning(f"âš ï¸ å†…å­˜ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç¦ç”¨å†…å­˜åŠŸèƒ½: {e}")
                self.memory = None
                self._custom_executor = None
        else:
            self.memory = None
        
        # åˆ†æå·¥å…·å°†åœ¨è¿è¡Œæ—¶åˆ›å»º
        self.image_tool = None
        self.video_tool = None  # è§†é¢‘åˆ†æå·¥å…·
        self.llm_with_tools = None
        self.tool_node = None
        
        # æ„å»ºå›¾ï¼ˆå°†åœ¨åˆ†ææ—¶åŠ¨æ€å®Œæˆï¼‰
        self._graph_template = None
        self.agent = None
        
        # èµ„æºæ¸…ç†çŠ¶æ€
        self._is_cleaned_up = False
        
        logger.info("æ·±åº¦è®ºæ–‡åˆ†ææ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def _build_graph_with_tools(self, tools: List[Any]) -> None:
        """ä½¿ç”¨å·¥å…·æ„å»ºç®€åŒ–çš„LangGraphå·¥ä½œæµ"""
        graph = StateGraph(DeepPaperAnalysisState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("initialize", self._initialize_node)
        graph.add_node("analysis_with_tools", self._analysis_with_tools_node)
        # æ·»åŠ  tool_node - ä½¿ç”¨åŠ¨æ€å·¥å…·åˆ—è¡¨
        self.tool_node = ToolNode(tools)
        graph.add_node("call_tools", self.tool_node)
        # æ·»åŠ å›¾ç‰‡è·¯å¾„ä¿®æ­£èŠ‚ç‚¹
        graph.add_node("correct_image_paths", self._correct_image_paths_node)
        
        # æ„å»ºç®€åŒ–æµç¨‹
        graph.add_edge(START, "initialize")
        graph.add_edge("initialize", "analysis_with_tools")
        
        # åˆ†æå·¥å…·è°ƒç”¨çš„æ¡ä»¶åˆ†æ”¯
        graph.add_conditional_edges(
            "analysis_with_tools",
            self._should_continue_analysis,
            {
                "call_tools": "call_tools",  # è°ƒç”¨å·¥å…·
                "continue": "analysis_with_tools",  # ç»§ç»­åˆ†æ
                "end": "correct_image_paths",  # åˆ†æå®Œæˆï¼Œè¿›è¡Œè·¯å¾„ä¿®æ­£
            }
        )
        
        # å·¥å…·è°ƒç”¨åå›åˆ°åˆ†æèŠ‚ç‚¹
        graph.add_edge("call_tools", "analysis_with_tools")
        
        # å›¾ç‰‡è·¯å¾„ä¿®æ­£åç»“æŸ
        graph.add_edge("correct_image_paths", END)
        
        # ç¼–è¯‘å›¾ - æ·»åŠ é”™è¯¯æ¢å¤æœºåˆ¶
        try:
            # å¦‚æœå†…å­˜ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨æ— çŠ¶æ€æ¨¡å¼
            checkpointer = self.memory if self.memory else None
            if checkpointer is None:
                logger.warning("âš ï¸ å†…å­˜ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ— çŠ¶æ€æ¨¡å¼")
            
            self.agent = graph.compile(checkpointer=checkpointer)
            logger.info("âœ… LangGraph å›¾ç¼–è¯‘æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ LangGraph å›¾ç¼–è¯‘å¤±è´¥: {e}")
            # å°è¯•æ— çŠ¶æ€ç¼–è¯‘ä½œä¸ºé™çº§å¤„ç†
            try:
                logger.info("å°è¯•æ— çŠ¶æ€ç¼–è¯‘ä½œä¸ºé™çº§å¤„ç†...")
                self.agent = graph.compile(checkpointer=None)
                logger.warning("âš ï¸ ä½¿ç”¨æ— çŠ¶æ€æ¨¡å¼ç¼–è¯‘æˆåŠŸ")
            except Exception as fallback_error:
                logger.error(f"âŒ é™çº§ç¼–è¯‘ä¹Ÿå¤±è´¥: {fallback_error}")
                raise
    
    def _initialize_node(self, state: DeepPaperAnalysisState) -> Dict[str, Any]:
        """åˆå§‹åŒ–èŠ‚ç‚¹"""
        logger.info("âœ… å·¥å…·å·²åœ¨åˆ†æå¼€å§‹å‰åˆå§‹åŒ–")
        
        # åˆ›å»ºåˆå§‹åˆ†ææç¤º
        initial_prompt = self._generate_initial_analysis_prompt(state)
        
        return {
            "messages": [SystemMessage(content=initial_prompt)],
            "is_complete": False
        }
    
    def _analysis_with_tools_node(self, state: DeepPaperAnalysisState) -> Dict[str, Any]:
        """å¸¦å·¥å…·è°ƒç”¨çš„åˆ†æèŠ‚ç‚¹ - ä½¿ç”¨æ ‡å‡† LangGraph æ¨¡å¼"""
        logger.info("å¼€å§‹LLMåˆ†æ...")
        
        messages = state["messages"]
        
        try:
            # ç¡®ä¿llm_with_toolså·²åˆå§‹åŒ–
            if self.llm_with_tools is None:
                logger.error("âŒ LLM with tools not initialized")
                return {"messages": [AIMessage(content="LLMå·¥å…·æœªåˆå§‹åŒ–")]}
            
            # æ˜¾ç¤ºè¾“å…¥æ¶ˆæ¯çš„è¯¦ç»†ä¿¡æ¯
            logger.info(f"ğŸ“¤ å‘é€æ¶ˆæ¯ç»™ LLM:")
            logger.info(f"  - æ¶ˆæ¯æ•°é‡: {len(messages)}")
            for i, msg in enumerate(messages[-3:]):  # åªæ˜¾ç¤ºæœ€å3æ¡æ¶ˆæ¯
                msg_type = type(msg).__name__
                msg_preview = str(msg.content)[:100] if hasattr(msg, 'content') else str(msg)[:100]
                logger.info(f"  - æ¶ˆæ¯ {i}: {msg_type} - {msg_preview}...")
            
            # LLMè‡ªä¸»å†³ç­–å¹¶å¯èƒ½è°ƒç”¨å·¥å…·
            response = self.llm_with_tools.invoke(messages)
            
            # è¯¦ç»†æ£€æŸ¥å“åº”
            logger.info(f"ğŸ’¬ LLM å“åº”:")
            logger.info(f"  - å“åº”ç±»å‹: {type(response).__name__}")
            if hasattr(response, 'content'):
                content_preview = str(response.content)[:200] if response.content else "<empty>"
                logger.info(f"  - å†…å®¹é¢„è§ˆ: {content_preview}...")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            tool_calls = getattr(response, 'tool_calls', None)
            if tool_calls:
                logger.info(f"ğŸ”§ LLMå†³å®šè°ƒç”¨ {len(tool_calls)} ä¸ªå·¥å…·:")
                for i, tool_call in enumerate(tool_calls):
                    try:
                        if hasattr(tool_call, 'get'):
                            tool_name = tool_call.get('name', 'unknown')
                            tool_args = tool_call.get('args', {})
                        else:
                            # å¤„ç†ä¸åŒçš„ tool_call å¯¹è±¡ç±»å‹
                            tool_name = getattr(tool_call, 'name', str(tool_call))
                            tool_args = getattr(tool_call, 'args', {})
                        
                        logger.info(f"  [{i+1}] å·¥å…·: {tool_name}")
                        if isinstance(tool_args, dict):
                            for key, value in tool_args.items():
                                value_preview = str(value)[:100] if len(str(value)) > 100 else str(value)
                                logger.info(f"      {key}: {value_preview}")
                        else:
                            logger.info(f"      å‚æ•°: {tool_args}")
                    except Exception as e:
                        logger.warning(f"      æ— æ³•è§£æå·¥å…·è°ƒç”¨ {i+1}: {e}")
            else:
                logger.info("ğŸš« LLMæœªè°ƒç”¨ä»»ä½•å·¥å…·")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„åˆ†æç»“æœ
                if hasattr(response, 'content') and response.content:
                    content = str(response.content)
                    # å¦‚æœå†…å®¹è¾ƒé•¿ä¸”ä¸æ˜¯å·¥å…·è°ƒç”¨ï¼Œå¯èƒ½æ˜¯æœ€ç»ˆåˆ†æç»“æœ
                    if len(content) > 1000:
                        logger.info(f"âœ… æ£€æµ‹åˆ°å®Œæ•´åˆ†æç»“æœ ({len(content)} å­—ç¬¦)")
                        return {
                            "messages": [response],
                            "analysis_result": content,
                            "is_complete": True
                        }
            
            return {"messages": [response]}
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æèŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            error_message = AIMessage(content=f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            return {"messages": [error_message]}
    
    def _should_continue_analysis(self, state: DeepPaperAnalysisState) -> str:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­åˆ†ææˆ–è°ƒç”¨å·¥å…·"""
        messages = state["messages"]
        
        logger.info(f"ğŸ”„ åˆ†ææ§åˆ¶æµ: æ£€æŸ¥ {len(messages)} æ¡æ¶ˆæ¯")
        
        # æ£€æŸ¥æœ€åçš„æ¶ˆæ¯
        if messages:
            last_message = messages[-1]
            last_msg_type = type(last_message).__name__
            logger.info(f"  - æœ€åæ¶ˆæ¯ç±»å‹: {last_msg_type}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ AI æ¶ˆæ¯å¹¶ä¸”åŒ…å«å·¥å…·è°ƒç”¨
            if isinstance(last_message, AIMessage):
                # ä½¿ç”¨ getattr å®‰å…¨æ£€æŸ¥ tool_calls å±æ€§
                tool_calls = getattr(last_message, 'tool_calls', None)
                if tool_calls:
                    logger.info(f"ğŸ”§ æ£€æµ‹åˆ° {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨ â†’ call_tools")
                    return "call_tools"
                
                # æ£€æŸ¥æ˜¯å¦LLMè¡¨ç¤ºåˆ†æå®Œæˆ
                content = last_message.content
                if isinstance(content, str):
                    content_lower = content.lower()
                    completion_keywords = [
                        "åˆ†æå®Œæˆ", "analysis complete", "å®Œæˆåˆ†æ",
                        "åˆ†æç»“æŸ", "analysis finished", "ç»“æŸåˆ†æ",
                        "analysis is complete", "finished analyzing"
                    ]
                    if any(keyword in content_lower for keyword in completion_keywords):
                        logger.info(f"âœ… LLMè¡¨ç¤ºåˆ†æå®Œæˆ â†’ end")
                        return "end"
                    
                    # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼Œå¦‚æœè¾ƒé•¿å¯èƒ½æ˜¯å®Œæ•´åˆ†æ
                    if len(content) > 2000:  # å†…å®¹è¾ƒé•¿ï¼Œå¯èƒ½å·²ç»å®Œæˆäº†åˆ†æ
                        logger.info(f"ğŸ“ å†…å®¹è¾ƒé•¿ ({len(content)} å­—ç¬¦)ï¼Œå¯èƒ½å·²å®Œæˆåˆ†æ â†’ end")
                        return "end"
            
            # å¦‚æœæ˜¯å·¥å…·æ¶ˆæ¯ï¼Œè®©LLMç»§ç»­å¤„ç†å·¥å…·ç»“æœ
            elif isinstance(last_message, ToolMessage):
                logger.info(f"ğŸ”§ æ”¶åˆ°å·¥å…·ç»“æœ â†’ continue")
                return "continue"
        
        # é˜²æ­¢æ— é™å¾ªç¯ï¼šæ£€æŸ¥æ¶ˆæ¯æ•°é‡
        if len(messages) > 15:  # å¢åŠ ä¸Šé™ï¼Œç»™æ›´å¤šæœºä¼šè¿›è¡Œå·¥å…·è°ƒç”¨
            logger.warning(f"âš ï¸ æ¶ˆæ¯æ•°é‡è¶…è¿‡é™åˆ¶ ({len(messages)}) â†’ end")
            return "end"
        
        # ç»Ÿè®¡å·¥å…·è°ƒç”¨æ¬¡æ•°
        tool_call_count = 0
        tool_message_count = 0
        for msg in messages:
            if isinstance(msg, AIMessage):
                tool_calls = getattr(msg, 'tool_calls', None)
                if tool_calls:
                    tool_call_count += len(tool_calls)
            elif isinstance(msg, ToolMessage):
                tool_message_count += 1
        
        logger.info(f"  - å·¥å…·è°ƒç”¨æ¬¡æ•°: {tool_call_count}, å·¥å…·å“åº”: {tool_message_count}")
        
        # å¦‚æœå·²ç»è¿›è¡Œäº†è¶³å¤Ÿçš„å·¥å…·è°ƒç”¨ï¼Œè€ƒè™‘ç»“æŸ
        if tool_call_count >= 3:  # å·²ç»è¿›è¡Œäº†å¤šæ¬¡å·¥å…·è°ƒç”¨
            logger.info(f"ğŸ”„ å·²è¿›è¡Œ {tool_call_count} æ¬¡å·¥å…·è°ƒç”¨ï¼Œè€ƒè™‘ç»“æŸåˆ†æ â†’ end")
            return "end"
        
        # é»˜è®¤ç»§ç»­åˆ†æ
        logger.info(f"ğŸ”„ ç»§ç»­åˆ†æ â†’ continue")
        return "continue"
    
    def _correct_image_paths_node(self, state: DeepPaperAnalysisState) -> Dict[str, Any]:
        """ä¿®æ­£markdownä¸­çš„å›¾ç‰‡è·¯å¾„ä¸ºæ ‡å‡†æ ¼å¼ imgs/xxx.jpg"""
        logger.info("ğŸ“ å¼€å§‹ä¿®æ­£å›¾ç‰‡è·¯å¾„...")
        
        analysis_result = state.get("analysis_result")
        if not analysis_result:
            logger.warning("âš ï¸ æ²¡æœ‰åˆ†æç»“æœéœ€è¦ä¿®æ­£")
            return {}
        
        # å›¾ç‰‡è·¯å¾„ä¿®æ­£çš„æ­£åˆ™è¡¨è¾¾å¼
        # åŒ¹é… ![æè¿°](è·¯å¾„) æ ¼å¼
        image_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
        
        def correct_path(match):
            description = match.group(1)
            path = match.group(2)
            
            logger.info(f"ğŸ” å‘ç°å›¾ç‰‡è·¯å¾„: {path}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è§†é¢‘æ–‡ä»¶ï¼Œä¿æŒvideos/è·¯å¾„ä¸å˜
            if '/videos/' in path or path.startswith('videos/'):
                logger.info(f"ğŸ“¹ ä¿æŒè§†é¢‘è·¯å¾„ä¸å˜: {path}")
                return f'![{description}]({path})'
            
            # æå–æ–‡ä»¶å
            filename = os.path.basename(path)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡æ–‡ä»¶
            image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg'}
            file_ext = os.path.splitext(filename)[1].lower()
            
            if file_ext in image_extensions:
                # æ ‡å‡†åŒ–ä¸º imgs/filename æ ¼å¼
                corrected_path = f"imgs/{filename}"
                logger.info(f"âœ… ä¿®æ­£è·¯å¾„: {path} â†’ {corrected_path}")
                return f'![{description}]({corrected_path})'
            else:
                # éå›¾ç‰‡æ–‡ä»¶ä¿æŒä¸å˜
                logger.info(f"â„¹ï¸ éå›¾ç‰‡æ–‡ä»¶ä¿æŒä¸å˜: {path}")
                return f'![{description}]({path})'
        
        # æ‰§è¡Œè·¯å¾„ä¿®æ­£
        original_text = analysis_result
        corrected_text = re.sub(image_pattern, correct_path, original_text)
        
        # ç»Ÿè®¡ä¿®æ­£æ•°é‡
        original_matches = re.findall(image_pattern, original_text)
        corrected_matches = re.findall(image_pattern, corrected_text)
        
        corrections_made = 0
        for (orig_desc, orig_path), (corr_desc, corr_path) in zip(original_matches, corrected_matches):
            if orig_path != corr_path:
                corrections_made += 1
        
        logger.info(f"ğŸ“Š å›¾ç‰‡è·¯å¾„ä¿®æ­£å®Œæˆ:")
        logger.info(f"  - å‘ç°å›¾ç‰‡å¼•ç”¨: {len(original_matches)} ä¸ª")
        logger.info(f"  - æ‰§è¡Œä¿®æ­£: {corrections_made} ä¸ª")
        
        return {
            "analysis_result": corrected_text
        }
    
    
    def _generate_initial_analysis_prompt(self, state: DeepPaperAnalysisState) -> str:
        """ç”Ÿæˆåˆå§‹åˆ†ææç¤ºè¯ - è¦æ±‚æ ‡å‡†Markdownè¾“å‡ºæ ¼å¼"""
        available_images = state.get('available_images', [])
        image_list = "\n".join([f"  - {img}" for img in available_images[:10]])  # æ˜¾ç¤ºå‰10ä¸ªå›¾ç‰‡
        if len(available_images) > 10:
            image_list += f"\n  ... and {len(available_images) - 10} more images"
        
        # åŠ¨æ€ç”Ÿæˆå·¥å…·æè¿°
        tools_description = "- `analyze_image`: ç”¨äºåˆ†æè®ºæ–‡ä¸­çš„ä»»ä½•å›¾ç‰‡/ç¤ºæ„å›¾\n"
        tools_description += "  - å½“ä½ éœ€è¦ç†è§£æ–‡æœ¬ä¸­å¼•ç”¨çš„è§†è§‰å†…å®¹æ—¶è°ƒç”¨æ­¤å·¥å…·\n"
        tools_description += "  - å§‹ç»ˆåˆ†æå…³é”®æ¶æ„å›¾ã€å®éªŒå›¾è¡¨å’Œé‡è¦è¡¨æ ¼\n"
        tools_description += "  - æä¾›å…·ä½“çš„åˆ†ææŸ¥è¯¢ï¼Œå¦‚\"åˆ†æè¿™ä¸ªæ¶æ„å›¾å¹¶è¯†åˆ«ä¸»è¦ç»„ä»¶\"æˆ–\"ä»è¿™ä¸ªå®éªŒå›¾è¡¨ä¸­æå–æ€§èƒ½æŒ‡æ ‡\"\n"
        
        if self.video_tool:
            tools_description += "- `process_video_resources`: ç”¨äºåˆ†æè®ºæ–‡ç›¸å…³çš„æ¼”ç¤ºè§†é¢‘æˆ–é¡¹ç›®è§†é¢‘\n"
            tools_description += "  - å½“è®ºæ–‡åŒ…å«é¡¹ç›®åœ°å€ã€GitHubé“¾æ¥æˆ–å¼€æºä»£ç æ—¶ä½¿ç”¨\n"
            tools_description += "  - è‡ªåŠ¨ä¸‹è½½è§†é¢‘å¹¶è¿›è¡Œå†…å®¹åˆ†æï¼Œç”Ÿæˆä¸­æ–‡æ€»ç»“\n"
            tools_description += "  - è§†é¢‘å°†ä¿å­˜åˆ°videos/æ–‡ä»¶å¤¹ï¼Œåœ¨Markdownä¸­å¼•ç”¨\n"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·æç¤ºè¯
        user_prompt_section = ""
        user_prompt = state.get('user_prompt')
        if self.config.enable_user_prompt and user_prompt:
            user_prompt_section = f"""

**ç”¨æˆ·ç‰¹åˆ«å…³æ³¨çš„æ–¹é¢:**
{user_prompt}

è¯·åœ¨åˆ†ææ—¶ç‰¹åˆ«å…³æ³¨ä»¥ä¸Šç”¨æˆ·æåˆ°çš„æ–¹é¢ï¼Œå¹¶åœ¨ç›¸åº”ç« èŠ‚ä¸­è¿›è¡Œæ·±å…¥åˆ†æã€‚
"""
        
        return f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶ã€‚ä½ æœ‰å›¾ç‰‡åˆ†æå·¥å…·{('å’Œè§†é¢‘åˆ†æå·¥å…·' if self.video_tool else '')}ï¼Œå¯ä»¥å¸®åŠ©ä½ ç†è§£è®ºæ–‡ä¸­çš„å›¾è¡¨ã€æ¶æ„å›¾ã€å®éªŒç»“æœ{('ä»¥åŠç›¸å…³çš„æ¼”ç¤ºè§†é¢‘' if self.video_tool else '')}ã€‚

**é‡è¦: æ‰€æœ‰åˆ†æç»“æœå¿…é¡»ä»¥æ ‡å‡†Markdownæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«å®Œæ•´çš„ç»“æ„ã€å…¬å¼ã€å›¾ç‰‡å¼•ç”¨ï¼Œå¹¶å°½å¯èƒ½æå–ä½œè€…ä¿¡æ¯ã€å•ä½å’Œé¡¹ç›®åœ°å€ã€‚è®ºæ–‡æ ‡é¢˜è¯·ç›´æ¥ä½¿ç”¨åŸæ–‡æ ‡é¢˜ï¼Œä¸è¦ç¿»è¯‘ã€‚æ‰€æœ‰ä¸“ä¸šåè¯è¯·ç›´æ¥ä¿ç•™åŸæ–‡ï¼Œä¸è¦ç¿»è¯‘ã€‚**
{user_prompt_section}
**å¯ç”¨å·¥å…·:**
{tools_description}

{('**è§†é¢‘ä½¿ç”¨è¯´æ˜:**' if self.video_tool else '')}
{('- æœ‰é¡¹ç›®é“¾æ¥å¿…é¡»è°ƒç”¨è§†é¢‘åˆ†æå·¥å…·è¿›è¡Œåˆ†æï¼Œå¹¶æ ¹æ®è§†é¢‘å†…å®¹é€‰æ‹©åˆé€‚çš„ä½ç½®æ’å…¥è§†é¢‘ï¼Œä¸è¦å›ºå®šåœ¨é¡¹ç›®ä¿¡æ¯éƒ¨åˆ†' if self.video_tool else '')}
{('- è§†é¢‘æ ¼å¼ï¼š<video controls width="100%"><source src="videos/è§†é¢‘æ–‡ä»¶å.mp4" type="video/mp4"></video>' if self.video_tool else '')}

**è®ºæ–‡å†…å®¹:**
{state['paper_text']}...

**Markdownè¾“å‡ºæ ¼å¼è¦æ±‚:**

1. **æ–‡æ¡£ç»“æ„**: ä½¿ç”¨æ ‡å‡†Markdownæ ‡é¢˜å±‚çº§ï¼ˆ#, ##, ###ç­‰ï¼‰
2. **ä½œè€…ä¿¡æ¯**: æå–å¹¶å±•ç¤ºä½œè€…å§“åã€å•ä½ï¼ˆå°¤å…¶æ˜¯ä¸€ä½œå•ä½ï¼‰ï¼Œå¦‚æœ‰é¡¹ç›®åœ°å€æˆ–æºç ä¹Ÿè¦åœ¨æ˜¾è‘—ä½ç½®æ ‡æ³¨ï¼ˆå¦‚GitHubã€é¡¹ç›®ä¸»é¡µç­‰ï¼‰
3. **æ•°å­¦å…¬å¼**: 
   - è¡Œé—´å…¬å¼ä½¿ç”¨ `$$...$$` 
   - è¡Œå†…å…¬å¼ä½¿ç”¨ `$...$`
   - ä¿ç•™è®ºæ–‡ä¸­çš„æ‰€æœ‰é‡è¦æ•°å­¦è¡¨è¾¾å¼
4. **å›¾ç‰‡å¼•ç”¨**: 
   - ä½¿ç”¨ `![å›¾ç‰‡æè¿°](å›¾ç‰‡è·¯å¾„)` è¯­æ³•
   - åœ¨åˆ†æé‡è¦å›¾è¡¨åï¼Œåœ¨é€‚å½“ä½ç½®æ’å…¥å›¾ç‰‡å¼•ç”¨
   - å›¾ç‰‡æè¿°åº”è¯¥å‡†ç¡®åæ˜ å›¾ç‰‡å†…å®¹å’Œé‡è¦æ€§
5. **è¡¨æ ¼**: ä½¿ç”¨Markdownè¡¨æ ¼è¯­æ³•å±•ç¤ºæ•°æ®
6. **åˆ—è¡¨**: ä½¿ç”¨`-`æˆ–æ•°å­—åˆ—è¡¨ç»„ç»‡ä¿¡æ¯
7. **ä»£ç **: å¦‚æœ‰ç®—æ³•æˆ–ä»£ç ï¼Œä½¿ç”¨```ä»£ç å—

**Markdownè¾“å‡ºæ¨¡æ¿ç»“æ„:**
```markdown
# è®ºæ–‡åŸæ–‡æ ‡é¢˜ï¼ˆè¯·ç›´æ¥å±•ç¤ºåŸå§‹æ ‡é¢˜ï¼Œä¸è¦ç¿»è¯‘ï¼‰

## 0. ä½œè€…ä¸é¡¹ç›®ä¿¡æ¯
- ä½œè€…: xxx ç­‰
- å•ä½: xxxå¤§å­¦/ç ”ç©¶æ‰€ï¼ˆè¯·æ ‡æ³¨ä¸€ä½œå•ä½ï¼‰
- é¡¹ç›®åœ°å€: [GitHub/ä¸»é¡µ/æºç é“¾æ¥](url)ï¼ˆå¦‚æœ‰è¯·æ ‡æ³¨ï¼‰

## 1. ç ”ç©¶èƒŒæ™¯ä¸ç›®æ ‡

## 2. ä¸»è¦è´¡çŒ®

## 3. æŠ€æœ¯æ–¹æ³•
ï¼ˆä¿ç•™é‡è¦æ•°å­¦å…¬å¼ï¼Œå¦‚ï¼š$$f(x) = \\sum_{{i=1}}^n w_i x_i$$ï¼‰

```

**æ€»ç»“çš„å†…å®¹ä¸é™äºä»¥ä¸Šå†…å®¹ï¼Œè¯·æ ¹æ®è®ºæ–‡å†…å®¹çµæ´»è°ƒæ•´ã€‚**

**æ‰§è¡ŒæŒ‡å—:**
1. ä»”ç»†é˜…è¯»è®ºæ–‡å†…å®¹ï¼Œè¯†åˆ«å…³é”®ä¿¡æ¯
2. ä¼˜å…ˆæå–ä½œè€…ã€å•ä½ã€é¡¹ç›®åœ°å€ç­‰å…ƒä¿¡æ¯
3. è®ºæ–‡æ ‡é¢˜è¯·ç›´æ¥ä½¿ç”¨åŸæ–‡æ ‡é¢˜ï¼Œä¸è¦ç¿»è¯‘
4. æ‰€æœ‰ä¸“ä¸šåè¯è¯·ç›´æ¥ä¿ç•™åŸæ–‡ï¼Œä¸è¦ç¿»è¯‘
5. å¯¹é‡è¦å›¾è¡¨ä½¿ç”¨analyze_imageå·¥å…·è¿›è¡Œæ·±å…¥åˆ†æ
{('6. å¦‚æœè®ºæ–‡åŒ…å«é¡¹ç›®åœ°å€æˆ–å¼€æºä»£ç ï¼Œå¿…é¡»è€ƒè™‘ä½¿ç”¨process_video_resourceså·¥å…·æœç´¢ç›¸å…³æ¼”ç¤ºè§†é¢‘' if self.video_tool else '')}
{('7. å°†åˆ†æç»“æœç»„ç»‡æˆæ ‡å‡†Markdownæ ¼å¼' if self.video_tool else '6. å°†åˆ†æç»“æœç»„ç»‡æˆæ ‡å‡†Markdownæ ¼å¼')}
{('8. ç¡®ä¿ä¿ç•™åŸæ–‡ä¸­çš„é‡è¦å…¬å¼å’Œæ•°æ®' if self.video_tool else '7. ç¡®ä¿ä¿ç•™åŸæ–‡ä¸­çš„é‡è¦å…¬å¼å’Œæ•°æ®')}
{('9. åœ¨é€‚å½“ä½ç½®å¼•ç”¨åˆ†æè¿‡çš„å›¾ç‰‡å’Œè§†é¢‘' if self.video_tool else '8. åœ¨é€‚å½“ä½ç½®å¼•ç”¨åˆ†æè¿‡çš„å›¾ç‰‡')}

ç°åœ¨å¼€å§‹ä½ çš„åˆ†æï¼Œè®°ä½è¾“å‡ºå¿…é¡»æ˜¯å®Œæ•´çš„ã€ç»“æ„åŒ–çš„Markdownæ–‡æ¡£ï¼ŒåŒ…å«æ‰€æœ‰é‡è¦çš„è§†è§‰å…ƒç´ ã€æ•°å­¦è¡¨è¾¾å¼å’Œå…ƒä¿¡æ¯ã€‚

**æ³¨æ„**: è¯·ç”¨ä¸­æ–‡è¿›è¡Œæ‰€æœ‰åˆ†æå’Œè¯´æ˜ï¼Œä½†éµå¾ªæ ‡å‡†Markdownè¯­æ³•æ ¼å¼ã€‚
"""
    
    
    def analyze_paper_folder(self, folder_path: str, thread_id: str = "1", 
                             user_prompt: Optional[str] = None) -> Dict[str, Any]:
        """
        åˆ†æè®ºæ–‡æ–‡ä»¶å¤¹çš„ä¸»å…¥å£
        
        Args:
            folder_path: è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
            thread_id: çº¿ç¨‹ID
            user_prompt: ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¯é€‰ï¼Œä¼šè¦†ç›–é…ç½®ä¸­çš„é»˜è®¤å€¼ï¼‰
            
        Returns:
            Dict: å®Œæ•´çš„åˆ†æç»“æœçŠ¶æ€
        """
        logger.info(f"å¼€å§‹åˆ†æè®ºæ–‡æ–‡ä»¶å¤¹: {folder_path}")
        
        try:
            # 0. é‡ç½® agent å®ä¾‹ä»¥ç¡®ä¿å…¨æ–°åˆ†æ
            reset_success = self.reset_agent_for_fresh_analysis()
            if not reset_success:
                logger.warning("âš ï¸ Agent é‡ç½®å¤±è´¥ï¼Œä½†ç»§ç»­åˆ†æ...")
            
            # 1. è§£ææ–‡ä»¶å¤¹å†…å®¹
            folder_data = self._parse_paper_folder(folder_path)
            
            # 2. åˆ›å»ºå›¾ç‰‡åˆ†æå·¥å…·
            logger.info("åˆ›å»ºå›¾ç‰‡åˆ†æå·¥å…·...")
            self.image_tool = create_image_analysis_tool(folder_path, self.config.vision_model)
            
            # 3. æ™ºèƒ½åˆå§‹åŒ–è§†é¢‘åˆ†æå·¥å…·ï¼ˆå¦‚æœå¯ç”¨ä¸”æ£€æµ‹åˆ°é¡¹ç›®ä¿¡æ¯ï¼‰
            logger.info("æ£€æŸ¥æ˜¯å¦éœ€è¦è§†é¢‘åˆ†æå·¥å…·...")
            self._initialize_video_tool_if_needed(folder_path, folder_data["paper_text"])
            
            # 4. åŠ¨æ€åˆ›å»ºå¸¦å·¥å…·çš„LLM
            tools = [self.image_tool]
            if self.video_tool:
                tools.append(self.video_tool)
                logger.info(f"  - è§†é¢‘åˆ†æå·¥å…·: {self.video_tool.name}")
            
            self.llm_with_tools = self.analysis_llm.bind_tools(tools)
            
            # 5. æ„å»ºå¹¶ç¼–è¯‘å®Œæ•´çš„å›¾
            logger.info("æ„å»º LangGraph å·¥ä½œæµ...")
            self._build_graph_with_tools(tools)
            
            logger.info(f"âœ… åˆå§‹åŒ–å®Œæˆ:")
            logger.info(f"  - å›¾ç‰‡åˆ†æå·¥å…·: {self.image_tool.name}")
            logger.info(f"  - å¯åˆ†æå›¾ç‰‡æ•°é‡: {len(folder_data['available_images'])}")
            logger.info(f"  - è§†è§‰æ¨¡å‹: {self.config.vision_model}")
            
            # 6. ç¡®å®šè¦ä½¿ç”¨çš„ç”¨æˆ·æç¤ºè¯
            # ä¼˜å…ˆä½¿ç”¨è¿è¡Œæ—¶ä¼ å…¥çš„æç¤ºè¯ï¼Œå…¶æ¬¡ä½¿ç”¨é…ç½®ä¸­çš„æç¤ºè¯
            effective_user_prompt = None
            if user_prompt:
                effective_user_prompt = user_prompt
                logger.info("ä½¿ç”¨è¿è¡Œæ—¶ä¼ å…¥çš„ç”¨æˆ·æç¤ºè¯")
            elif self.config.enable_user_prompt and self.config.user_prompt:
                effective_user_prompt = self.config.user_prompt
                logger.info("ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç”¨æˆ·æç¤ºè¯")
            
            if effective_user_prompt:
                logger.info(f"ç”¨æˆ·æç¤ºè¯é¢„è§ˆ: {effective_user_prompt[:100]}...")
            
            # 7. åˆ›å»ºåˆå§‹çŠ¶æ€
            initial_state: DeepPaperAnalysisState = {
                "base_folder_path": folder_path,
                "paper_text": folder_data["paper_text"],
                "available_images": folder_data["available_images"],
                "image_mappings": folder_data["image_mappings"],
                
                "messages": [],
                "analysis_result": None,
                "is_complete": False,
                "user_prompt": effective_user_prompt  # æ·»åŠ ç”¨æˆ·æç¤ºè¯åˆ°çŠ¶æ€
            }
            
            # 8. é…ç½®LangGraph
            config = RunnableConfig(
                configurable={"thread_id": thread_id},
                recursion_limit=100
            )
            
            # 9. æ‰§è¡Œåˆ†æ
            logger.info("å¼€å§‹æ‰§è¡ŒLangGraphå·¥ä½œæµ...")
            result = self.agent.invoke(initial_state, config)
            
            logger.info("è®ºæ–‡åˆ†æå®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"è®ºæ–‡åˆ†æå¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return {
                "error": f"åˆ†æå¤±è´¥: {str(e)}",
                "folder_path": folder_path
            }
        finally:
            # ç¡®ä¿æ¯æ¬¡åˆ†æåæ¸…ç†èµ„æº
            self._cleanup_analysis_resources()
    
    def cleanup(self) -> None:
        """ä¸»åŠ¨æ¸…ç†æ‰€æœ‰èµ„æº"""
        if self._is_cleaned_up:
            return
            
        logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†æ·±åº¦è®ºæ–‡åˆ†ææ™ºèƒ½ä½“èµ„æº...")
        
        try:
            # æ¸…ç†åˆ†æç›¸å…³èµ„æº
            self._cleanup_analysis_resources()
            
            # æ¸…ç†çº¿ç¨‹æ± æ‰§è¡Œå™¨
            if self._custom_executor and not self._custom_executor._shutdown:
                logger.info("å…³é—­è‡ªå®šä¹‰çº¿ç¨‹æ± æ‰§è¡Œå™¨...")
                self._custom_executor.shutdown(wait=False)
                
            # æ¸…ç†å†…å­˜ç®¡ç†å™¨
            if self.memory:
                logger.info("æ¸…ç†å†…å­˜ç®¡ç†å™¨...")
                self.memory = None
                
            self._is_cleaned_up = True
            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ èµ„æºæ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
    
    def _cleanup_analysis_resources(self) -> None:
        """æ¸…ç†å•æ¬¡åˆ†æçš„ç›¸å…³èµ„æº"""
        try:
            # æ¸…ç† LangGraph agent
            if self.agent:
                self.agent = None
                
            # æ¸…ç†å·¥å…·èŠ‚ç‚¹
            if self.tool_node:
                self.tool_node = None
                
            # æ¸…ç†ç»‘å®šçš„ LLM
            if self.llm_with_tools:
                self.llm_with_tools = None
                
            # æ¸…ç†å›¾ç‰‡å·¥å…·
            if self.image_tool:
                self.image_tool = None
                
        except Exception as e:
            logger.warning(f"âš ï¸ åˆ†æèµ„æºæ¸…ç†å¼‚å¸¸: {e}")
    
    def reset_agent_for_fresh_analysis(self) -> bool:
        """é‡ç½® agent å®ä¾‹ä»¥è¿›è¡Œå…¨æ–°åˆ†æï¼Œé˜²æ­¢çŠ¶æ€ç´¯ç§¯"""
        try:
            logger.info("ğŸ”„ é‡ç½® agent å®ä¾‹ä»¥è¿›è¡Œå…¨æ–°åˆ†æ...")
            
            # æ¸…ç†ç°æœ‰èµ„æº
            self._cleanup_analysis_resources()
            
            # é‡ç½®å†…å­˜ç®¡ç†å™¨ - åˆ›å»ºæ–°çš„å®ä¾‹
            if self.config.memory_enabled and self._custom_executor and not self._custom_executor._shutdown:
                try:
                    self.memory = MemorySaver()
                    logger.info("âœ… å†…å­˜ç®¡ç†å™¨å·²é‡ç½®")
                except Exception as e:
                    logger.warning(f"âš ï¸ å†…å­˜ç®¡ç†å™¨é‡ç½®å¤±è´¥ï¼Œå°†ä½¿ç”¨æ— çŠ¶æ€æ¨¡å¼: {e}")
                    self.memory = None
            
            logger.info("âœ… Agent é‡ç½®å®Œæˆï¼Œå‡†å¤‡è¿›è¡Œå…¨æ–°åˆ†æ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Agent é‡ç½®å¤±è´¥: {e}")
            return False
    
    def check_resource_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥èµ„æºå¥åº·çŠ¶æ€"""
        health_status = {
            "overall_healthy": True,
            "issues": [],
            "warnings": [],
            "custom_executor": {
                "available": False,
                "shutdown": True
            },
            "memory_manager": {
                "available": False,
                "enabled": self.config.memory_enabled
            },
            "analysis_llm": {
                "available": False
            }
        }
        
        try:
            # æ£€æŸ¥è‡ªå®šä¹‰æ‰§è¡Œå™¨çŠ¶æ€
            if self._custom_executor:
                health_status["custom_executor"]["available"] = True
                health_status["custom_executor"]["shutdown"] = self._custom_executor._shutdown
                if self._custom_executor._shutdown:
                    health_status["issues"].append("è‡ªå®šä¹‰çº¿ç¨‹æ± æ‰§è¡Œå™¨å·²å…³é—­")
                    health_status["overall_healthy"] = False
            else:
                health_status["warnings"].append("æœªä½¿ç”¨è‡ªå®šä¹‰çº¿ç¨‹æ± æ‰§è¡Œå™¨")
            
            # æ£€æŸ¥å†…å­˜ç®¡ç†å™¨çŠ¶æ€
            if self.memory:
                health_status["memory_manager"]["available"] = True
            else:
                if self.config.memory_enabled:
                    health_status["warnings"].append("å†…å­˜ç®¡ç†å™¨å·²ç¦ç”¨æˆ–ä¸å¯ç”¨")
            
            # æ£€æŸ¥ LLM çŠ¶æ€
            if self.analysis_llm:
                health_status["analysis_llm"]["available"] = True
            else:
                health_status["issues"].append("åˆ†æ LLM ä¸å¯ç”¨")
                health_status["overall_healthy"] = False
            
            # æ£€æŸ¥æ˜¯å¦å·²æ¸…ç†
            if self._is_cleaned_up:
                health_status["issues"].append("Agent å·²è¢«æ¸…ç†ï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ–")
                health_status["overall_healthy"] = False
        
        except Exception as e:
            health_status["issues"].append(f"å¥åº·æ£€æŸ¥å¼‚å¸¸: {str(e)}")
            health_status["overall_healthy"] = False
        
        return health_status
    
    def analyze_paper_folder_with_fallback(self, folder_path: str, thread_id: str = "1", 
                                           user_prompt: Optional[str] = None) -> Dict[str, Any]:
        """
        å¸¦é™çº§å¤„ç†çš„è®ºæ–‡åˆ†ææ–¹æ³•
        
        å¦‚æœæ ‡å‡†åˆ†æå¤±è´¥ï¼Œä¼šå°è¯•é™çº§å¤„ç†ï¼š
        1. ç¦ç”¨å†…å­˜ç®¡ç†
        2. é‡æ–°åˆ›å»º agent å®ä¾‹
        3. ç®€åŒ–åˆ†ææµç¨‹
        
        Args:
            folder_path: è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
            thread_id: çº¿ç¨‹ID
            user_prompt: ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¯é€‰ï¼‰
        """
        # é¦–å…ˆæ£€æŸ¥èµ„æºå¥åº·çŠ¶æ€
        health = self.check_resource_health()
        if not health["overall_healthy"]:
            logger.warning(f"âš ï¸ èµ„æºå¥åº·æ£€æŸ¥å‘ç°é—®é¢˜: {health['issues']}")
            for warning in health["warnings"]:
                logger.warning(f"âš ï¸ {warning}")
        
        try:
            # å°è¯•æ ‡å‡†åˆ†æ
            return self.analyze_paper_folder(folder_path, thread_id, user_prompt)
            
        except Exception as primary_error:
            logger.error(f"âŒ æ ‡å‡†åˆ†æå¤±è´¥: {primary_error}")
            
            # å°è¯•é™çº§å¤„ç†
            logger.info("ğŸ”„ å°è¯•é™çº§å¤„ç†...")
            try:
                return self._fallback_analysis(folder_path, thread_id, primary_error, user_prompt)
            except Exception as fallback_error:
                logger.error(f"âŒ é™çº§å¤„ç†ä¹Ÿå¤±è´¥: {fallback_error}")
                return {
                    "error": f"åˆ†æå®Œå…¨å¤±è´¥ - ä¸»è¦é”™è¯¯: {str(primary_error)}, é™çº§é”™è¯¯: {str(fallback_error)}",
                    "folder_path": folder_path,
                    "health_status": health
                }
    
    def _fallback_analysis(self, folder_path: str, thread_id: str, original_error: Exception,
                          user_prompt: Optional[str] = None) -> Dict[str, Any]:
        """é™çº§åˆ†æå¤„ç†"""
        logger.info("ğŸ“‹ æ‰§è¡Œé™çº§åˆ†æ...")
        
        try:
            # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰èµ„æº
            self._cleanup_analysis_resources()
            
            # ç¦ç”¨å†…å­˜ç®¡ç†
            original_memory_enabled = self.config.memory_enabled
            self.config.memory_enabled = False
            self.memory = None
            logger.info("âœ… å·²ç¦ç”¨å†…å­˜ç®¡ç†å™¨")
            
            # é‡æ–°è§£ææ–‡ä»¶å¤¹
            folder_data = self._parse_paper_folder(folder_path)
            
            # åˆ›å»ºç®€åŒ–çš„å›¾ç‰‡å·¥å…·
            self.image_tool = create_image_analysis_tool(folder_path, self.config.vision_model)
            self.llm_with_tools = self.analysis_llm.bind_tools([self.image_tool])
            
            # é‡æ–°æ„å»ºå›¾ï¼ˆæ— çŠ¶æ€æ¨¡å¼ï¼‰
            self._build_graph_with_tools([self.image_tool])
            
            # åˆ›å»ºç®€åŒ–çš„åˆå§‹çŠ¶æ€
            initial_state: DeepPaperAnalysisState = {
                "base_folder_path": folder_path,
                "paper_text": folder_data["paper_text"],
                "available_images": folder_data["available_images"],
                "image_mappings": folder_data["image_mappings"],
                "messages": [],
                "analysis_result": None,
                "is_complete": False,
                "user_prompt": user_prompt  # æ·»åŠ ç”¨æˆ·æç¤ºè¯
            }
            
            # ä½¿ç”¨ç®€åŒ–é…ç½®æ‰§è¡Œåˆ†æ
            config = RunnableConfig(
                configurable={"thread_id": f"{thread_id}_fallback"},
                recursion_limit=50  # é™ä½é€’å½’é™åˆ¶
            )
            
            logger.info("ğŸš€ å¼€å§‹é™çº§åˆ†æ...")
            result = self.agent.invoke(initial_state, config)
            
            # æ¢å¤åŸå§‹é…ç½®
            self.config.memory_enabled = original_memory_enabled
            
            # æ·»åŠ é™çº§æ ‡è®°
            result["fallback_used"] = True
            result["original_error"] = str(original_error)
            
            logger.info("âœ… é™çº§åˆ†æå®Œæˆ")
            return result
            
        except Exception as e:
            # æ¢å¤åŸå§‹é…ç½®
            self.config.memory_enabled = original_memory_enabled
            raise e
    
    @staticmethod
    def _cleanup_executor(executor):
        """é™æ€æ–¹æ³•ç”¨äº weakref.finalize æ¸…ç†æ‰§è¡Œå™¨"""
        try:
            if executor and not executor._shutdown:
                executor.shutdown(wait=False)
        except Exception:
            pass  # å¿½ç•¥æ¸…ç†æ—¶çš„å¼‚å¸¸
    
    def __del__(self):
        """ææ„å‡½æ•°ç¡®ä¿èµ„æºé‡Šæ”¾"""
        try:
            self.cleanup()
        except Exception:
            pass  # å¿½ç•¥ææ„æ—¶çš„å¼‚å¸¸
    
    def _should_trigger_video_analysis(self, paper_content: str) -> bool:
        """æ£€æµ‹è®ºæ–‡æ˜¯å¦åŒ…å«é¡¹ç›®ç›¸å…³ä¿¡æ¯ï¼Œå†³å®šæ˜¯å¦å¯ç”¨è§†é¢‘åˆ†æ"""
        video_indicators = [
            "github.com", "gitlab.com", "bitbucket.org", "é¡¹ç›®åœ°å€", "æºç é“¾æ¥",
            "code available", "open source", "implementation", "repository", 
            "demo video", "é¡¹ç›®ä¸»é¡µ", "source code", "github", "code is available",
            "available at", "project page", "supplementary material"
        ]
        content_lower = paper_content.lower()
        return any(indicator.lower() in content_lower for indicator in video_indicators)
    
    def _initialize_video_tool_if_needed(self, folder_path: str, paper_content: str) -> None:
        """æ ¹æ®è®ºæ–‡å†…å®¹å’Œé…ç½®æ™ºèƒ½å†³å®šæ˜¯å¦å¯ç”¨è§†é¢‘åˆ†æ"""
        if not self.config.enable_video_analysis:
            logger.info("â„¹ï¸ è§†é¢‘åˆ†æåŠŸèƒ½æœªå¯ç”¨")
            return
        
        if not self._should_trigger_video_analysis(paper_content):
            logger.info("â„¹ï¸ è®ºæ–‡ä¸­æœªæ£€æµ‹åˆ°é¡¹ç›®ç›¸å…³ä¿¡æ¯ï¼Œè·³è¿‡è§†é¢‘åˆ†æ")
            return
        
        try:
            # åˆ›å»ºè§†é¢‘ç›®å½•
            video_dir = os.path.join(folder_path, "videos")
            os.makedirs(video_dir, exist_ok=True)
            
            # åˆ›å»ºè§†é¢‘åˆ†æå·¥å…·
            self.video_tool = VideoResourceProcessor(
                base_folder_path=folder_path,
                summarization_model=self.config.video_analysis_model
            )
            
            logger.info("âœ… æ£€æµ‹åˆ°é¡¹ç›®ä¿¡æ¯ï¼Œå·²å¯ç”¨è§†é¢‘åˆ†æåŠŸèƒ½")
            logger.info(f"   è§†é¢‘ä¿å­˜ç›®å½•: {video_dir}")
            logger.info(f"   ä½¿ç”¨æ¨¡å‹: {self.config.video_analysis_model}")
            
        except Exception as e:
            logger.error(f"âŒ è§†é¢‘åˆ†æå·¥å…·åˆ›å»ºå¤±è´¥: {e}")
            self.video_tool = None
    
    def _parse_paper_folder(self, folder_path: str) -> Dict[str, Any]:
        """è§£æè®ºæ–‡æ–‡ä»¶å¤¹ç»“æ„"""
        # ä½¿ç”¨ä¸“é—¨çš„è§£æå™¨
        parser = create_paper_folder_parser(folder_path)
        
        # éªŒè¯æ–‡ä»¶å¤¹å®Œæ•´æ€§
        validation = parser.validate_folder_integrity()
        if not validation["is_valid"]:
            logger.warning(f"æ–‡ä»¶å¤¹éªŒè¯å¤±è´¥: {validation['issues']}")
        
        # è§£ææ–‡ä»¶å¤¹
        parse_result = parser.parse_folder()
        
        # æå–éœ€è¦çš„ä¿¡æ¯
        return {
            "paper_text": parse_result["paper_text"],
            "available_images": parse_result["available_images"],
            "image_mappings": parse_result["image_mappings"],
            "latex_formulas": parse_result["latex_formulas"],
            "image_references": parse_result["image_references"],
            "content_sections": parse_result["content_sections"]
        }
    
    def get_config(self) -> DeepPaperAnalysisConfig:
        """è·å–å½“å‰é…ç½®"""
        return self.config
    
    def generate_markdown_report(self, analysis_result: Dict[str, Any], output_path: Optional[str] = None) -> str:
        """
        ç”Ÿæˆmarkdownåˆ†ææŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœçŠ¶æ€
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: markdownæŠ¥å‘Šå†…å®¹
        """
        logger.info("ç”Ÿæˆmarkdownåˆ†ææŠ¥å‘Š...")
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨ï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰
        formatter = create_markdown_formatter("zh")
        
        # ç”ŸæˆæŠ¥å‘Š
        report_content = formatter.format_analysis_report(analysis_result)
        
        # ä¿å­˜æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šäº†è·¯å¾„ï¼‰
        if output_path:
            success = formatter.save_report(report_content, output_path)
            if success:
                logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
            else:
                logger.error("æŠ¥å‘Šä¿å­˜å¤±è´¥")
        
        return report_content
    
    def analyze_and_generate_report(self, folder_path: str, output_path: Optional[str] = None, 
                                   thread_id: str = "1", user_prompt: Optional[str] = None) -> tuple[Dict[str, Any], str]:
        """
        å®Œæ•´çš„åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆæµç¨‹
        
        Args:
            folder_path: è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
            output_path: æŠ¥å‘Šè¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            thread_id: çº¿ç¨‹ID
            user_prompt: ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            tuple: (åˆ†æç»“æœ, markdownæŠ¥å‘Šå†…å®¹)
        """
        logger.info(f"å¼€å§‹å®Œæ•´çš„è®ºæ–‡åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆæµç¨‹: {folder_path}")
        
        # æ‰§è¡Œåˆ†æï¼ˆä¼ é€’ç”¨æˆ·æç¤ºè¯ï¼‰
        analysis_result = self.analyze_paper_folder(folder_path, thread_id, user_prompt)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_content = self.generate_markdown_report(analysis_result, output_path)
        
        logger.info("å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæˆ")
        return analysis_result, report_content


# ä¾¿æ·å‡½æ•°
def create_deep_paper_analysis_agent(
    analysis_model: str = "deepseek.DeepSeek_V3",
    vision_model: str = "ollama.Qwen2_5_VL_7B",
    **kwargs
) -> DeepPaperAnalysisAgent:
    """åˆ›å»ºæ·±åº¦è®ºæ–‡åˆ†æagentçš„ä¾¿æ·å‡½æ•°"""
    config = DeepPaperAnalysisConfig(
        analysis_model=analysis_model,
        vision_model=vision_model,
        **kwargs
    )
    return DeepPaperAnalysisAgent(config=config)


def create_robust_paper_analysis_agent(
    analysis_model: str = "deepseek.DeepSeek_V3",
    vision_model: str = "ollama.Qwen2_5_VL_7B",
    enable_memory: bool = True,
    **kwargs
) -> DeepPaperAnalysisAgent:
    """
    åˆ›å»ºå¸¦å¥å£®æ€§å¤„ç†çš„æ·±åº¦è®ºæ–‡åˆ†æagent
    
    è¿™ä¸ªç‰ˆæœ¬åŒ…å«ï¼š
    - å¢å¼ºçš„èµ„æºç®¡ç†
    - è‡ªåŠ¨é™çº§å¤„ç†
    - å¥åº·çŠ¶æ€ç›‘æ§
    - æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ
    """
    config = DeepPaperAnalysisConfig(
        analysis_model=analysis_model,
        vision_model=vision_model,
        memory_enabled=enable_memory,
        **kwargs
    )
    
    agent = DeepPaperAnalysisAgent(config=config)
    
    # æ·»åŠ å®‰å…¨åˆ†ææ–¹æ³•
    def safe_analyze(folder_path: str, thread_id: str = "1") -> Dict[str, Any]:
        """å®‰å…¨çš„åˆ†ææ–¹æ³•ï¼Œè‡ªåŠ¨ä½¿ç”¨é™çº§å¤„ç†"""
        return agent.analyze_paper_folder_with_fallback(folder_path, thread_id)
    
    # æ›¿æ¢é»˜è®¤åˆ†ææ–¹æ³•
    agent.safe_analyze_paper_folder = safe_analyze
    
    return agent


def create_video_enhanced_analysis_agent(
    analysis_model: str = "deepseek.DeepSeek_V3",
    vision_model: str = "ollama.Qwen2_5_VL_7B",
    video_analysis_model: str = "ollama.Qwen3_30B",
    **kwargs
) -> DeepPaperAnalysisAgent:
    """
    åˆ›å»ºå¸¦è§†é¢‘åˆ†æåŠŸèƒ½çš„æ·±åº¦è®ºæ–‡åˆ†æagent
    
    è¿™ä¸ªç‰ˆæœ¬åŒ…å«ï¼š
    - å›¾ç‰‡åˆ†æï¼šä½¿ç”¨æŒ‡å®šçš„è§†è§‰æ¨¡å‹åˆ†æè®ºæ–‡ä¸­çš„å›¾è¡¨
    - è§†é¢‘åˆ†æï¼šè‡ªåŠ¨æ£€æµ‹é¡¹ç›®ä¿¡æ¯å¹¶æœç´¢ç›¸å…³æ¼”ç¤ºè§†é¢‘
    - æ™ºèƒ½è§¦å‘ï¼šåªåœ¨è®ºæ–‡åŒ…å«é¡¹ç›®åœ°å€æ—¶å¯ç”¨è§†é¢‘åŠŸèƒ½
    - å®Œæ•´è¾“å‡ºï¼šMarkdownæŠ¥å‘ŠåŒ…å«å›¾ç‰‡å’Œè§†é¢‘å±•ç¤º
    
    Args:
        analysis_model: ä¸»åˆ†æLLMæ¨¡å‹
        vision_model: å›¾ç‰‡ç†è§£VLMæ¨¡å‹
        video_analysis_model: è§†é¢‘åˆ†ææ¨¡å‹
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
    
    Returns:
        DeepPaperAnalysisAgent: é…ç½®äº†è§†é¢‘åˆ†æåŠŸèƒ½çš„æ™ºèƒ½ä½“
    
    Example:
        # åˆ›å»ºå¸¦è§†é¢‘åˆ†æçš„agent
        agent = create_video_enhanced_analysis_agent()
        
        # åˆ†æåŒ…å«é¡¹ç›®ä¿¡æ¯çš„è®ºæ–‡
        result = agent.analyze_paper_folder("/path/to/paper/folder")
        
        # å¦‚æœè®ºæ–‡åŒ…å«GitHubé“¾æ¥ç­‰é¡¹ç›®ä¿¡æ¯ï¼Œä¼šè‡ªåŠ¨æœç´¢ç›¸å…³è§†é¢‘
        # è§†é¢‘å°†ä¿å­˜åˆ°è®ºæ–‡ç›®å½•ä¸‹çš„videos/æ–‡ä»¶å¤¹
        # åœ¨Markdownè¾“å‡ºä¸­ä¼šåŒ…å«è§†é¢‘å±•ç¤ºéƒ¨åˆ†
    """
    config = DeepPaperAnalysisConfig(
        analysis_model=analysis_model,
        vision_model=vision_model,
        enable_video_analysis=True,  # å¯ç”¨è§†é¢‘åˆ†æ
        video_analysis_model=video_analysis_model,
        **kwargs
    )
    return DeepPaperAnalysisAgent(config=config)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•åŸºç¡€agentåˆ›å»º
    agent = create_deep_paper_analysis_agent()
    print(f"æ·±åº¦è®ºæ–‡åˆ†æAgentåˆ›å»ºæˆåŠŸ")
    print(f"é…ç½®: {agent.get_config().__dict__}")
    
    # æµ‹è¯•å¸¦è§†é¢‘åˆ†æçš„agentåˆ›å»º
    video_agent = create_video_enhanced_analysis_agent()
    print(f"\nè§†é¢‘å¢å¼ºè®ºæ–‡åˆ†æAgentåˆ›å»ºæˆåŠŸ")
    print(f"è§†é¢‘åˆ†æåŠŸèƒ½: {video_agent.get_config().enable_video_analysis}")
    print(f"è§†é¢‘åˆ†ææ¨¡å‹: {video_agent.get_config().video_analysis_model}")