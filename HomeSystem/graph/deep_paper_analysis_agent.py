"""
æ·±åº¦è®ºæ–‡åˆ†æAgent

åŸºäºLangGraphçš„è®ºæ–‡æ·±åº¦åˆ†ææ™ºèƒ½ä½“ï¼Œä½¿ç”¨æ ‡å‡†å·¥å…·è°ƒç”¨æ¨¡å¼ï¼š
1. äº‘ç«¯LLMä¸»å¯¼åˆ†æï¼Œè‡ªåŠ¨å†³ç­–ä½•æ—¶è°ƒç”¨å›¾ç‰‡åˆ†æå·¥å…·
2. ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆå®Œæ•´çš„åˆ†æç»“æœ
3. æ”¯æŒåŒè¯­åˆ†æç»“æœè¾“å‡º

é‡‡ç”¨æ ‡å‡†LangGraphå·¥å…·è°ƒç”¨æ¶æ„ï¼ŒLLMè‡ªä¸»å†³ç­–å·¥å…·ä½¿ç”¨ã€‚
"""

import json
from typing import Annotated, Any, Dict, List, Optional
from typing_extensions import TypedDict

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import AIMessage, SystemMessage, ToolMessage, BaseMessage
from loguru import logger

from .base_graph import BaseGraph
from .llm_factory import get_llm
from .tool.image_analysis_tool import create_image_analysis_tool
from .parser.paper_folder_parser import create_paper_folder_parser
from .formatter.markdown_formatter import create_markdown_formatter


class DeepPaperAnalysisState(TypedDict):
    """æ·±åº¦è®ºæ–‡åˆ†æçŠ¶æ€"""
    # è¾“å…¥æ•°æ®
    base_folder_path: str                           # è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
    paper_text: str                                 # è®ºæ–‡markdownæ–‡æœ¬
    available_images: List[str]                     # å¯ç”¨å›¾ç‰‡åˆ—è¡¨
    image_mappings: Dict[str, str]                  # å›¾ç‰‡è·¯å¾„æ˜ å°„
    
    # LangGraphæ¶ˆæ¯å†å²
    messages: Annotated[list, add_messages]         # å¯¹è¯å†å²
    
    # åˆ†æç»“æœ
    analysis_result: Optional[str]                  # æœ€ç»ˆåˆ†æç»“æœ
    
    # æ‰§è¡ŒçŠ¶æ€
    is_complete: bool                               # æ˜¯å¦å®Œæˆåˆ†æ


class DeepPaperAnalysisConfig:
    """æ·±åº¦è®ºæ–‡åˆ†æé…ç½®ç±»"""
    
    def __init__(self,
                 analysis_model: str = "deepseek.DeepSeek_V3",
                 vision_model: str = "ollama.llava", 
                 memory_enabled: bool = True,
                 custom_settings: Optional[Dict[str, Any]] = None):
        
        self.analysis_model = analysis_model          # ä¸»åˆ†æLLM
        self.vision_model = vision_model              # å›¾ç‰‡ç†è§£VLM
        self.memory_enabled = memory_enabled
        self.custom_settings = custom_settings or {}
    
    @classmethod
    def load_from_file(cls, config_path: str) -> "DeepPaperAnalysisConfig":
        """ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            return cls(**config_data)
        except Exception as e:
            logger.warning(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
            return cls()


class DeepPaperAnalysisAgent(BaseGraph):
    """æ·±åº¦è®ºæ–‡åˆ†ææ™ºèƒ½ä½“
    
    åŠŸèƒ½ï¼š
    1. ä½¿ç”¨æ ‡å‡†LangGraphå·¥å…·è°ƒç”¨æ¨¡å¼
    2. äº‘ç«¯LLMè‡ªä¸»å†³ç­–å·¥å…·ä½¿ç”¨
    3. ç»“æ„åŒ–è¾“å‡ºå’ŒåŒè¯­æ”¯æŒ
    """
    
    def __init__(self,
                 config: Optional[DeepPaperAnalysisConfig] = None,
                 config_path: Optional[str] = None,
                 **kwargs):
        
        super().__init__(**kwargs)
        
        # åŠ è½½é…ç½®
        if config_path:
            self.config = DeepPaperAnalysisConfig.load_from_file(config_path)
        elif config:
            self.config = config
        else:
            self.config = DeepPaperAnalysisConfig()
        
        logger.info(f"åˆå§‹åŒ–æ·±åº¦è®ºæ–‡åˆ†ææ™ºèƒ½ä½“")
        logger.info(f"åˆ†ææ¨¡å‹: {self.config.analysis_model}")
        logger.info(f"è§†è§‰æ¨¡å‹: {self.config.vision_model}")
        
        # åˆ›å»ºä¸»åˆ†æLLM
        self.analysis_llm = get_llm(self.config.analysis_model)
        
        # ç§»é™¤äº†ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½ï¼Œç®€åŒ–ä¸ºç›´æ¥æ–‡æœ¬è¾“å‡º
        
        # è®¾ç½®å†…å­˜ç®¡ç†
        self.memory = MemorySaver() if self.config.memory_enabled else None
        
        # å›¾ç‰‡åˆ†æå·¥å…·å°†åœ¨è¿è¡Œæ—¶åˆ›å»º
        self.image_tool = None
        self.llm_with_tools = None
        self.tool_node = None
        
        # æ„å»ºå›¾ï¼ˆå°†åœ¨åˆ†ææ—¶åŠ¨æ€å®Œæˆï¼‰
        self._graph_template = None
        self.agent = None
        
        logger.info("æ·±åº¦è®ºæ–‡åˆ†ææ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def _build_graph_with_tools(self, image_tool) -> None:
        """ä½¿ç”¨å·¥å…·æ„å»ºç®€åŒ–çš„LangGraphå·¥ä½œæµ"""
        graph = StateGraph(DeepPaperAnalysisState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("initialize", self._initialize_node)
        graph.add_node("analysis_with_tools", self._analysis_with_tools_node)
        # æ·»åŠ  tool_node
        self.tool_node = ToolNode([image_tool])
        graph.add_node("call_tools", self.tool_node)
        
        # æ„å»ºç®€åŒ–æµç¨‹
        graph.add_edge(START, "initialize")
        graph.add_edge("initialize", "analysis_with_tools")
        
        # åˆ†æå·¥å…·è°ƒç”¨çš„æ¡ä»¶åˆ†æ”¯
        graph.add_conditional_edges(
            "analysis_with_tools",
            self._should_continue_analysis,
            {
                "call_tools": "call_tools",  # è°ƒç”¨å·¥å…·
                "continue": "analysis_with_tools",  # ç»§ç»­åˆ†æ
                "end": END,  # åˆ†æå®Œæˆï¼Œç›´æ¥ç»“æŸ
            }
        )
        
        # å·¥å…·è°ƒç”¨åå›åˆ°åˆ†æèŠ‚ç‚¹
        graph.add_edge("call_tools", "analysis_with_tools")
        
        # ç¼–è¯‘å›¾
        try:
            self.agent = graph.compile(checkpointer=self.memory)
            logger.info("âœ… LangGraph å›¾ç¼–è¯‘æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ LangGraph å›¾ç¼–è¯‘å¤±è´¥: {e}")
            raise
    
    def _initialize_node(self, state: DeepPaperAnalysisState) -> Dict[str, Any]:
        """åˆå§‹åŒ–èŠ‚ç‚¹"""
        logger.info("âœ… å·¥å…·å·²åœ¨åˆ†æå¼€å§‹å‰åˆå§‹åŒ–")
        
        # åˆ›å»ºåˆå§‹åˆ†ææç¤º
        initial_prompt = self._generate_initial_analysis_prompt(state)
        
        return {
            "messages": [SystemMessage(content=initial_prompt)],
            "is_complete": False
        }
    
    def _analysis_with_tools_node(self, state: DeepPaperAnalysisState) -> Dict[str, Any]:
        """å¸¦å·¥å…·è°ƒç”¨çš„åˆ†æèŠ‚ç‚¹ - ä½¿ç”¨æ ‡å‡† LangGraph æ¨¡å¼"""
        logger.info("å¼€å§‹LLMåˆ†æ...")
        
        messages = state["messages"]
        
        try:
            # ç¡®ä¿llm_with_toolså·²åˆå§‹åŒ–
            if self.llm_with_tools is None:
                logger.error("âŒ LLM with tools not initialized")
                return {"messages": [AIMessage(content="LLMå·¥å…·æœªåˆå§‹åŒ–")]}
            
            # æ˜¾ç¤ºè¾“å…¥æ¶ˆæ¯çš„è¯¦ç»†ä¿¡æ¯
            logger.info(f"ğŸ“¤ å‘é€æ¶ˆæ¯ç»™ LLM:")
            logger.info(f"  - æ¶ˆæ¯æ•°é‡: {len(messages)}")
            for i, msg in enumerate(messages[-3:]):  # åªæ˜¾ç¤ºæœ€å3æ¡æ¶ˆæ¯
                msg_type = type(msg).__name__
                msg_preview = str(msg.content)[:100] if hasattr(msg, 'content') else str(msg)[:100]
                logger.info(f"  - æ¶ˆæ¯ {i}: {msg_type} - {msg_preview}...")
            
            # LLMè‡ªä¸»å†³ç­–å¹¶å¯èƒ½è°ƒç”¨å·¥å…·
            response = self.llm_with_tools.invoke(messages)
            
            # è¯¦ç»†æ£€æŸ¥å“åº”
            logger.info(f"ğŸ’¬ LLM å“åº”:")
            logger.info(f"  - å“åº”ç±»å‹: {type(response).__name__}")
            if hasattr(response, 'content'):
                content_preview = str(response.content)[:200] if response.content else "<empty>"
                logger.info(f"  - å†…å®¹é¢„è§ˆ: {content_preview}...")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            tool_calls = getattr(response, 'tool_calls', None)
            if tool_calls:
                logger.info(f"ğŸ”§ LLMå†³å®šè°ƒç”¨ {len(tool_calls)} ä¸ªå·¥å…·:")
                for i, tool_call in enumerate(tool_calls):
                    try:
                        if hasattr(tool_call, 'get'):
                            tool_name = tool_call.get('name', 'unknown')
                            tool_args = tool_call.get('args', {})
                        else:
                            # å¤„ç†ä¸åŒçš„ tool_call å¯¹è±¡ç±»å‹
                            tool_name = getattr(tool_call, 'name', str(tool_call))
                            tool_args = getattr(tool_call, 'args', {})
                        
                        logger.info(f"  [{i+1}] å·¥å…·: {tool_name}")
                        if isinstance(tool_args, dict):
                            for key, value in tool_args.items():
                                value_preview = str(value)[:100] if len(str(value)) > 100 else str(value)
                                logger.info(f"      {key}: {value_preview}")
                        else:
                            logger.info(f"      å‚æ•°: {tool_args}")
                    except Exception as e:
                        logger.warning(f"      æ— æ³•è§£æå·¥å…·è°ƒç”¨ {i+1}: {e}")
            else:
                logger.info("ğŸš« LLMæœªè°ƒç”¨ä»»ä½•å·¥å…·")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„åˆ†æç»“æœ
                if hasattr(response, 'content') and response.content:
                    content = str(response.content)
                    # å¦‚æœå†…å®¹è¾ƒé•¿ä¸”ä¸æ˜¯å·¥å…·è°ƒç”¨ï¼Œå¯èƒ½æ˜¯æœ€ç»ˆåˆ†æç»“æœ
                    if len(content) > 1000:
                        logger.info(f"âœ… æ£€æµ‹åˆ°å®Œæ•´åˆ†æç»“æœ ({len(content)} å­—ç¬¦)")
                        return {
                            "messages": [response],
                            "analysis_result": content,
                            "is_complete": True
                        }
            
            return {"messages": [response]}
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æèŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            error_message = AIMessage(content=f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            return {"messages": [error_message]}
    
    def _should_continue_analysis(self, state: DeepPaperAnalysisState) -> str:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­åˆ†ææˆ–è°ƒç”¨å·¥å…·"""
        messages = state["messages"]
        
        logger.info(f"ğŸ”„ åˆ†ææ§åˆ¶æµ: æ£€æŸ¥ {len(messages)} æ¡æ¶ˆæ¯")
        
        # æ£€æŸ¥æœ€åçš„æ¶ˆæ¯
        if messages:
            last_message = messages[-1]
            last_msg_type = type(last_message).__name__
            logger.info(f"  - æœ€åæ¶ˆæ¯ç±»å‹: {last_msg_type}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ AI æ¶ˆæ¯å¹¶ä¸”åŒ…å«å·¥å…·è°ƒç”¨
            if isinstance(last_message, AIMessage):
                # ä½¿ç”¨ getattr å®‰å…¨æ£€æŸ¥ tool_calls å±æ€§
                tool_calls = getattr(last_message, 'tool_calls', None)
                if tool_calls:
                    logger.info(f"ğŸ”§ æ£€æµ‹åˆ° {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨ â†’ call_tools")
                    return "call_tools"
                
                # æ£€æŸ¥æ˜¯å¦LLMè¡¨ç¤ºåˆ†æå®Œæˆ
                content = last_message.content
                if isinstance(content, str):
                    content_lower = content.lower()
                    completion_keywords = [
                        "åˆ†æå®Œæˆ", "analysis complete", "å®Œæˆåˆ†æ",
                        "åˆ†æç»“æŸ", "analysis finished", "ç»“æŸåˆ†æ",
                        "analysis is complete", "finished analyzing"
                    ]
                    if any(keyword in content_lower for keyword in completion_keywords):
                        logger.info(f"âœ… LLMè¡¨ç¤ºåˆ†æå®Œæˆ â†’ end")
                        return "end"
                    
                    # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼Œå¦‚æœè¾ƒé•¿å¯èƒ½æ˜¯å®Œæ•´åˆ†æ
                    if len(content) > 2000:  # å†…å®¹è¾ƒé•¿ï¼Œå¯èƒ½å·²ç»å®Œæˆäº†åˆ†æ
                        logger.info(f"ğŸ“ å†…å®¹è¾ƒé•¿ ({len(content)} å­—ç¬¦)ï¼Œå¯èƒ½å·²å®Œæˆåˆ†æ â†’ end")
                        return "end"
            
            # å¦‚æœæ˜¯å·¥å…·æ¶ˆæ¯ï¼Œè®©LLMç»§ç»­å¤„ç†å·¥å…·ç»“æœ
            elif isinstance(last_message, ToolMessage):
                logger.info(f"ğŸ”§ æ”¶åˆ°å·¥å…·ç»“æœ â†’ continue")
                return "continue"
        
        # é˜²æ­¢æ— é™å¾ªç¯ï¼šæ£€æŸ¥æ¶ˆæ¯æ•°é‡
        if len(messages) > 15:  # å¢åŠ ä¸Šé™ï¼Œç»™æ›´å¤šæœºä¼šè¿›è¡Œå·¥å…·è°ƒç”¨
            logger.warning(f"âš ï¸ æ¶ˆæ¯æ•°é‡è¶…è¿‡é™åˆ¶ ({len(messages)}) â†’ end")
            return "end"
        
        # ç»Ÿè®¡å·¥å…·è°ƒç”¨æ¬¡æ•°
        tool_call_count = 0
        tool_message_count = 0
        for msg in messages:
            if isinstance(msg, AIMessage):
                tool_calls = getattr(msg, 'tool_calls', None)
                if tool_calls:
                    tool_call_count += len(tool_calls)
            elif isinstance(msg, ToolMessage):
                tool_message_count += 1
        
        logger.info(f"  - å·¥å…·è°ƒç”¨æ¬¡æ•°: {tool_call_count}, å·¥å…·å“åº”: {tool_message_count}")
        
        # å¦‚æœå·²ç»è¿›è¡Œäº†è¶³å¤Ÿçš„å·¥å…·è°ƒç”¨ï¼Œè€ƒè™‘ç»“æŸ
        if tool_call_count >= 3:  # å·²ç»è¿›è¡Œäº†å¤šæ¬¡å·¥å…·è°ƒç”¨
            logger.info(f"ğŸ”„ å·²è¿›è¡Œ {tool_call_count} æ¬¡å·¥å…·è°ƒç”¨ï¼Œè€ƒè™‘ç»“æŸåˆ†æ â†’ end")
            return "end"
        
        # é»˜è®¤ç»§ç»­åˆ†æ
        logger.info(f"ğŸ”„ ç»§ç»­åˆ†æ â†’ continue")
        return "continue"
    
    
    def _generate_initial_analysis_prompt(self, state: DeepPaperAnalysisState) -> str:
        """ç”Ÿæˆåˆå§‹åˆ†ææç¤ºè¯ - è¦æ±‚æ ‡å‡†Markdownè¾“å‡ºæ ¼å¼"""
        available_images = state.get('available_images', [])
        image_list = "\n".join([f"  - {img}" for img in available_images[:10]])  # æ˜¾ç¤ºå‰10ä¸ªå›¾ç‰‡
        if len(available_images) > 10:
            image_list += f"\n  ... and {len(available_images) - 10} more images"
        
        return f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶ã€‚ä½ æœ‰ä¸€ä¸ªå›¾ç‰‡åˆ†æå·¥å…·ï¼Œå¯ä»¥å¸®åŠ©ä½ ç†è§£è®ºæ–‡ä¸­çš„å›¾è¡¨ã€æ¶æ„å›¾å’Œå®éªŒç»“æœã€‚

**é‡è¦: æ‰€æœ‰åˆ†æç»“æœå¿…é¡»ä»¥æ ‡å‡†Markdownæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«å®Œæ•´çš„ç»“æ„ã€å…¬å¼ã€å›¾ç‰‡å¼•ç”¨ï¼Œå¹¶å°½å¯èƒ½æå–ä½œè€…ä¿¡æ¯ã€å•ä½å’Œé¡¹ç›®åœ°å€ã€‚è®ºæ–‡æ ‡é¢˜è¯·ç›´æ¥ä½¿ç”¨åŸæ–‡æ ‡é¢˜ï¼Œä¸è¦ç¿»è¯‘ã€‚æ‰€æœ‰ä¸“ä¸šåè¯è¯·ç›´æ¥ä¿ç•™åŸæ–‡ï¼Œä¸è¦ç¿»è¯‘ã€‚**

**å¯ç”¨å·¥å…·:**
- `analyze_image`: ç”¨äºåˆ†æè®ºæ–‡ä¸­çš„ä»»ä½•å›¾ç‰‡/å›¾è¡¨/è¡¨æ ¼/ç¤ºæ„å›¾
  - å½“ä½ éœ€è¦ç†è§£æ–‡æœ¬ä¸­å¼•ç”¨çš„è§†è§‰å†…å®¹æ—¶è°ƒç”¨æ­¤å·¥å…·
  - å§‹ç»ˆåˆ†æå…³é”®å›¾è¡¨ã€æ¶æ„å›¾ã€å®éªŒå›¾è¡¨å’Œé‡è¦è¡¨æ ¼
  - æä¾›å…·ä½“çš„åˆ†ææŸ¥è¯¢ï¼Œå¦‚"åˆ†æè¿™ä¸ªæ¶æ„å›¾å¹¶è¯†åˆ«ä¸»è¦ç»„ä»¶"æˆ–"ä»è¿™ä¸ªå®éªŒå›¾è¡¨ä¸­æå–æ€§èƒ½æŒ‡æ ‡"

**è®ºæ–‡å†…å®¹:**
{state['paper_text']}...

**Markdownè¾“å‡ºæ ¼å¼è¦æ±‚:**

1. **æ–‡æ¡£ç»“æ„**: ä½¿ç”¨æ ‡å‡†Markdownæ ‡é¢˜å±‚çº§ï¼ˆ#, ##, ###ç­‰ï¼‰
2. **ä½œè€…ä¿¡æ¯**: æå–å¹¶å±•ç¤ºä½œè€…å§“åã€å•ä½ï¼ˆå°¤å…¶æ˜¯ä¸€ä½œå•ä½ï¼‰ï¼Œå¦‚æœ‰é¡¹ç›®åœ°å€æˆ–æºç ä¹Ÿè¦åœ¨æ˜¾è‘—ä½ç½®æ ‡æ³¨ï¼ˆå¦‚GitHubã€é¡¹ç›®ä¸»é¡µç­‰ï¼‰
3. **æ•°å­¦å…¬å¼**: 
   - è¡Œé—´å…¬å¼ä½¿ç”¨ `$$...$$` 
   - è¡Œå†…å…¬å¼ä½¿ç”¨ `$...$`
   - ä¿ç•™è®ºæ–‡ä¸­çš„æ‰€æœ‰é‡è¦æ•°å­¦è¡¨è¾¾å¼
4. **å›¾ç‰‡å¼•ç”¨**: 
   - ä½¿ç”¨ `![å›¾ç‰‡æè¿°](å›¾ç‰‡è·¯å¾„)` è¯­æ³•
   - åœ¨åˆ†æé‡è¦å›¾è¡¨åï¼Œåœ¨é€‚å½“ä½ç½®æ’å…¥å›¾ç‰‡å¼•ç”¨
   - å›¾ç‰‡æè¿°åº”è¯¥å‡†ç¡®åæ˜ å›¾ç‰‡å†…å®¹å’Œé‡è¦æ€§
5. **è¡¨æ ¼**: ä½¿ç”¨Markdownè¡¨æ ¼è¯­æ³•å±•ç¤ºæ•°æ®
6. **åˆ—è¡¨**: ä½¿ç”¨`-`æˆ–æ•°å­—åˆ—è¡¨ç»„ç»‡ä¿¡æ¯
7. **ä»£ç **: å¦‚æœ‰ç®—æ³•æˆ–ä»£ç ï¼Œä½¿ç”¨```ä»£ç å—

**Markdownè¾“å‡ºæ¨¡æ¿ç»“æ„:**
```markdown
# è®ºæ–‡åŸæ–‡æ ‡é¢˜ï¼ˆè¯·ç›´æ¥å±•ç¤ºåŸå§‹æ ‡é¢˜ï¼Œä¸è¦ç¿»è¯‘ï¼‰

## 0. ä½œè€…ä¸é¡¹ç›®ä¿¡æ¯
- ä½œè€…: xxx ç­‰
- å•ä½: xxxå¤§å­¦/ç ”ç©¶æ‰€ï¼ˆè¯·æ ‡æ³¨ä¸€ä½œå•ä½ï¼‰
- é¡¹ç›®åœ°å€: [GitHub/ä¸»é¡µ/æºç é“¾æ¥](url)ï¼ˆå¦‚æœ‰è¯·æ ‡æ³¨ï¼‰

## 1. ç ”ç©¶èƒŒæ™¯ä¸ç›®æ ‡

## 2. ä¸»è¦è´¡çŒ®

## 3. æŠ€æœ¯æ–¹æ³•
### 3.1 æ ¸å¿ƒç®—æ³•
ï¼ˆä¿ç•™é‡è¦æ•°å­¦å…¬å¼ï¼Œå¦‚ï¼š$$f(x) = \sum_{{i=1}}^n w_i x_i$$ï¼‰

### 3.2 æ¶æ„è®¾è®¡
ï¼ˆæ’å…¥é‡è¦æ¶æ„å›¾ï¼š![ç³»ç»Ÿæ¶æ„å›¾](imgs/architecture.jpg)ï¼‰

## 4. å®éªŒç»“æœ
### 4.1 æ•°æ®é›†ä¸è®¾ç½®
### 4.2 æ€§èƒ½åˆ†æ
ï¼ˆæ’å…¥å®éªŒç»“æœå›¾è¡¨ï¼‰

## 5. å…³é”®å‘ç°

## 6. æ€»ç»“ä¸è¯„ä»·
```

**æ‰§è¡ŒæŒ‡å—:**
1. ä»”ç»†é˜…è¯»è®ºæ–‡å†…å®¹ï¼Œè¯†åˆ«å…³é”®ä¿¡æ¯
2. ä¼˜å…ˆæå–ä½œè€…ã€å•ä½ã€é¡¹ç›®åœ°å€ç­‰å…ƒä¿¡æ¯
3. è®ºæ–‡æ ‡é¢˜è¯·ç›´æ¥ä½¿ç”¨åŸæ–‡æ ‡é¢˜ï¼Œä¸è¦ç¿»è¯‘
4. æ‰€æœ‰ä¸“ä¸šåè¯è¯·ç›´æ¥ä¿ç•™åŸæ–‡ï¼Œä¸è¦ç¿»è¯‘
5. å¯¹é‡è¦å›¾è¡¨ä½¿ç”¨analyze_imageå·¥å…·è¿›è¡Œæ·±å…¥åˆ†æ
6. å°†åˆ†æç»“æœç»„ç»‡æˆæ ‡å‡†Markdownæ ¼å¼
7. ç¡®ä¿ä¿ç•™åŸæ–‡ä¸­çš„é‡è¦å…¬å¼å’Œæ•°æ®
8. åœ¨é€‚å½“ä½ç½®å¼•ç”¨åˆ†æè¿‡çš„å›¾ç‰‡

ç°åœ¨å¼€å§‹ä½ çš„åˆ†æï¼Œè®°ä½è¾“å‡ºå¿…é¡»æ˜¯å®Œæ•´çš„ã€ç»“æ„åŒ–çš„Markdownæ–‡æ¡£ï¼ŒåŒ…å«æ‰€æœ‰é‡è¦çš„è§†è§‰å…ƒç´ ã€æ•°å­¦è¡¨è¾¾å¼å’Œå…ƒä¿¡æ¯ã€‚

**æ³¨æ„**: è¯·ç”¨ä¸­æ–‡è¿›è¡Œæ‰€æœ‰åˆ†æå’Œè¯´æ˜ï¼Œä½†éµå¾ªæ ‡å‡†Markdownè¯­æ³•æ ¼å¼ã€‚
"""
    
    
    def analyze_paper_folder(self, folder_path: str, thread_id: str = "1") -> Dict[str, Any]:
        """
        åˆ†æè®ºæ–‡æ–‡ä»¶å¤¹çš„ä¸»å…¥å£
        
        Args:
            folder_path: è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
            thread_id: çº¿ç¨‹ID
            
        Returns:
            Dict: å®Œæ•´çš„åˆ†æç»“æœçŠ¶æ€
        """
        logger.info(f"å¼€å§‹åˆ†æè®ºæ–‡æ–‡ä»¶å¤¹: {folder_path}")
        
        try:
            # 1. è§£ææ–‡ä»¶å¤¹å†…å®¹
            folder_data = self._parse_paper_folder(folder_path)
            
            # 2. åˆ›å»ºå›¾ç‰‡åˆ†æå·¥å…·
            logger.info("åˆ›å»ºå›¾ç‰‡åˆ†æå·¥å…·...")
            self.image_tool = create_image_analysis_tool(folder_path, self.config.vision_model)
            
            # 3. åˆ›å»ºå¸¦å·¥å…·çš„LLM
            self.llm_with_tools = self.analysis_llm.bind_tools([self.image_tool])
            
            # 4. æ„å»ºå¹¶ç¼–è¯‘å®Œæ•´çš„å›¾
            logger.info("æ„å»º LangGraph å·¥ä½œæµ...")
            self._build_graph_with_tools(self.image_tool)
            
            logger.info(f"âœ… åˆå§‹åŒ–å®Œæˆ:")
            logger.info(f"  - å›¾ç‰‡åˆ†æå·¥å…·: {self.image_tool.name}")
            logger.info(f"  - å¯åˆ†æå›¾ç‰‡æ•°é‡: {len(folder_data['available_images'])}")
            logger.info(f"  - è§†è§‰æ¨¡å‹: {self.config.vision_model}")
            
            # 5. åˆ›å»ºåˆå§‹çŠ¶æ€
            initial_state: DeepPaperAnalysisState = {
                "base_folder_path": folder_path,
                "paper_text": folder_data["paper_text"],
                "available_images": folder_data["available_images"],
                "image_mappings": folder_data["image_mappings"],
                
                "messages": [],
                "analysis_result": None,
                "is_complete": False
            }
            
            # 6. é…ç½®LangGraph
            config = RunnableConfig(
                configurable={"thread_id": thread_id},
                recursion_limit=100
            )
            
            # 7. æ‰§è¡Œåˆ†æ
            logger.info("å¼€å§‹æ‰§è¡ŒLangGraphå·¥ä½œæµ...")
            result = self.agent.invoke(initial_state, config)
            
            logger.info("è®ºæ–‡åˆ†æå®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"è®ºæ–‡åˆ†æå¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return {
                "error": f"åˆ†æå¤±è´¥: {str(e)}",
                "folder_path": folder_path
            }
    
    def _parse_paper_folder(self, folder_path: str) -> Dict[str, Any]:
        """è§£æè®ºæ–‡æ–‡ä»¶å¤¹ç»“æ„"""
        # ä½¿ç”¨ä¸“é—¨çš„è§£æå™¨
        parser = create_paper_folder_parser(folder_path)
        
        # éªŒè¯æ–‡ä»¶å¤¹å®Œæ•´æ€§
        validation = parser.validate_folder_integrity()
        if not validation["is_valid"]:
            logger.warning(f"æ–‡ä»¶å¤¹éªŒè¯å¤±è´¥: {validation['issues']}")
        
        # è§£ææ–‡ä»¶å¤¹
        parse_result = parser.parse_folder()
        
        # æå–éœ€è¦çš„ä¿¡æ¯
        return {
            "paper_text": parse_result["paper_text"],
            "available_images": parse_result["available_images"],
            "image_mappings": parse_result["image_mappings"],
            "latex_formulas": parse_result["latex_formulas"],
            "image_references": parse_result["image_references"],
            "content_sections": parse_result["content_sections"]
        }
    
    def get_config(self) -> DeepPaperAnalysisConfig:
        """è·å–å½“å‰é…ç½®"""
        return self.config
    
    def generate_markdown_report(self, analysis_result: Dict[str, Any], output_path: Optional[str] = None) -> str:
        """
        ç”Ÿæˆmarkdownåˆ†ææŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœçŠ¶æ€
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: markdownæŠ¥å‘Šå†…å®¹
        """
        logger.info("ç”Ÿæˆmarkdownåˆ†ææŠ¥å‘Š...")
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨ï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰
        formatter = create_markdown_formatter("zh")
        
        # ç”ŸæˆæŠ¥å‘Š
        report_content = formatter.format_analysis_report(analysis_result)
        
        # ä¿å­˜æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šäº†è·¯å¾„ï¼‰
        if output_path:
            success = formatter.save_report(report_content, output_path)
            if success:
                logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
            else:
                logger.error("æŠ¥å‘Šä¿å­˜å¤±è´¥")
        
        return report_content
    
    def analyze_and_generate_report(self, folder_path: str, output_path: Optional[str] = None, thread_id: str = "1") -> tuple[Dict[str, Any], str]:
        """
        å®Œæ•´çš„åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆæµç¨‹
        
        Args:
            folder_path: è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
            output_path: æŠ¥å‘Šè¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            thread_id: çº¿ç¨‹ID
            
        Returns:
            tuple: (åˆ†æç»“æœ, markdownæŠ¥å‘Šå†…å®¹)
        """
        logger.info(f"å¼€å§‹å®Œæ•´çš„è®ºæ–‡åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆæµç¨‹: {folder_path}")
        
        # æ‰§è¡Œåˆ†æ
        analysis_result = self.analyze_paper_folder(folder_path, thread_id)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_content = self.generate_markdown_report(analysis_result, output_path)
        
        logger.info("å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæˆ")
        return analysis_result, report_content


# ä¾¿æ·å‡½æ•°
def create_deep_paper_analysis_agent(
    analysis_model: str = "deepseek.DeepSeek_V3",
    vision_model: str = "ollama.llava",
    **kwargs
) -> DeepPaperAnalysisAgent:
    """åˆ›å»ºæ·±åº¦è®ºæ–‡åˆ†æagentçš„ä¾¿æ·å‡½æ•°"""
    config = DeepPaperAnalysisConfig(
        analysis_model=analysis_model,
        vision_model=vision_model,
        **kwargs
    )
    return DeepPaperAnalysisAgent(config=config)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•agentåˆ›å»º
    agent = create_deep_paper_analysis_agent()
    print(f"æ·±åº¦è®ºæ–‡åˆ†æAgentåˆ›å»ºæˆåŠŸ")
    print(f"é…ç½®: {agent.get_config().__dict__}")