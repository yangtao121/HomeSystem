# ç§»é™¤ langchain_community ä¾èµ–ï¼Œç›´æ¥ä½¿ç”¨ ArXiv API
from loguru import logger
import pprint
from tqdm import tqdm
import os
import re
from datetime import datetime
import io
from typing import Optional

import requests
import xml.etree.ElementTree as ET
import urllib.parse
import time
import feedparser

# OCR ç›¸å…³å¯¼å…¥
from pix2text import Pix2Text
import fitz  # PyMuPDF
from PIL import Image
import numpy as np


class ArxivData:
    def __init__(self, result: dict):
        """
        ç”¨äºå­˜å‚¨å•æ¡arxiv çš„æœç´¢ç»“æœã€‚
        è¾“å…¥çš„ result å¿…é¡»åŒ…å«çš„ key å¦‚ä¸‹ï¼š
        - title: æ ‡é¢˜
        - link: é“¾æ¥
        - snippet: æ‘˜è¦
        - categories: åˆ†ç±»
        :param result: å•æ¡æœç´¢ç»“æœ
        :type result: dict
        """
        self.title = None
        self.link = None
        self.snippet = None
        self.categories = None

        for key, value in result.items():
            setattr(self, key, value)

        # è·å–pdfé“¾æ¥
        self.pdf_link = self.link.replace("abs", "pdf")

        self.pdf = None

        self.pdf_path = None

        # è®ºæ–‡çš„tag
        self.tag: list[str] = []
        
        # OCRè¯†åˆ«ç»“æœ
        self.ocr_result: Optional[str] = None
        
        # ç»“æ„åŒ–æ‘˜è¦å­—æ®µ
        self.research_background: Optional[str] = None
        self.research_objectives: Optional[str] = None
        self.methods: Optional[str] = None
        self.key_findings: Optional[str] = None
        self.conclusions: Optional[str] = None
        self.limitations: Optional[str] = None
        self.future_work: Optional[str] = None
        self.keywords: Optional[str] = None
        
        # è®ºæ–‡åˆ†æç›¸å…³å­—æ®µ
        self.abstract_is_relevant: bool = False
        self.abstract_relevance_score: float = 0.0
        self.abstract_analysis_justification: Optional[str] = None
        self.full_paper_analyzed: bool = False
        self.full_paper_is_relevant: Optional[bool] = None
        self.full_paper_relevance_score: Optional[float] = None
        self.full_paper_analysis_justification: Optional[str] = None
        self.paper_summarized: bool = False
        self.paper_summary: Optional[dict] = None
        self.final_is_relevant: bool = False
        self.final_relevance_score: float = 0.0
        self.search_query: Optional[str] = None
        
        # æå–ArXiv IDå’Œå‘å¸ƒæ—¶é—´
        self.arxiv_id = self._extract_arxiv_id()
        self.published_date = self._extract_published_date()

    def setTag(self, tag: list[str]):
        """
        è®¾ç½®è®ºæ–‡çš„tag
        """

        if not isinstance(tag, list):
            logger.error(
                f"The tag of the paper is not a list, but a {type(tag)}.")
            return
        self.tag = tag

    def _extract_arxiv_id(self) -> str:
        """
        ä»é“¾æ¥ä¸­æå–ArXiv ID
        """
        if not self.link:
            return None
        
        # ArXivé“¾æ¥æ ¼å¼: http://arxiv.org/abs/1909.03550v1
        match = re.search(r'arxiv\.org/abs/([0-9]{4}\.[0-9]{4,5})', self.link)
        if match:
            return match.group(1)
        return None

    def _extract_published_date(self) -> str:
        """
        ä»ArXiv IDä¸­æå–å‘å¸ƒæ—¥æœŸ
        ArXiv IDæ ¼å¼è¯´æ˜:
        - 2007å¹´3æœˆå‰: æ ¼å¼å¦‚ math.GT/0309136 (subject-class/YYMMnnn)
        - 2007å¹´4æœˆå: æ ¼å¼å¦‚ 0704.0001 æˆ– 1909.03550 (YYMM.NNNN)
        """
        if not self.arxiv_id:
            return "æœªçŸ¥æ—¥æœŸ"
        
        try:
            # æ–°æ ¼å¼ (2007å¹´4æœˆå): YYMM.NNNN
            if '.' in self.arxiv_id and len(self.arxiv_id.split('.')[0]) == 4:
                year_month = self.arxiv_id.split('.')[0]
                year = int(year_month[:2])
                month = int(year_month[2:4])
                
                # å¤„ç†å¹´ä»½ (07-99 è¡¨ç¤º 2007-2099, 00-06 è¡¨ç¤º 2000-2006)
                if year >= 7:
                    full_year = 2000 + year
                else:
                    full_year = 2000 + year
                
                # è°ƒæ•´å¹´ä»½é€»è¾‘ï¼š92-99æ˜¯1992-1999, 00-06æ˜¯2000-2006, 07-91æ˜¯2007-2091
                if year >= 92:
                    full_year = 1900 + year
                elif year <= 6:
                    full_year = 2000 + year
                else:
                    full_year = 2000 + year
                
                return f"{full_year}å¹´{month:02d}æœˆ"
            else:
                return "æ—¥æœŸæ ¼å¼ä¸æ”¯æŒ"
        except (ValueError, IndexError):
            return "æ—¥æœŸè§£æå¤±è´¥"

    def get_formatted_info(self) -> str:
        """
        è·å–æ ¼å¼åŒ–çš„è®ºæ–‡ä¿¡æ¯ï¼ŒåŒ…å«æ—¶é—´
        """
        return f"æ ‡é¢˜: {self.title}\nå‘å¸ƒæ—¶é—´: {self.published_date}\né“¾æ¥: {self.link}\næ‘˜è¦: {self.snippet}"

    def downloadPdf(self, save_path: str = None):
        """
        ä¸‹è½½PDFå¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„

        Args:
            save_path: PDFä¿å­˜è·¯å¾„
        Returns:
            bytes: PDFå†…å®¹
        Raises:
            RequestException: å½“ä¸‹è½½å¤±è´¥æ—¶æŠ›å‡º
            IOError: å½“æ–‡ä»¶ä¿å­˜å¤±è´¥æ—¶æŠ›å‡º
        """
        if not self.pdf_link:
            raise ValueError("PDFé“¾æ¥ä¸èƒ½ä¸ºç©º")

        try:
            # å‘é€HEADè¯·æ±‚è·å–æ–‡ä»¶å¤§å°
            head = requests.head(self.pdf_link)
            total_size = int(head.headers.get('content-length', 0))

            # ä½¿ç”¨æµå¼è¯·æ±‚ä¸‹è½½
            response = requests.get(self.pdf_link, stream=True)
            response.raise_for_status()  # æ£€æŸ¥å“åº”çŠ¶æ€

            # åˆå§‹åŒ–è¿›åº¦æ¡
            progress = 0
            chunk_size = 1024  # 1KB

            content = bytearray()

            # åŒæ—¶ä¸‹è½½åˆ°å†…å­˜å’Œä¿å­˜åˆ°æ–‡ä»¶
            # å»é™¤æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦
            pdf_title = (self.title or 'æ— æ ‡é¢˜').replace("/", "_")
            pdf_title = pdf_title.replace(":", "_")
            pdf_title = pdf_title.replace("*", "_")
            pdf_title = pdf_title.replace("?", "_")
            pdf_title = pdf_title.replace("\\", "_")
            pdf_title = pdf_title.replace("<", "_")
            pdf_title = pdf_title.replace(">", "_")
            pdf_title = pdf_title.replace("|", "_")

            # pdf_title = pdf_title.replace(" ", "_")

            # å¦‚æœæ²¡æœ‰æŒ‡å®šä¿å­˜è·¯å¾„ï¼Œåˆ™ä¸ä¿å­˜
            if save_path is None:
                with tqdm(total=total_size, desc="Downloading PDF", unit='B', unit_scale=True) as pbar:
                    for chunk in response.iter_content(chunk_size=chunk_size):
                        if chunk:
                            content.extend(chunk)
                            progress += len(chunk)
                            pbar.update(len(chunk))
            else:
                pdf_path = os.path.join(save_path, pdf_title + ".pdf")

                self.pdf_path = pdf_path

                with open(pdf_path, 'wb') as f, \
                        tqdm(total=total_size, desc="Downloading PDF", unit='B', unit_scale=True) as pbar:
                    for chunk in response.iter_content(chunk_size=chunk_size):
                        if chunk:
                            content.extend(chunk)
                            f.write(chunk)
                            progress += len(chunk)
                            pbar.update(len(chunk))

                logger.info(f"PDFå·²ä¿å­˜åˆ°: {pdf_path}")

            self.pdf = bytes(content)

            return self.pdf

        except requests.exceptions.RequestException as e:
            raise Exception(f"PDFä¸‹è½½å¤±è´¥: {str(e)}")
        except IOError as e:
            raise Exception(f"PDFä¿å­˜å¤±è´¥: {str(e)}")

    def clearPdf(self):
        """
        æ¸…ç©ºPDFå†…å®¹, é‡Šæ”¾å†…å­˜
        """
        self.pdf = None
    
    def performOCR(self, max_pages: int = 25, use_pix2text: bool = False) -> tuple[Optional[str], dict]:
        """
        å¯¹PDFè¿›è¡ŒOCRæ–‡å­—è¯†åˆ«ï¼Œé»˜è®¤ä½¿ç”¨PyMuPDFå¿«é€Ÿæå–ï¼Œå¯é€‰ä½¿ç”¨pix2texté«˜ç²¾åº¦è¯†åˆ«
        
        Args:
            max_pages: æœ€å¤§å¤„ç†é¡µæ•°ï¼Œé»˜è®¤25é¡µï¼ˆæ¶µç›–å¤§éƒ¨åˆ†æ­£å¸¸è®ºæ–‡ï¼‰
            use_pix2text: æ˜¯å¦ä½¿ç”¨pix2textè¿›è¡Œé«˜ç²¾åº¦OCRï¼Œé»˜è®¤Falseä½¿ç”¨PyMuPDF
            
        Returns:
            tuple: (OCRè¯†åˆ«ç»“æœæ–‡æœ¬, çŠ¶æ€ä¿¡æ¯å­—å…¸)
                - str: OCRè¯†åˆ«ç»“æœæ–‡æœ¬ï¼Œå¦‚æœå¤±è´¥è¿”å›None
                - dict: åŒ…å«çŠ¶æ€ä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š
                    - 'total_pages': æ€»é¡µæ•°
                    - 'processed_pages': å®é™…å¤„ç†é¡µæ•°
                    - 'is_oversized': æ˜¯å¦è¶…è¿‡é¡µæ•°é™åˆ¶ï¼ˆå¯èƒ½æ˜¯æ¯•ä¸šè®ºæ–‡ç­‰é•¿æ–‡æ¡£ï¼‰
                    - 'char_count': å®é™…æå–çš„å­—ç¬¦æ•°
                    - 'method': ä½¿ç”¨çš„OCRæ–¹æ³• ('pymupdf' æˆ– 'pix2text')
            
        Raises:
            ValueError: å½“PDFå†…å®¹ä¸ºç©ºæ—¶æŠ›å‡º
            Exception: å½“OCRå¤„ç†å¤±è´¥æ—¶æŠ›å‡º
        """
        if self.pdf is None:
            raise ValueError("PDFå†…å®¹ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨downloadPdfæ–¹æ³•ä¸‹è½½PDF")
        
        # å¦‚æœæ˜ç¡®è¦æ±‚ä½¿ç”¨pix2textï¼Œæˆ–è€…PyMuPDFæ–¹æ³•å¤±è´¥æ—¶å›é€€
        if use_pix2text:
            return self._performOCR_pix2text(max_pages)
        else:
            try:
                return self._performOCR_pymupdf(max_pages)
            except Exception as e:
                logger.warning(f"PyMuPDF OCRå¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°pix2text")
                return self._performOCR_pix2text(max_pages)
    
    def _performOCR_pymupdf(self, max_pages: int = 25) -> tuple[Optional[str], dict]:
        """
        ä½¿ç”¨PyMuPDFè¿›è¡Œå¿«é€Ÿæ–‡æœ¬æå–ï¼ˆé»˜è®¤æ–¹æ³•ï¼‰
        """
        import tempfile
        
        logger.info(f"å¼€å§‹ä½¿ç”¨PyMuPDFè¿›è¡Œæ–‡æœ¬æå–ï¼Œæœ€å¤§å¤„ç†{max_pages}é¡µ")
        
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                # ä¿å­˜PDFåˆ°ä¸´æ—¶æ–‡ä»¶
                tmp_pdf_path = os.path.join(temp_dir, 'input.pdf')
                with open(tmp_pdf_path, 'wb') as f:
                    f.write(self.pdf)
                
                # æ‰“å¼€PDFæ–‡æ¡£
                pdf_document = fitz.open(tmp_pdf_path)
                total_pages = len(pdf_document)
                
                logger.info(f"PDFæ€»é¡µæ•°: {total_pages}")
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºè¶…é•¿æ–‡æ¡£
                is_oversized = total_pages > max_pages
                if is_oversized:
                    logger.warning(f"æ–‡æ¡£é¡µæ•°({total_pages})è¶…è¿‡é™åˆ¶({max_pages})ï¼Œå°†åªå¤„ç†å‰{max_pages}é¡µ")
                
                # å†³å®šå¤„ç†çš„é¡µæ•°
                pages_to_process = min(max_pages, total_pages)
                
                # æå–æ–‡æœ¬
                all_content = []
                total_chars = 0
                
                for page_num in range(pages_to_process):
                    try:
                        page = pdf_document[page_num]
                        text = page.get_text()
                        
                        if text.strip():
                            # æ¸…ç†æ–‡æœ¬
                            clean_text = text.strip()
                            clean_text = re.sub(r'\n\s*\n', '\n\n', clean_text)  # è§„èŒƒåŒ–ç©ºè¡Œ
                            clean_text = re.sub(r'[ \t]+', ' ', clean_text)  # åˆå¹¶å¤šä½™ç©ºæ ¼
                            
                            if clean_text:
                                all_content.append(f"=== ç¬¬{page_num + 1}é¡µ ===\n{clean_text}")
                                total_chars += len(clean_text)
                                
                    except Exception as e:
                        logger.warning(f"å¤„ç†ç¬¬{page_num + 1}é¡µå¤±è´¥: {e}")
                        continue
                
                pdf_document.close()
                
                # æ„å»ºçŠ¶æ€ä¿¡æ¯
                status_info = {
                    'total_pages': total_pages,
                    'processed_pages': pages_to_process,
                    'is_oversized': is_oversized,
                    'char_count': total_chars,
                    'method': 'pymupdf'
                }
                
                if all_content:
                    self.ocr_result = "\n\n".join(all_content)
                    
                    status_msg = f"PyMuPDFæ–‡æœ¬æå–å®Œæˆï¼Œå¤„ç†äº† {pages_to_process}/{total_pages} é¡µï¼Œæå–æ–‡æœ¬ {total_chars} ä¸ªå­—ç¬¦"
                    if is_oversized:
                        status_msg += f" (æ–‡æ¡£è¶…é•¿ï¼Œå¯èƒ½æ˜¯æ¯•ä¸šè®ºæ–‡æˆ–ä¹¦ç±)"
                    
                    logger.info(status_msg)
                    return self.ocr_result, status_info
                else:
                    logger.warning("PyMuPDFæœªæå–åˆ°ä»»ä½•æ–‡æœ¬")
                    self.ocr_result = ""
                    return self.ocr_result, status_info
                    
        except Exception as e:
            error_msg = f"PyMuPDFå¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)
    
    def _performOCR_pix2text(self, max_pages: int = 25) -> tuple[Optional[str], dict]:
        """
        ä½¿ç”¨pix2textè¿›è¡Œé«˜ç²¾åº¦OCRè¯†åˆ«ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
        """
        try:
            import os
            import tempfile
            import shutil
            
            # å¼ºåˆ¶è®¾ç½®CPUæ¨¡å¼ï¼Œé¿å…CUDAç›¸å…³é”™è¯¯
            os.environ['CUDA_VISIBLE_DEVICES'] = ''
            os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
            os.environ['OMP_NUM_THREADS'] = '1'
            
            logger.info(f"å¼€å§‹å¯¹PDFè¿›è¡ŒOCRè¯†åˆ«ï¼Œä½¿ç”¨pix2textï¼Œæœ€å¤§å¤„ç†{max_pages}é¡µ")
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                # ä¿å­˜PDFåˆ°ä¸´æ—¶æ–‡ä»¶
                tmp_pdf_path = os.path.join(temp_dir, 'input.pdf')
                with open(tmp_pdf_path, 'wb') as f:
                    f.write(self.pdf)
                
                # ä½¿ç”¨PyMuPDFæ£€æŸ¥æ€»é¡µæ•°
                pdf_document = fitz.open(tmp_pdf_path)
                total_pages = len(pdf_document)
                pdf_document.close()
                
                logger.info(f"PDFæ€»é¡µæ•°: {total_pages}")
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºè¶…é•¿æ–‡æ¡£ï¼ˆå¯èƒ½æ˜¯æ¯•ä¸šè®ºæ–‡æˆ–ä¹¦ç±ï¼‰
                is_oversized = total_pages > max_pages
                if is_oversized:
                    logger.warning(f"æ–‡æ¡£é¡µæ•°({total_pages})è¶…è¿‡é™åˆ¶({max_pages})ï¼Œå¯èƒ½æ˜¯æ¯•ä¸šè®ºæ–‡æˆ–ä¹¦ç±ï¼Œå°†åªå¤„ç†å‰{max_pages}é¡µ")
                
                # å†³å®šå¤„ç†çš„é¡µæ•°
                pages_to_process = min(max_pages, total_pages)
                page_numbers = list(range(pages_to_process))
                
                logger.info(f"å°†å¤„ç†é¡µé¢: {page_numbers} (å…±{pages_to_process}é¡µ)")
                
                # ä½¿ç”¨å®˜æ–¹æ–¹æ³•ï¼šåˆå§‹åŒ–pix2textå¹¶è¯†åˆ«PDF
                try:
                    # ç¦ç”¨å¯èƒ½å¯¼è‡´é—®é¢˜çš„ç»„ä»¶ï¼Œåªä¿ç•™æ–‡æœ¬OCR
                    config = {
                        'text_ocr': {'enabled': True},
                        'layout': {'enabled': True},  # ç¦ç”¨å¸ƒå±€æ£€æµ‹
                        'formula': {'enabled': True},  # ç¦ç”¨å…¬å¼è¯†åˆ«
                        'table': {'enabled': True},   # ç¦ç”¨è¡¨æ ¼è¯†åˆ«
                        'mfd': {'enabled': True}       # ç¦ç”¨æ•°å­¦å…¬å¼æ£€æµ‹
                    }
                    p2t = Pix2Text.from_config(config=config)
                    logger.info("ä½¿ç”¨ç®€åŒ–é…ç½®åˆå§‹åŒ–pix2textæˆåŠŸ")
                except Exception as e:
                    logger.warning(f"ç®€åŒ–é…ç½®åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°è¯•é»˜è®¤é…ç½®")
                    try:
                        # ä½¿ç”¨é»˜è®¤é…ç½®
                        p2t = Pix2Text()
                        logger.info("ä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–pix2textæˆåŠŸ")
                    except Exception as e2:
                        logger.error(f"é»˜è®¤é…ç½®ä¹Ÿå¤±è´¥: {e2}")
                        raise Exception(f"pix2textåˆå§‹åŒ–å¤±è´¥: {e2}")
                
                doc = p2t.recognize_pdf(tmp_pdf_path, page_numbers=page_numbers)
                
                # å¯¼å‡ºmarkdownåˆ°ä¸´æ—¶ç›®å½•
                # output_md_dir = os.path.join(temp_dir, 'output-md')

                # ä¿å­˜åˆ°å½“å‰ç›®å½•ä¸‹ç”¨äºdebug

                output_md_dir = os.path.join(os.getcwd(), 'output-md')
                doc.to_markdown(output_md_dir)
                
                logger.info(f"markdownæ–‡ä»¶å·²å¯¼å‡ºåˆ°: {output_md_dir}")
                
                # è¯»å–ç”Ÿæˆçš„markdownæ–‡ä»¶ï¼Œä¸é™åˆ¶å­—ç¬¦æ•°
                all_content = []
                total_chars = 0
                
                if os.path.exists(output_md_dir):
                    # éå†æ‰€æœ‰markdownæ–‡ä»¶
                    for root, dirs, files in os.walk(output_md_dir):
                        # æŒ‰æ–‡ä»¶åæ’åºä»¥ä¿æŒé¡µé¢é¡ºåº
                        md_files = sorted([f for f in files if f.endswith('.md')])
                        
                        for filename in md_files:
                            filepath = os.path.join(root, filename)
                            try:
                                with open(filepath, 'r', encoding='utf-8') as f:
                                    content = f.read().strip()
                                
                                if content:
                                    # æ¸…ç†markdownæ ¼å¼ï¼Œè½¬æ¢ä¸ºçº¯æ–‡æœ¬
                                    import re
                                    clean_content = content
                                    clean_content = re.sub(r'!\[.*?\]\(.*?\)', '', clean_content)  # ç§»é™¤å›¾ç‰‡
                                    clean_content = re.sub(r'\[([^\]]*)\]\([^\)]*\)', r'\1', clean_content)  # ä¿ç•™é“¾æ¥æ–‡æœ¬
                                    clean_content = re.sub(r'#{1,6}\s*', '', clean_content)  # ç§»é™¤æ ‡é¢˜æ ‡è®°
                                    clean_content = re.sub(r'\*{1,2}(.*?)\*{1,2}', r'\1', clean_content)  # ç§»é™¤ç²—ä½“/æ–œä½“
                                    clean_content = re.sub(r'`{1,3}(.*?)`{1,3}', r'\1', clean_content)  # ç§»é™¤ä»£ç æ ‡è®°
                                    clean_content = re.sub(r'\n\s*\n', '\n\n', clean_content)  # è§„èŒƒåŒ–ç©ºè¡Œ
                                    clean_content = clean_content.strip()
                                    
                                    if clean_content:
                                        content_chars = len(clean_content)
                                        all_content.append(f"=== {filename} ===\n{clean_content}")
                                        total_chars += content_chars
                                            
                            except Exception as e:
                                logger.warning(f"è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                                continue
                
                # åˆå¹¶æ‰€æœ‰å†…å®¹å¹¶æ„å»ºçŠ¶æ€ä¿¡æ¯
                status_info = {
                    'total_pages': total_pages,
                    'processed_pages': pages_to_process,
                    'is_oversized': is_oversized,
                    'char_count': total_chars,
                    'method': 'pix2text'
                }
                
                if all_content:
                    self.ocr_result = "\n\n".join(all_content)
                    
                    # è®°å½•è¯¦ç»†ä¿¡æ¯
                    status_msg = f"pix2text OCRè¯†åˆ«å®Œæˆï¼Œå¤„ç†äº† {pages_to_process}/{total_pages} é¡µï¼Œæå–æ–‡æœ¬ {total_chars} ä¸ªå­—ç¬¦"
                    if is_oversized:
                        status_msg += f" (æ–‡æ¡£è¶…é•¿ï¼Œå¯èƒ½æ˜¯æ¯•ä¸šè®ºæ–‡æˆ–ä¹¦ç±)"
                    
                    logger.info(status_msg)
                    return self.ocr_result, status_info
                else:
                    logger.warning("pix2text OCRè¯†åˆ«æœªæå–åˆ°ä»»ä½•æ–‡æœ¬")
                    self.ocr_result = ""
                    return self.ocr_result, status_info
                
        except Exception as e:
            error_msg = f"pix2text OCRè¯†åˆ«å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)
    
    def getOcrResult(self) -> Optional[str]:
        """
        è·å–OCRè¯†åˆ«ç»“æœ
        
        Returns:
            str: OCRè¯†åˆ«ç»“æœï¼Œå¦‚æœæœªè¿›è¡ŒOCRè¯†åˆ«åˆ™è¿”å›None
        """
        return self.ocr_result
    
    def clearOcrResult(self):
        """
        æ¸…ç©ºOCRè¯†åˆ«ç»“æœï¼Œé‡Šæ”¾å†…å­˜
        """
        self.ocr_result = None

    def cleanup(self):
        """
        æ¸…ç†ArxivDataå¯¹è±¡çš„æ‰€æœ‰å†…éƒ¨æ•°æ®ï¼Œé‡Šæ”¾å†…å­˜
        åŒ…æ‹¬PDFæ•°æ®ã€OCRç»“æœã€ç»“æ„åŒ–åˆ†æå­—æ®µç­‰æ‰€æœ‰å¤§å†…å­˜å ç”¨çš„å±æ€§
        """
        # æ¸…ç†PDFç›¸å…³æ•°æ®
        self.pdf = None
        self.pdf_path = None
        
        # æ¸…ç†OCRç»“æœ
        self.ocr_result = None
        
        # æ¸…ç†ç»“æ„åŒ–æ‘˜è¦å­—æ®µ
        self.research_background = None
        self.research_objectives = None
        self.methods = None
        self.key_findings = None
        self.conclusions = None
        self.limitations = None
        self.future_work = None
        self.keywords = None
        
        # æ¸…ç†åˆ†æç»“æœ
        self.abstract_analysis_justification = None
        self.full_paper_analysis_justification = None
        self.paper_summary = None
        
        # æ¸…ç†æ ‡ç­¾å’Œå…¶ä»–åˆ—è¡¨æ•°æ®
        if hasattr(self, 'tag') and self.tag:
            self.tag.clear()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        logger.debug(f"ArxivDataå¯¹è±¡å·²æ¸…ç†: {self.title[:50] if self.title else 'unknown'}...")

    def clear_invalid_characters(self, string: str) -> str:
        """
        å»é™¤å­—ç¬¦ä¸²ä¸­çš„éæ³•å­—ç¬¦
        """
        invalid_characters = ['/', ':', '*', '?',
                              '\\', '<', '>', '|', ' ', '"', "'"]
        for char in invalid_characters:
            string = string.replace(char, '_')
        return string


class ArxivResult:

    def __init__(self, results: list[dict]):
        """
        æœç´¢ç»“æœçš„ä¿å­˜ç±»ã€‚

        :param results: æœç´¢ç»“æœ
        :type results: list[dict]
        """
        self.results = [ArxivData(result) for result in results]

        self.num_results = len(self.results)

    def __iter__(self):
        """
        å®ç°è¿­ä»£å™¨åè®®
        """
        return iter(self.results)
    
    def display_results(self, display_range: str = "all", max_display: int = 10, 
                       show_details: bool = True, show_summary: bool = True):
        """
        ç»“æ„åŒ–æ˜¾ç¤ºæœç´¢ç»“æœ
        
        :param display_range: æ˜¾ç¤ºèŒƒå›´ "all" æˆ– "limited"
        :type display_range: str
        :param max_display: å½“display_rangeä¸º"limited"æ—¶çš„æœ€å¤§æ˜¾ç¤ºæ•°é‡
        :type max_display: int  
        :param show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        :type show_details: bool
        :param show_summary: æ˜¯å¦æ˜¾ç¤ºæ‘˜è¦ç»Ÿè®¡
        :type show_summary: bool
        """
        if self.num_results == 0:
            print("ğŸ“‹ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
            return
            
        # ç¡®å®šæ˜¾ç¤ºæ•°é‡
        if display_range == "all":
            display_count = self.num_results
            range_text = "å…¨éƒ¨"
        else:
            display_count = min(max_display, self.num_results)
            range_text = f"å‰ {display_count}"
            
        # æ˜¾ç¤ºæ ‡é¢˜
        print("=" * 80)
        print(f"ğŸ“š ArXiv æœç´¢ç»“æœ - {range_text} {display_count} ç¯‡è®ºæ–‡")
        print("=" * 80)
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        if show_details:
            for i, paper in enumerate(self.results[:display_count], 1):
                self._display_single_paper(i, paper)
                if i < display_count:  # ä¸æ˜¯æœ€åä¸€ä¸ªåˆ™æ˜¾ç¤ºåˆ†éš”çº¿
                    print("-" * 80)
        
        # æ˜¾ç¤ºæ‘˜è¦ç»Ÿè®¡
        if show_summary:
            self._display_summary(display_count)
    
    def _display_single_paper(self, index: int, paper: ArxivData):
        """æ˜¾ç¤ºå•ä¸ªè®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯"""
        print(f"\nğŸ“„ è®ºæ–‡ {index}")
        print(f"ğŸ“Œ æ ‡é¢˜: {paper.title}")
        print(f"ğŸ”— ArXiv ID: {paper.arxiv_id or 'æœªçŸ¥'}")
        print(f"ğŸ“… å‘å¸ƒæ—¶é—´: {paper.published_date}")
        print(f"ğŸ·ï¸  åˆ†ç±»: {paper.categories}")
        print(f"ğŸŒ é“¾æ¥: {paper.link}")
        if paper.snippet:
            print(f"ğŸ“ æ‘˜è¦: {paper.snippet[:200]}..." if len(paper.snippet) > 200 else f"ğŸ“ æ‘˜è¦: {paper.snippet}")
        else:
            print("ğŸ“ æ‘˜è¦: æ— ")
        print(f"ğŸ“¥ PDF: {paper.pdf_link}")
        
        # æ˜¾ç¤ºæ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
        if paper.tag:
            print(f"ğŸ·ï¸  æ ‡ç­¾: {', '.join(paper.tag)}")
    
    def _display_summary(self, displayed_count: int):
        """æ˜¾ç¤ºæ‘˜è¦ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 80)
        print("ğŸ“Š æœç´¢æ‘˜è¦")
        print("=" * 80)
        print(f"ğŸ“‹ æ€»ç»“æœæ•°: {self.num_results}")
        print(f"ğŸ–¥ï¸  å·²æ˜¾ç¤º: {displayed_count}")
        
        if displayed_count < self.num_results:
            print(f"âš ï¸  å‰©ä½™æœªæ˜¾ç¤º: {self.num_results - displayed_count}")
        
        # å‘å¸ƒæ—¶é—´ç»Ÿè®¡
        if self.num_results > 0:
            date_counts = {}
            for paper in self.results:
                date = paper.published_date
                if date and date != "æœªçŸ¥æ—¥æœŸ":
                    date_counts[date] = date_counts.get(date, 0) + 1
            
            if date_counts:
                print(f"\nğŸ“ˆ å‘å¸ƒæ—¶é—´åˆ†å¸ƒ (å‰5):")
                sorted_dates = sorted(date_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                for date, count in sorted_dates:
                    print(f"   {date}: {count} ç¯‡")
        
        # åˆ†ç±»ç»Ÿè®¡  
        if self.num_results > 0:
            category_counts = {}
            for paper in self.results:
                categories = paper.categories.split(', ') if paper.categories else ['Unknown']
                for cat in categories:
                    category_counts[cat] = category_counts.get(cat, 0) + 1
            
            if category_counts:
                print(f"\nğŸ·ï¸  åˆ†ç±»åˆ†å¸ƒ (å‰5):")
                sorted_cats = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                for cat, count in sorted_cats:
                    print(f"   {cat}: {count} ç¯‡")
        
        print("=" * 80)
    
    def display_brief(self, max_display: int = 5):
        """ç®€æ´æ˜¾ç¤ºæ¨¡å¼ï¼Œåªæ˜¾ç¤ºæ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯"""
        if self.num_results == 0:
            print("ğŸ“‹ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
            return
            
        display_count = min(max_display, self.num_results)
        
        print("=" * 60)
        print(f"ğŸ“š ArXiv æœç´¢ç»“æœæ¦‚è§ˆ - å‰ {display_count} ç¯‡")
        print("=" * 60)
        
        for i, paper in enumerate(self.results[:display_count], 1):
            print(f"{i:2d}. {paper.published_date} | {(paper.title or 'æ— æ ‡é¢˜')[:60]}...")
            print(f"    ğŸ”— {paper.arxiv_id or 'æœªçŸ¥ID'} | ğŸ·ï¸ {paper.categories or 'æ— åˆ†ç±»'}")
            print()
        
        if display_count < self.num_results:
            print(f"... è¿˜æœ‰ {self.num_results - display_count} ç¯‡è®ºæ–‡æœªæ˜¾ç¤º")
        print("=" * 60)
    
    def display_titles_only(self, max_display: int = None):
        """ä»…æ˜¾ç¤ºæ ‡é¢˜åˆ—è¡¨"""
        if self.num_results == 0:
            print("ğŸ“‹ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
            return
            
        display_count = max_display if max_display else self.num_results
        display_count = min(display_count, self.num_results)
        
        print(f"ğŸ“œ è®ºæ–‡æ ‡é¢˜åˆ—è¡¨ ({display_count}/{self.num_results}):")
        print("-" * 50)
        
        for i, paper in enumerate(self.results[:display_count], 1):
            print(f"{i:3d}. {paper.title or 'æ— æ ‡é¢˜'}")
        
        if display_count < self.num_results:
            print(f"\n... è¿˜æœ‰ {self.num_results - display_count} ç¯‡è®ºæ–‡")
    
    def get_papers_by_date_range(self, start_year: int = None, end_year: int = None):
        """æ ¹æ®å‘å¸ƒå¹´ä»½ç­›é€‰è®ºæ–‡"""
        filtered_papers = []
        
        for paper in self.results:
            if paper.published_date and paper.published_date != "æœªçŸ¥æ—¥æœŸ":
                # æå–å¹´ä»½
                try:
                    year_match = re.search(r'(\d{4})å¹´', paper.published_date)
                    if year_match:
                        year = int(year_match.group(1))
                        
                        # æ£€æŸ¥å¹´ä»½èŒƒå›´
                        if start_year and year < start_year:
                            continue
                        if end_year and year > end_year:
                            continue
                        
                        filtered_papers.append(paper)
                except:
                    continue
        
        # åˆ›å»ºæ–°çš„ç»“æœå¯¹è±¡
        filtered_results = []
        for paper in filtered_papers:
            result_dict = {
                'title': paper.title,
                'link': paper.link, 
                'snippet': paper.snippet,
                'categories': paper.categories
            }
            filtered_results.append(result_dict)
        
        return ArxivResult(filtered_results)


class ArxivTool:
    def __init__(self, search_host: str = None):
        """
        ç›´æ¥ä½¿ç”¨ ArXiv API è¿›è¡Œæœç´¢ï¼Œä¸å†ä¾èµ– SearxNGã€‚

        :param search_host: ä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼Œä½†ä¸å†ä½¿ç”¨
        :type search_host: str
        """
        # ä¿ç•™å‚æ•°ä½†ä¸å†ä½¿ç”¨ï¼Œé¿å…ç ´åç°æœ‰è°ƒç”¨ä»£ç 
        self.search_host = search_host

    def arxivSearch(self, query: str,
                    num_results: int = 20,
                    sort_by: str = "relevance",
                    order: str = "desc",
                    max_results: int = None,
                    kwargs: dict = None,
                    use_direct_api: bool = True
                    ) -> ArxivResult:
        """
        ä½¿ç”¨ ArXiv API ç›´æ¥æœç´¢ï¼Œæ— é™åˆ¶ä¸”è·å–æœ€æ–°æ•°æ®ã€‚

        :param query: æœç´¢çš„æŸ¥è¯¢
        :type query: str
        :param num_results: è¿”å›çš„ç»“æœæ•°é‡
        :type num_results: int
        :param sort_by: æ’åºæ–¹å¼ï¼Œå¯é€‰ "relevance", "lastUpdatedDate", "submittedDate"
        :type sort_by: str
        :param order: æ’åºé¡ºåºï¼Œå¯é€‰ "asc" (å‡åº) æˆ– "desc" (é™åº)
        :type order: str
        :param max_results: ä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼Œä½†ä¸å†ä½¿ç”¨
        :type max_results: int
        :param kwargs: ä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼Œä½†ä¸å†ä½¿ç”¨
        :type kwargs: dict
        :param use_direct_api: ä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼Œæ€»æ˜¯ä½¿ç”¨ç›´æ¥API
        :type use_direct_api: bool
        :return: æœç´¢ç»“æœ
        :rtype: ArxivResult
        """
        
        # ç°åœ¨æ€»æ˜¯ä½¿ç”¨ç›´æ¥ArXiv API
        logger.info(f"ä½¿ç”¨ç›´æ¥ArXiv APIæœç´¢: {query}")
        
        return self.directArxivSearch(
            query=query,
            num_results=num_results,
            sort_by=sort_by,
            order="descending" if order == "desc" else "ascending"
        )

    # ç§»é™¤åˆ†é¡µæœç´¢æ–¹æ³•ï¼Œç›´æ¥APIæ”¯æŒå¤§é‡ç»“æœ

    def getLatestPapers(self, query: str, num_results: int = 20) -> ArxivResult:
        """
        è·å–æœ€æ–°çš„è®ºæ–‡ï¼ŒæŒ‰æäº¤æ—¥æœŸé™åºæ’åˆ—
        
        :param query: æœç´¢çš„æŸ¥è¯¢
        :type query: str
        :param num_results: è¿”å›çš„ç»“æœæ•°é‡
        :type num_results: int
        :return: æœç´¢ç»“æœ
        :rtype: ArxivResult
        """
        return self.arxivSearch(query=query, 
                               num_results=num_results,
                               sort_by="submittedDate", 
                               order="desc")

    def getRecentlyUpdated(self, query: str, num_results: int = 20) -> ArxivResult:
        """
        è·å–æœ€è¿‘æ›´æ–°çš„è®ºæ–‡ï¼ŒæŒ‰æ›´æ–°æ—¥æœŸé™åºæ’åˆ—
        
        :param query: æœç´¢çš„æŸ¥è¯¢
        :type query: str
        :param num_results: è¿”å›çš„ç»“æœæ•°é‡
        :type num_results: int
        :return: æœç´¢ç»“æœ
        :rtype: ArxivResult
        """
        return self.arxivSearch(query=query, 
                               num_results=num_results,
                               sort_by="lastUpdatedDate", 
                               order="desc")

    def searchWithHighLimit(self, query: str, num_results: int = 50, 
                           sort_by: str = "relevance", order: str = "desc",
                           max_single_request: int = 20) -> ArxivResult:
        """
        é«˜é™åˆ¶æœç´¢æ–¹æ³•ï¼Œå¯ä»¥è·å–æ›´å¤šç»“æœ
        
        :param query: æœç´¢æŸ¥è¯¢
        :param num_results: ç›®æ ‡ç»“æœæ•°é‡ï¼ˆå¯ä»¥å¾ˆå¤§ï¼‰
        :param sort_by: æ’åºæ–¹å¼
        :param order: æ’åºé¡ºåº
        :param max_single_request: å•æ¬¡è¯·æ±‚çš„æœ€å¤§ç»“æœæ•°
        :return: æœç´¢ç»“æœ
        """
        return self.arxivSearch(query=query, 
                               num_results=num_results,
                               sort_by=sort_by,
                               order=order,
                               max_results=max_single_request)

    def directArxivSearch(self, query: str, num_results: int = 20,
                         sort_by: str = "relevance", order: str = "descending") -> ArxivResult:
        """
        ç›´æ¥ä½¿ç”¨ArXiv APIè¿›è¡Œæœç´¢ï¼Œè·å–æœ€æ–°æ•°æ®
        
        :param query: æœç´¢æŸ¥è¯¢
        :param num_results: ç»“æœæ•°é‡
        :param sort_by: æ’åºæ–¹å¼ ("relevance", "lastUpdatedDate", "submittedDate")
        :param order: æ’åºé¡ºåº ("ascending", "descending")
        :return: æœç´¢ç»“æœ
        """
        # ArXiv API URL
        base_url = "http://export.arxiv.org/api/query"
        
        # æ˜ å°„æ’åºå‚æ•°
        sort_map = {
            "relevance": "relevance",
            "lastUpdatedDate": "lastUpdatedDate", 
            "submittedDate": "submittedDate"
        }
        
        params = {
            "search_query": query,
            "start": 0,
            "max_results": min(num_results, 2000),  # ArXiv APIé™åˆ¶
            "sortBy": sort_map.get(sort_by, "relevance"),
            "sortOrder": order
        }
        
        try:
            logger.info(f"ç›´æ¥è°ƒç”¨ArXiv APIæœç´¢: {query}")
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
            
            # è§£æRSS/Atomæ ¼å¼å“åº”
            feed = feedparser.parse(response.content)
            
            if not feed.entries:
                logger.warning("ArXiv APIæœªè¿”å›ç»“æœ")
                return ArxivResult([])
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            results = []
            for entry in feed.entries[:num_results]:
                # æå–åˆ†ç±»
                categories = []
                if hasattr(entry, 'tags'):
                    categories = [tag.term for tag in entry.tags]
                elif hasattr(entry, 'arxiv_primary_category'):
                    categories = [entry.arxiv_primary_category['term']]
                
                # æå–ä½œè€…ä¿¡æ¯
                authors = []
                if hasattr(entry, 'authors'):
                    authors = [author.name for author in entry.authors]
                elif hasattr(entry, 'author'):
                    authors = [entry.author]
                
                result = {
                    'title': entry.title,
                    'link': entry.link,
                    'snippet': entry.summary,
                    'categories': ', '.join(categories) if categories else 'Unknown',
                    'authors': ', '.join(authors) if authors else 'Unknown'
                }
                results.append(result)
            
            logger.info(f"ArXiv APIè¿”å› {len(results)} ä¸ªç»“æœ")
            return ArxivResult(results)
            
        except requests.exceptions.RequestException as e:
            logger.error(f"ArXiv APIè¯·æ±‚å¤±è´¥: {str(e)}")
            # å›é€€åˆ°SearxNGæœç´¢
            logger.info("å›é€€åˆ°SearxNGæœç´¢")
            return self.arxivSearch(query, num_results, sort_by, "desc" if order == "descending" else "asc")
        except Exception as e:
            logger.error(f"ArXiv APIè§£æå¤±è´¥: {str(e)}")
            return self.arxivSearch(query, num_results, sort_by, "desc" if order == "descending" else "asc")

    def getLatestPapersDirectly(self, query: str, num_results: int = 20) -> ArxivResult:
        """
        ç›´æ¥ä»ArXiv APIè·å–æœ€æ–°è®ºæ–‡
        """
        return self.directArxivSearch(query, num_results, "submittedDate", "descending")

    # ç§»é™¤SearxNGç›¸å…³æ–¹æ³•ï¼Œç°åœ¨å®Œå…¨ä½¿ç”¨ç›´æ¥API


if __name__ == "__main__":
    # ArXiv API å·¥å…·æµ‹è¯•å’Œç»“æ„åŒ–æ˜¾ç¤ºåŠŸèƒ½æ¼”ç¤º
    arxiv_tool = ArxivTool()
    
    print("=== ArXiv API å·¥å…·åŠŸèƒ½æµ‹è¯• ===")
    
    # æµ‹è¯•1: åŸºç¡€æœç´¢
    print("ğŸ” æµ‹è¯•1: åŸºç¡€æœç´¢ - machine learning (10ä¸ªç»“æœ)")
    results = arxiv_tool.arxivSearch(query="machine learning", num_results=10)
    print(f"âœ… æœç´¢å®Œæˆ: {results.num_results} ä¸ªç»“æœ\n")
    
    # æ¼”ç¤ºç»“æ„åŒ–æ˜¾ç¤º - é™åˆ¶æ˜¾ç¤ºå‰3ä¸ª
    print("ğŸ“‹ ç»“æ„åŒ–æ˜¾ç¤ºæ¼”ç¤º - å‰3ä¸ªç»“æœ:")
    results.display_results(display_range="limited", max_display=3)
    
    print("\n" + "="*80 + "\n")
    
    # æµ‹è¯•2: ç®€æ´æ˜¾ç¤ºæ¨¡å¼
    print("ğŸ” æµ‹è¯•2: æœ€æ–°è®ºæ–‡æœç´¢ - deep learning")
    latest_papers = arxiv_tool.getLatestPapersDirectly(query="deep learning", num_results=15)
    
    print("ğŸ“‹ ç®€æ´æ˜¾ç¤ºæ¼”ç¤º:")
    latest_papers.display_brief(max_display=5)
    
    print("\n" + "="*80 + "\n")
    
    # æµ‹è¯•3: ä»…æ ‡é¢˜æ¨¡å¼
    print("ğŸ” æµ‹è¯•3: ç¥ç»ç½‘ç»œæœç´¢")
    nn_results = arxiv_tool.arxivSearch(query="neural networks", num_results=20)
    
    print("ğŸ“‹ ä»…æ ‡é¢˜æ˜¾ç¤ºæ¼”ç¤º:")
    nn_results.display_titles_only(max_display=8)
    
    print("\n" + "="*80 + "\n")
    
    # æµ‹è¯•4: å®Œæ•´æ˜¾ç¤ºæ¨¡å¼ï¼ˆå°æ•°æ®é›†ï¼‰
    print("ğŸ” æµ‹è¯•4: è®¡ç®—æœºè§†è§‰æœç´¢")
    cv_results = arxiv_tool.arxivSearch(query="computer vision", num_results=5)
    
    print("ğŸ“‹ å®Œæ•´æ˜¾ç¤ºæ¼”ç¤º - æ˜¾ç¤ºå…¨éƒ¨:")
    cv_results.display_results(display_range="all", show_summary=True)
    
    print("\n" + "="*80 + "\n")
    
    # æ¼”ç¤ºå¹´ä»½ç­›é€‰åŠŸèƒ½
    if latest_papers.num_results > 0:
        print("ğŸ“‹ å¹´ä»½ç­›é€‰æ¼”ç¤º - ç­›é€‰2020å¹´åçš„è®ºæ–‡:")
        recent_papers = latest_papers.get_papers_by_date_range(start_year=2020)
        if recent_papers.num_results > 0:
            recent_papers.display_brief(max_display=50)
        else:
            print("   æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡")
    
    print("\n=== ğŸ‰ ArXiv API é‡æ„å®Œæˆï¼ç°åœ¨æ”¯æŒä¸°å¯Œçš„ç»“æ„åŒ–æ˜¾ç¤ºåŠŸèƒ½ ===")
    
    # OCR åŠŸèƒ½æµ‹è¯•
    print("\n" + "="*50)
    print("ğŸ” OCRåŠŸèƒ½æµ‹è¯•")
    print("="*50)
    
    if results.num_results > 0:
        test_paper = results.results[2]
        print(f"ğŸ“„ æµ‹è¯•è®ºæ–‡: {test_paper.title[:60]}...")
        
        try:
            # ä¸‹è½½PDF
            print("ğŸ“¥ ä¸‹è½½PDFä¸­...")
            test_paper.downloadPdf()
            
            # æ‰§è¡ŒOCRï¼ˆä¸é™åˆ¶å­—ç¬¦æ•°ï¼Œåªé™åˆ¶é¡µæ•°ï¼‰
            print("ğŸ” æ‰§è¡ŒOCRè¯†åˆ«...")
            ocr_result, status_info = test_paper.performOCR()
            
            if ocr_result:
                print(f"âœ… OCRå®Œæˆï¼Œæå–æ–‡æœ¬: {len(ocr_result)} å­—ç¬¦")
                print(f"ğŸ“„ å¤„ç†äº† {status_info['processed_pages']}/{status_info['total_pages']} é¡µ")
                if status_info['is_oversized']:
                    print("âš ï¸ æ–‡æ¡£è¶…é•¿ï¼Œå¯èƒ½æ˜¯æ¯•ä¸šè®ºæ–‡æˆ–ä¹¦ç±")
                print(f"ğŸ“ ç»“æœé¢„è§ˆ: {ocr_result}")
            else:
                print("âŒ OCRæœªæå–åˆ°æ–‡æœ¬")
                
        except Exception as e:
            print(f"âŒ OCRæµ‹è¯•å¤±è´¥: {str(e)}")
    
    print("="*50)
    
    # ä½¿ç”¨æŒ‡å—
    print("\nğŸ“– ç»“æ„åŒ–æ˜¾ç¤ºåŠŸèƒ½ä½¿ç”¨æŒ‡å—:")
    print("   results.display_results()           # å®Œæ•´æ˜¾ç¤ºæ‰€æœ‰ç»“æœ")
    print("   results.display_results('limited')  # é™åˆ¶æ˜¾ç¤ºå‰Nä¸ª") 
    print("   results.display_brief()             # ç®€æ´æ¨¡å¼")
    print("   results.display_titles_only()       # ä»…æ˜¾ç¤ºæ ‡é¢˜")
    print("   results.get_papers_by_date_range()  # æŒ‰å¹´ä»½ç­›é€‰")
