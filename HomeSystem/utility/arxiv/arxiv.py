# ç§»é™¤ langchain_community ä¾èµ–ï¼Œç›´æ¥ä½¿ç”¨ ArXiv API
from loguru import logger
import pprint
from tqdm import tqdm
import os
import re
from datetime import datetime
import io
from typing import Optional
from enum import Enum

import requests
import xml.etree.ElementTree as ET
import urllib.parse
import time
import feedparser

# åŸºç¡€PDFå¤„ç†å¯¼å…¥
import fitz  # PyMuPDF - åŸºç¡€åŒ…ï¼Œè‚¯å®šæœ‰çš„

# OCR ç›¸å…³å¯¼å…¥
try:
    from paddleocr import PPStructureV3
    from PIL import Image
    import numpy as np
    OCR_AVAILABLE = True
except ImportError:
    # OCR åŠŸèƒ½ä¸å¯ç”¨ï¼Œä½†ä¸å½±å“åŸºæœ¬åŠŸèƒ½
    OCR_AVAILABLE = False
    pass

from pathlib import Path


class ArxivSearchMode(Enum):
    """ArXivæœç´¢æ¨¡å¼æšä¸¾"""
    LATEST = "latest"                    # æœ€æ–°è®ºæ–‡ (æŒ‰æäº¤æ—¥æœŸé™åº)
    MOST_RELEVANT = "most_relevant"      # æœ€ç›¸å…³ (æŒ‰ç›¸å…³æ€§æ’åº)
    RECENTLY_UPDATED = "recently_updated" # æœ€è¿‘æ›´æ–° (æŒ‰æ›´æ–°æ—¥æœŸé™åº)
    DATE_RANGE = "date_range"            # æŒ‡å®šå¹´ä»½èŒƒå›´
    AFTER_YEAR = "after_year"            # æŸå¹´ä¹‹åçš„è®ºæ–‡


class ArxivData:
    def __init__(self, result: Optional[dict] = None):
        """
        ç”¨äºå­˜å‚¨å•æ¡arxiv çš„æœç´¢ç»“æœã€‚
        è¾“å…¥çš„ result å¯ä»¥åŒ…å«çš„ key å¦‚ä¸‹ï¼š
        - title: æ ‡é¢˜
        - link: é“¾æ¥
        - snippet: æ‘˜è¦
        - categories: åˆ†ç±»
        :param result: å•æ¡æœç´¢ç»“æœï¼Œå¯ä»¥ä¸ºNone
        :type result: dict or None
        """
        self.title = None
        self.link = None
        self.snippet = None
        self.categories = None
        self.authors = None

        if result is not None:
            for key, value in result.items():
                setattr(self, key, value)

        # è·å–pdfé“¾æ¥
        self.pdf_link = self.link.replace("abs", "pdf") if self.link else ""

        self.pdf = None

        self.pdf_path = None

        # è®ºæ–‡çš„tag
        self.tag: list[str] = []
        
        # OCRè¯†åˆ«ç»“æœ
        self.ocr_result: Optional[str] = None
        
        # PaddleOCRç»“æ„åŒ–è¯†åˆ«ç»“æœ
        self.paddle_ocr_result: Optional[str] = None
        self.paddle_ocr_images: dict = {}
        
        # ç»“æ„åŒ–æ‘˜è¦å­—æ®µ
        self.research_background: Optional[str] = None
        self.research_objectives: Optional[str] = None
        self.methods: Optional[str] = None
        self.key_findings: Optional[str] = None
        self.conclusions: Optional[str] = None
        self.limitations: Optional[str] = None
        self.future_work: Optional[str] = None
        self.keywords: Optional[str] = None
        
        # è®ºæ–‡åˆ†æç›¸å…³å­—æ®µ
        self.abstract_is_relevant: bool = False
        self.abstract_relevance_score: float = 0.0
        self.abstract_analysis_justification: Optional[str] = None
        self.full_paper_analyzed: bool = False
        self.full_paper_is_relevant: Optional[bool] = None
        self.full_paper_relevance_score: Optional[float] = None
        self.full_paper_analysis_justification: Optional[str] = None
        self.paper_summarized: bool = False
        self.paper_summary: Optional[dict] = None
        self.final_is_relevant: bool = False
        self.final_relevance_score: float = 0.0
        self.search_query: Optional[str] = None
        
        # æå–ArXiv IDå’Œå‘å¸ƒæ—¶é—´
        self.arxiv_id = self._extract_arxiv_id()
        self.published_date = self._extract_published_date()

    def set_pdf_path(self, pdf_path: str, extract_metadata: bool = True):
        """
        è®¾ç½®PDFæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºä»ç°æœ‰PDFæ–‡ä»¶åˆ›å»ºArxivDataå¯¹è±¡
        å¯é€‰æ‹©æ˜¯å¦è‡ªåŠ¨æå–è®ºæ–‡æ ‡é¢˜ã€ä½œè€…å’Œæ‘˜è¦
        
        :param pdf_path: PDFæ–‡ä»¶çš„è·¯å¾„
        :type pdf_path: str
        :param extract_metadata: æ˜¯å¦è‡ªåŠ¨æå–å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ï¼‰ï¼Œé»˜è®¤True
        :type extract_metadata: bool
        :return: è¿”å›selfä»¥æ”¯æŒé“¾å¼è°ƒç”¨
        :rtype: ArxivData
        """
        self.pdf_path = pdf_path
        
        # å°è¯•ä»æ–‡ä»¶åæå–arxiv_idï¼ˆå¦‚æœå½“å‰æ²¡æœ‰çš„è¯ï¼‰
        if not self.arxiv_id or self.arxiv_id == "":
            import os
            filename = os.path.basename(pdf_path)
            # å°è¯•åŒ¹é…ArXiv IDæ ¼å¼ï¼šYYMM.NNNN
            arxiv_match = re.search(r'(\d{4}\.\d{4,5})', filename)
            if arxiv_match:
                self.arxiv_id = arxiv_match.group(1)
                # æ›´æ–°å‘å¸ƒæ—¥æœŸ
                self.published_date = self._extract_published_date()
                logger.info(f"ä»æ–‡ä»¶åæå–ArXiv ID: {self.arxiv_id}")
        
        # åŠ è½½PDFåˆ°å†…å­˜ç”¨äºOCR
        try:
            with open(pdf_path, 'rb') as f:
                self.pdf = f.read()
            logger.info(f"æˆåŠŸåŠ è½½PDFæ–‡ä»¶: {pdf_path}")
            
            # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦æ‰§è¡ŒOCRå’Œå…ƒæ•°æ®æå–
            if extract_metadata:
                self._extract_metadata_from_pdf()
            else:
                logger.info("è·³è¿‡è‡ªåŠ¨å…ƒæ•°æ®æå–ï¼ˆextract_metadata=Falseï¼‰")
            
        except Exception as e:
            logger.error(f"åŠ è½½PDFæ–‡ä»¶å¤±è´¥: {pdf_path}, é”™è¯¯: {e}")
        
        return self
    
    def _extract_metadata_from_pdf(self):
        """
        ä»PDFä¸­æå–å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ï¼‰
        ä½¿ç”¨OCR + LLMçš„æ–¹å¼ï¼Œæ”¯æŒè¿œç¨‹OCRæœåŠ¡
        """
        if not self.pdf:
            logger.warning("PDFå†…å®¹ä¸ºç©ºï¼Œè·³è¿‡å…ƒæ•°æ®æå–")
            return
        
        try:
            logger.info("å¼€å§‹ä»PDFæå–å…ƒæ•°æ®...")
            
            # æ£€æŸ¥æ˜¯å¦é…ç½®äº†è¿œç¨‹OCRæœåŠ¡
            remote_endpoint = os.getenv('REMOTE_OCR_ENDPOINT')
            use_remote_ocr = bool(remote_endpoint)
            
            # æ ¹æ®OCRç±»å‹å†³å®šå¤„ç†é¡µæ•°
            if use_remote_ocr:
                # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®çš„æœ€å¤§é¡µæ•°
                ocr_max_pages = int(os.getenv('REMOTE_OCR_MAX_PAGES', '25'))
                logger.info(f"ğŸŒ å…ƒæ•°æ®æå–ä½¿ç”¨è¿œç¨‹OCRæœåŠ¡: {remote_endpoint} (æœ€å¤§é¡µæ•°: {ocr_max_pages})")
            else:
                # æœ¬åœ°OCRé™åˆ¶é¡µæ•°ä»¥èŠ‚çœèµ„æº
                ocr_max_pages = 3
                logger.info("ğŸ” å…ƒæ•°æ®æå–ä½¿ç”¨æœ¬åœ°PaddleOCR (é™åˆ¶3é¡µ)")
            
            # æ‰§è¡ŒOCRï¼Œä½¿ç”¨é…ç½®çš„é¡µæ•°å’ŒOCRæ–¹æ³•
            ocr_result, ocr_status = self.performOCR(
                max_pages=ocr_max_pages, 
                use_paddleocr=True, 
                use_remote_ocr=use_remote_ocr
            )
            
            if not ocr_result:
                logger.warning("OCRæå–å¤±è´¥ï¼Œè·³è¿‡å…ƒæ•°æ®æå–")
                return
            
            logger.info(f"OCRæå–æˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(ocr_result)}")
            
            # ä½¿ç”¨LLMæå–å…ƒæ•°æ®
            try:
                from .paper_metadata_extractor import PaperMetadataLLM
                
                extractor = PaperMetadataLLM()
                if not extractor.is_available():
                    logger.warning("LLMä¸å¯ç”¨ï¼Œè·³è¿‡å…ƒæ•°æ®æå–")
                    return
                
                metadata = extractor.extract_metadata(ocr_result)
                
                # è®¾ç½®æå–çš„å…ƒæ•°æ®åˆ°å¯¹è±¡å±æ€§
                if metadata.title:
                    self.title = metadata.title
                    logger.info(f"æå–åˆ°æ ‡é¢˜: {metadata.title[:100]}...")
                
                if metadata.authors:
                    self.authors = metadata.authors
                    logger.info(f"æå–åˆ°ä½œè€…: {metadata.authors}")
                
                if metadata.abstract:
                    self.snippet = metadata.abstract
                    logger.info(f"æå–åˆ°æ‘˜è¦: {metadata.abstract[:200]}...")
                
                logger.info("å…ƒæ•°æ®æå–å®Œæˆ")
                
            except ImportError as e:
                logger.error(f"å¯¼å…¥PaperMetadataLLMå¤±è´¥: {e}")
            except Exception as e:
                logger.error(f"LLMå…ƒæ•°æ®æå–å¤±è´¥: {e}")
            
        except Exception as e:
            logger.error(f"å…ƒæ•°æ®æå–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        
        # æ¸…ç†ä¸´æ—¶OCRç»“æœï¼Œåªä¿ç•™PDF
        self.paddle_ocr_result = None
        self.paddle_ocr_images = {}

    def load_from_pdf(self):
        """
        ä»è®¾ç½®çš„PDFè·¯å¾„åŠ è½½PDFå†…å®¹åˆ°å†…å­˜
        
        :return: æ˜¯å¦åŠ è½½æˆåŠŸ
        :rtype: bool
        """
        if not self.pdf_path:
            logger.error("æœªè®¾ç½®PDFè·¯å¾„ï¼Œæ— æ³•åŠ è½½")
            return False
        
        try:
            with open(self.pdf_path, 'rb') as f:
                self.pdf = f.read()
            logger.info(f"æˆåŠŸä» {self.pdf_path} åŠ è½½PDFå†…å®¹")
            return True
        except Exception as e:
            logger.error(f"ä» {self.pdf_path} åŠ è½½PDFå¤±è´¥: {e}")
            return False

    def fetch_metadata_from_link(self, link: str = None) -> bool:
        """
        ä»ArXivé“¾æ¥è·å–è®ºæ–‡å…ƒæ•°æ®å¹¶å¡«å……å¯¹è±¡å±æ€§
        
        :param link: ArXivè®ºæ–‡é“¾æ¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨self.link
        :type link: str
        :return: æ˜¯å¦æˆåŠŸè·å–å…ƒæ•°æ®
        :rtype: bool
        """
        # ä½¿ç”¨æä¾›çš„é“¾æ¥æˆ–è‡ªèº«çš„é“¾æ¥
        arxiv_link = link or self.link
        if not arxiv_link:
            logger.error("æ²¡æœ‰æä¾›ArXivé“¾æ¥")
            return False
        
        # ä»é“¾æ¥ä¸­æå–ArXiv ID
        arxiv_id = self._extract_arxiv_id_from_link(arxiv_link)
        if not arxiv_id:
            logger.error(f"æ— æ³•ä»é“¾æ¥ä¸­æå–ArXiv ID: {arxiv_link}")
            return False
        
        logger.info(f"ä»ArXivé“¾æ¥è·å–å…ƒæ•°æ®: {arxiv_link} (ID: {arxiv_id})")
        
        try:
            # ä½¿ç”¨ArXiv APIè·å–è®ºæ–‡ä¿¡æ¯
            base_url = "http://export.arxiv.org/api/query"
            params = {
                "search_query": f"id:{arxiv_id}",
                "start": 0,
                "max_results": 1
            }
            
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
            
            # è§£æRSS/Atomæ ¼å¼å“åº”
            import feedparser
            feed = feedparser.parse(response.content)
            
            if not feed.entries:
                logger.error(f"ArXiv APIæœªæ‰¾åˆ°è®ºæ–‡: {arxiv_id}")
                return False
            
            entry = feed.entries[0]
            
            # æå–åˆ†ç±»
            categories = []
            if hasattr(entry, 'tags'):
                categories = [tag.term for tag in entry.tags]
            elif hasattr(entry, 'arxiv_primary_category'):
                categories = [entry.arxiv_primary_category.get('term', '')]
            
            # æå–ä½œè€…ä¿¡æ¯
            authors = []
            if hasattr(entry, 'authors'):
                authors = [author.name for author in entry.authors]
            elif hasattr(entry, 'author'):
                authors = [entry.author]
            
            # å¡«å……å¯¹è±¡å±æ€§
            self.title = entry.title.strip() if hasattr(entry, 'title') else None
            self.link = entry.link if hasattr(entry, 'link') else arxiv_link
            self.snippet = entry.summary.strip() if hasattr(entry, 'summary') else None
            self.categories = ', '.join(categories) if categories else 'Unknown'
            
            # è®¾ç½®PDFé“¾æ¥
            if self.link:
                self.pdf_link = self.link.replace("abs", "pdf")
            
            # æ›´æ–°ArXiv IDå’Œå‘å¸ƒæ—¥æœŸ
            self.arxiv_id = arxiv_id
            self.published_date = self._extract_published_date()
            
            # å¦‚æœæœ‰ä½œè€…ä¿¡æ¯ï¼Œä¿å­˜åˆ°authorså±æ€§ï¼ˆå¦‚æœæ²¡æœ‰è¿™ä¸ªå±æ€§åˆ™æ·»åŠ ï¼‰
            if authors:
                self.authors = ', '.join(authors)
            
            logger.info(f"æˆåŠŸè·å–è®ºæ–‡å…ƒæ•°æ®: {self.title}")
            return True
            
        except requests.exceptions.RequestException as e:
            logger.error(f"ArXiv APIè¯·æ±‚å¤±è´¥: {str(e)}")
            return False
        except Exception as e:
            logger.error(f"è§£æArXiv APIå“åº”å¤±è´¥: {str(e)}")
            return False

    def _extract_arxiv_id_from_link(self, link: str) -> str:
        """
        ä»ArXivé“¾æ¥ä¸­æå–è®ºæ–‡ID
        æ”¯æŒçš„æ ¼å¼:
        - https://arxiv.org/abs/2503.21460
        - http://arxiv.org/abs/2503.21460v1
        - arxiv.org/abs/2503.21460
        
        :param link: ArXivé“¾æ¥
        :type link: str
        :return: ArXiv IDï¼Œå¦‚æœè§£æå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
        :rtype: str
        """
        if not link:
            return ""
        
        # åŒ¹é…ArXivé“¾æ¥ä¸­çš„ID
        # æ”¯æŒæ ¼å¼: YYMM.NNNN æˆ– subject-class/YYMMnnn
        patterns = [
            r'arxiv\.org/abs/([0-9]{4}\.[0-9]{4,5})',  # æ–°æ ¼å¼: 2503.21460
            r'arxiv\.org/abs/([a-z-]+/[0-9]{7})',      # æ—§æ ¼å¼: math-ph/0309045
            r'arxiv\.org/abs/([0-9]{4}\.[0-9]{4,5}v[0-9]+)',  # å¸¦ç‰ˆæœ¬å·: 2503.21460v1
        ]
        
        for pattern in patterns:
            match = re.search(pattern, link)
            if match:
                arxiv_id = match.group(1)
                # å»æ‰ç‰ˆæœ¬å·ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if 'v' in arxiv_id and not '/' in arxiv_id:
                    arxiv_id = re.sub(r'v[0-9]+$', '', arxiv_id)
                return arxiv_id
        
        return ""

    @classmethod
    def from_arxiv_link(cls, link: str) -> 'ArxivData':
        """
        ä»ArXivé“¾æ¥åˆ›å»ºArxivDataå¯¹è±¡å¹¶è·å–å…ƒæ•°æ®
        
        :param link: ArXivè®ºæ–‡é“¾æ¥
        :type link: str
        :return: å¡«å……äº†å…ƒæ•°æ®çš„ArxivDataå¯¹è±¡
        :rtype: ArxivData
        :raises ValueError: å½“é“¾æ¥æ— æ•ˆæˆ–è·å–å…ƒæ•°æ®å¤±è´¥æ—¶æŠ›å‡º
        """
        # åˆ›å»ºç©ºå¯¹è±¡
        paper = cls()
        
        # è·å–å…ƒæ•°æ®
        success = paper.fetch_metadata_from_link(link)
        if not success:
            raise ValueError(f"æ— æ³•ä»ArXivé“¾æ¥è·å–å…ƒæ•°æ®: {link}")
        
        return paper

    def setTag(self, tag: list[str]):
        """
        è®¾ç½®è®ºæ–‡çš„tag
        """

        if not isinstance(tag, list):
            logger.error(
                f"The tag of the paper is not a list, but a {type(tag)}.")
            return
        self.tag = tag

    def _extract_arxiv_id(self) -> str:
        """
        ä»é“¾æ¥ä¸­æå–ArXiv ID
        """
        if not self.link:
            return ""
        
        # ArXivé“¾æ¥æ ¼å¼: http://arxiv.org/abs/1909.03550v1
        match = re.search(r'arxiv\.org/abs/([0-9]{4}\.[0-9]{4,5})', self.link)
        if match:
            return match.group(1)
        return ""

    def _extract_published_date(self) -> str:
        """
        ä»ArXiv IDä¸­æå–å‘å¸ƒæ—¥æœŸ
        ArXiv IDæ ¼å¼è¯´æ˜:
        - 2007å¹´3æœˆå‰: æ ¼å¼å¦‚ math.GT/0309136 (subject-class/YYMMnnn)
        - 2007å¹´4æœˆå: æ ¼å¼å¦‚ 0704.0001 æˆ– 1909.03550 (YYMM.NNNN)
        """
        if not self.arxiv_id:
            return "æœªçŸ¥æ—¥æœŸ"
        
        try:
            # æ–°æ ¼å¼ (2007å¹´4æœˆå): YYMM.NNNN
            if '.' in self.arxiv_id and len(self.arxiv_id.split('.')[0]) == 4:
                year_month = self.arxiv_id.split('.')[0]
                year = int(year_month[:2])
                month = int(year_month[2:4])
                
                # å¤„ç†å¹´ä»½ (07-99 è¡¨ç¤º 2007-2099, 00-06 è¡¨ç¤º 2000-2006)
                if year >= 7:
                    full_year = 2000 + year
                else:
                    full_year = 2000 + year
                
                # è°ƒæ•´å¹´ä»½é€»è¾‘ï¼š92-99æ˜¯1992-1999, 00-06æ˜¯2000-2006, 07-91æ˜¯2007-2091
                if year >= 92:
                    full_year = 1900 + year
                elif year <= 6:
                    full_year = 2000 + year
                else:
                    full_year = 2000 + year
                
                return f"{full_year}å¹´{month:02d}æœˆ"
            else:
                return "æ—¥æœŸæ ¼å¼ä¸æ”¯æŒ"
        except (ValueError, IndexError):
            return "æ—¥æœŸè§£æå¤±è´¥"

    def get_formatted_info(self) -> str:
        """
        è·å–æ ¼å¼åŒ–çš„è®ºæ–‡ä¿¡æ¯ï¼ŒåŒ…å«æ—¶é—´
        """
        return f"æ ‡é¢˜: {self.title}\nå‘å¸ƒæ—¶é—´: {self.published_date}\né“¾æ¥: {self.link}\næ‘˜è¦: {self.snippet}"

    def downloadPdf(self, save_path: Optional[str] = None, use_standard_path: bool = False, check_existing: bool = False):
        """
        ä¸‹è½½PDFå¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„

        Args:
            save_path: PDFä¿å­˜è·¯å¾„ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            use_standard_path: æ˜¯å¦ä½¿ç”¨æ ‡å‡†ç›®å½•ç»“æ„ï¼ˆå½“save_pathä¸ºNoneæ—¶ç”Ÿæ•ˆï¼‰
            check_existing: æ˜¯å¦æ£€æŸ¥æ–‡ä»¶å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤ä¸‹è½½ï¼‰
        Returns:
            bytes: PDFå†…å®¹
        Raises:
            RequestException: å½“ä¸‹è½½å¤±è´¥æ—¶æŠ›å‡º
            IOError: å½“æ–‡ä»¶ä¿å­˜å¤±è´¥æ—¶æŠ›å‡º
        """
        if not self.pdf_link:
            raise ValueError("PDFé“¾æ¥ä¸èƒ½ä¸ºç©º")

        # å†³å®šå®é™…çš„ä¿å­˜è·¯å¾„
        actual_save_path = None
        pdf_path = None
        
        if save_path is not None:
            # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„è·¯å¾„ï¼ˆä¿æŒåŸæœ‰è¡Œä¸ºï¼‰
            actual_save_path = save_path
            # å»é™¤æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦
            pdf_title = (self.title or 'æ— æ ‡é¢˜').replace("/", "_")
            pdf_title = pdf_title.replace(":", "_")
            pdf_title = pdf_title.replace("*", "_")
            pdf_title = pdf_title.replace("?", "_")
            pdf_title = pdf_title.replace("\\", "_")
            pdf_title = pdf_title.replace("<", "_")
            pdf_title = pdf_title.replace(">", "_")
            pdf_title = pdf_title.replace("|", "_")
            pdf_path = os.path.join(actual_save_path, pdf_title + ".pdf")
        elif use_standard_path:
            # ä½¿ç”¨æ ‡å‡†ç›®å½•ç»“æ„
            try:
                pdf_path = self.get_default_pdf_path()
                actual_save_path = pdf_path.parent
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                actual_save_path.mkdir(parents=True, exist_ok=True)
                pdf_path = str(pdf_path)  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²è·¯å¾„
            except ValueError as e:
                logger.error(f"æ— æ³•ä½¿ç”¨æ ‡å‡†è·¯å¾„: {e}")
                actual_save_path = None
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if check_existing and pdf_path and os.path.exists(pdf_path):
            logger.info(f"PDFæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {pdf_path}")
            # è¯»å–ç°æœ‰æ–‡ä»¶å¹¶è¿”å›å†…å®¹
            try:
                with open(pdf_path, 'rb') as f:
                    self.pdf = f.read()
                    self.pdf_path = pdf_path
                    return self.pdf
            except Exception as e:
                logger.warning(f"è¯»å–ç°æœ‰PDFæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°ä¸‹è½½")

        try:
            # å‘é€HEADè¯·æ±‚è·å–æ–‡ä»¶å¤§å°
            head = requests.head(self.pdf_link)
            total_size = int(head.headers.get('content-length', 0))

            # ä½¿ç”¨æµå¼è¯·æ±‚ä¸‹è½½
            response = requests.get(self.pdf_link, stream=True)
            response.raise_for_status()  # æ£€æŸ¥å“åº”çŠ¶æ€

            # åˆå§‹åŒ–è¿›åº¦æ¡
            progress = 0
            chunk_size = 1024  # 1KB

            content = bytearray()

            # å¦‚æœæ²¡æœ‰ä¿å­˜è·¯å¾„ï¼Œåˆ™åªä¸‹è½½åˆ°å†…å­˜
            if actual_save_path is None:
                with tqdm(total=total_size, desc="Downloading PDF", unit='B', unit_scale=True) as pbar:
                    for chunk in response.iter_content(chunk_size=chunk_size):
                        if chunk:
                            content.extend(chunk)
                            progress += len(chunk)
                            pbar.update(len(chunk))
            else:
                # åŒæ—¶ä¸‹è½½åˆ°å†…å­˜å’Œä¿å­˜åˆ°æ–‡ä»¶
                self.pdf_path = pdf_path

                if pdf_path:
                    with open(pdf_path, 'wb') as f, \
                            tqdm(total=total_size, desc="Downloading PDF", unit='B', unit_scale=True) as pbar:
                        for chunk in response.iter_content(chunk_size=chunk_size):
                            if chunk:
                                content.extend(chunk)
                                f.write(chunk)
                                progress += len(chunk)
                                pbar.update(len(chunk))

                    logger.info(f"PDFå·²ä¿å­˜åˆ°: {pdf_path}")

            self.pdf = bytes(content)

            return self.pdf

        except requests.exceptions.RequestException as e:
            raise Exception(f"PDFä¸‹è½½å¤±è´¥: {str(e)}")
        except IOError as e:
            raise Exception(f"PDFä¿å­˜å¤±è´¥: {str(e)}")

    def clearPdf(self):
        """
        æ¸…ç©ºPDFå†…å®¹, é‡Šæ”¾å†…å­˜
        """
        self.pdf = None
    
    def performOCR(self, max_pages: int = 25, use_paddleocr: bool = False, use_remote_ocr: bool = False, auto_save: bool = False, save_path: Optional[str] = None) -> tuple[Optional[str], dict]:
        """
        å¯¹PDFè¿›è¡ŒOCRæ–‡å­—è¯†åˆ«ï¼Œæ”¯æŒæœ¬åœ°PyMuPDF/PaddleOCRæˆ–è¿œç¨‹PaddleOCRæœåŠ¡
        
        Args:
            max_pages: æœ€å¤§å¤„ç†é¡µæ•°ï¼Œé»˜è®¤25é¡µï¼ˆæ¶µç›–å¤§éƒ¨åˆ†æ­£å¸¸è®ºæ–‡ï¼‰
            use_paddleocr: æ˜¯å¦ä½¿ç”¨æœ¬åœ°PaddleOCRè¿›è¡Œç»“æ„åŒ–è¯†åˆ«ï¼Œé»˜è®¤Falseä½¿ç”¨PyMuPDF
            use_remote_ocr: æ˜¯å¦ä½¿ç”¨è¿œç¨‹PaddleOCRæœåŠ¡ï¼Œé»˜è®¤False
            auto_save: æ˜¯å¦è‡ªåŠ¨ä¿å­˜OCRç»“æœåˆ°æ ‡å‡†ç›®å½•ï¼ˆå½“save_pathä¸ºNoneæ—¶ç”Ÿæ•ˆï¼‰
            save_path: æŒ‡å®šOCRç»“æœä¿å­˜ç›®å½•ï¼ˆä¼˜å…ˆçº§é«˜äºauto_saveï¼‰
            
        Returns:
            tuple: (OCRè¯†åˆ«ç»“æœæ–‡æœ¬, çŠ¶æ€ä¿¡æ¯å­—å…¸)
                - str: OCRè¯†åˆ«ç»“æœæ–‡æœ¬ï¼Œå¦‚æœå¤±è´¥è¿”å›None
                - dict: åŒ…å«çŠ¶æ€ä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š
                    - 'total_pages': æ€»é¡µæ•°
                    - 'processed_pages': å®é™…å¤„ç†é¡µæ•°
                    - 'is_oversized': æ˜¯å¦è¶…è¿‡é¡µæ•°é™åˆ¶ï¼ˆå¯èƒ½æ˜¯æ¯•ä¸šè®ºæ–‡ç­‰é•¿æ–‡æ¡£ï¼‰
                    - 'char_count': å®é™…æå–çš„å­—ç¬¦æ•°
                    - 'method': ä½¿ç”¨çš„OCRæ–¹æ³• ('pymupdf', 'paddleocr', æˆ– 'remote_paddleocr')
                    - 'saved_files': ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå½“å¯ç”¨ä¿å­˜æ—¶ï¼‰
            
        Raises:
            ValueError: å½“PDFå†…å®¹ä¸ºç©ºæ—¶æŠ›å‡º
            Exception: å½“OCRå¤„ç†å¤±è´¥æ—¶æŠ›å‡º
        """
        if self.pdf is None:
            raise ValueError("PDFå†…å®¹ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨downloadPdfæ–¹æ³•ä¸‹è½½PDF")
        
        # å†³å®šä¿å­˜ç›®å½•
        ocr_save_path = None
        if save_path is not None:
            # ç”¨æˆ·æŒ‡å®šäº†ä¿å­˜è·¯å¾„
            ocr_save_path = save_path
        elif auto_save:
            # è‡ªåŠ¨ä¿å­˜åˆ°æ ‡å‡†ç›®å½•
            try:
                paper_dir = self.get_paper_directory()
                paper_dir.mkdir(parents=True, exist_ok=True)
                ocr_save_path = str(paper_dir)
            except ValueError as e:
                logger.warning(f"æ— æ³•ä½¿ç”¨æ ‡å‡†ä¿å­˜è·¯å¾„: {e}")
        
        # é€‰æ‹©OCRæ–¹æ³•ï¼šè¿œç¨‹PaddleOCR > æœ¬åœ°PaddleOCR > æœ¬åœ°PyMuPDF
        if use_remote_ocr:
            # ä½¿ç”¨è¿œç¨‹PaddleOCRæœåŠ¡
            ocr_result, status_info = self._performOCR_remote(max_pages, ocr_save_path)
        elif use_paddleocr:
            # ä½¿ç”¨æœ¬åœ°PaddleOCR
            ocr_result, status_info = self._performOCR_paddleocr(max_pages, ocr_save_path)
        else:
            # ä½¿ç”¨æœ¬åœ°PyMuPDFï¼ˆé»˜è®¤ï¼‰
            try:
                ocr_result, status_info = self._performOCR_pymupdf(max_pages)
                # å¦‚æœéœ€è¦ä¿å­˜PyMuPDFç»“æœ
                if ocr_save_path and ocr_result:
                    saved_files = self._save_pymupdf_result(ocr_result, ocr_save_path)
                    status_info['saved_files'] = saved_files
            except Exception as e:
                logger.warning(f"PyMuPDF OCRå¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°PaddleOCR")
                ocr_result, status_info = self._performOCR_paddleocr(max_pages, ocr_save_path)
        
        return ocr_result, status_info
    
    def _save_pymupdf_result(self, ocr_result: str, save_path: str) -> list:
        """
        ä¿å­˜PyMuPDF OCRç»“æœåˆ°æ–‡ä»¶
        
        Args:
            ocr_result: OCRè¯†åˆ«ç»“æœæ–‡æœ¬
            save_path: ä¿å­˜ç›®å½•è·¯å¾„
            
        Returns:
            list: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        saved_files = []
        try:
            if not self.arxiv_id or self.arxiv_id == "":
                logger.warning("ArXiv IDä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆæ ‡å‡†æ–‡ä»¶å")
                return saved_files
            
            save_dir = Path(save_path)
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            text_file = save_dir / f"{self.arxiv_id}_ocr.txt"
            
            # ä¿å­˜æ–‡æœ¬æ–‡ä»¶
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(ocr_result)
            
            saved_files.append(str(text_file))
            logger.info(f"PyMuPDF OCRç»“æœå·²ä¿å­˜åˆ°: {text_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜PyMuPDF OCRç»“æœå¤±è´¥: {e}")
        
        return saved_files
    
    def _performOCR_pymupdf(self, max_pages: int = 25) -> tuple[Optional[str], dict]:
        """
        ä½¿ç”¨PyMuPDFè¿›è¡Œå¿«é€Ÿæ–‡æœ¬æå–ï¼ˆé»˜è®¤æ–¹æ³•ï¼‰
        """
        import tempfile
        
        logger.info(f"å¼€å§‹ä½¿ç”¨PyMuPDFè¿›è¡Œæ–‡æœ¬æå–ï¼Œæœ€å¤§å¤„ç†{max_pages}é¡µ")
        
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                # ä¿å­˜PDFåˆ°ä¸´æ—¶æ–‡ä»¶
                tmp_pdf_path = os.path.join(temp_dir, 'input.pdf')
                with open(tmp_pdf_path, 'wb') as f:
                    f.write(self.pdf)
                
                # æ‰“å¼€PDFæ–‡æ¡£
                pdf_document = fitz.open(tmp_pdf_path)
                total_pages = len(pdf_document)
                
                logger.info(f"PDFæ€»é¡µæ•°: {total_pages}")
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºè¶…é•¿æ–‡æ¡£
                is_oversized = total_pages > max_pages
                if is_oversized:
                    logger.warning(f"æ–‡æ¡£é¡µæ•°({total_pages})è¶…è¿‡é™åˆ¶({max_pages})ï¼Œå°†åªå¤„ç†å‰{max_pages}é¡µ")
                
                # å†³å®šå¤„ç†çš„é¡µæ•°
                pages_to_process = min(max_pages, total_pages)
                
                # æå–æ–‡æœ¬
                all_content = []
                total_chars = 0
                
                for page_num in range(pages_to_process):
                    try:
                        page = pdf_document[page_num]
                        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„PyMuPDF
                        if hasattr(page, 'get_text'):
                            text = page.get_text()
                        elif hasattr(page, 'getText'):
                            text = page.getText()
                        else:
                            text = ""
                        
                        if text.strip():
                            # æ¸…ç†æ–‡æœ¬
                            clean_text = text.strip()
                            clean_text = re.sub(r'\n\s*\n', '\n\n', clean_text)  # è§„èŒƒåŒ–ç©ºè¡Œ
                            clean_text = re.sub(r'[ \t]+', ' ', clean_text)  # åˆå¹¶å¤šä½™ç©ºæ ¼
                            
                            if clean_text:
                                all_content.append(f"=== ç¬¬{page_num + 1}é¡µ ===\n{clean_text}")
                                total_chars += len(clean_text)
                                
                    except Exception as e:
                        logger.warning(f"å¤„ç†ç¬¬{page_num + 1}é¡µå¤±è´¥: {e}")
                        continue
                
                pdf_document.close()
                
                # æ„å»ºçŠ¶æ€ä¿¡æ¯
                status_info = {
                    'total_pages': total_pages,
                    'processed_pages': pages_to_process,
                    'is_oversized': is_oversized,
                    'char_count': total_chars,
                    'method': 'pymupdf'
                }
                
                if all_content:
                    self.ocr_result = "\n\n".join(all_content)
                    
                    status_msg = f"PyMuPDFæ–‡æœ¬æå–å®Œæˆï¼Œå¤„ç†äº† {pages_to_process}/{total_pages} é¡µï¼Œæå–æ–‡æœ¬ {total_chars} ä¸ªå­—ç¬¦"
                    if is_oversized:
                        status_msg += f" (æ–‡æ¡£è¶…é•¿ï¼Œå¯èƒ½æ˜¯æ¯•ä¸šè®ºæ–‡æˆ–ä¹¦ç±)"
                    
                    logger.info(status_msg)
                    return self.ocr_result, status_info
                else:
                    logger.warning("PyMuPDFæœªæå–åˆ°ä»»ä½•æ–‡æœ¬")
                    self.ocr_result = ""
                    return self.ocr_result, status_info
                    
        except Exception as e:
            error_msg = f"PyMuPDFå¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)
    
    def _performOCR_remote(self, max_pages: int = 25, output_path: Optional[str] = None) -> tuple[Optional[str], dict]:
        """
        ä½¿ç”¨è¿œç¨‹PaddleOCRæœåŠ¡è¿›è¡Œç»“æ„åŒ–æ–‡æ¡£è§£æ
        
        Args:
            max_pages: æœ€å¤§å¤„ç†é¡µæ•°ï¼Œé»˜è®¤25é¡µ
            output_path: è¾“å‡ºç›®å½•è·¯å¾„ï¼Œç”¨äºä¿å­˜ç»“æœ
            
        Returns:
            tuple: (Markdownæ–‡æœ¬, çŠ¶æ€ä¿¡æ¯å­—å…¸)
        """
        import tempfile
        import json
        
        logger.info(f"å¼€å§‹ä½¿ç”¨è¿œç¨‹PaddleOCRæœåŠ¡è¿›è¡Œç»“æ„åŒ–æ–‡æ¡£è§£æï¼Œæœ€å¤§å¤„ç†{max_pages}é¡µ")
        
        # è·å–è¿œç¨‹OCRæœåŠ¡é…ç½®
        remote_endpoint = os.getenv('REMOTE_OCR_ENDPOINT')
        if not remote_endpoint:
            error_msg = "è¿œç¨‹OCRæœåŠ¡æœªé…ç½®ï¼Œè¯·è®¾ç½®REMOTE_OCR_ENDPOINTç¯å¢ƒå˜é‡"
            logger.error(error_msg)
            return None, {
                'error': error_msg,
                'total_pages': 0,
                'processed_pages': 0,
                'is_oversized': False,
                'char_count': 0,
                'method': 'remote_paddleocr',
                'saved_files': []
            }
        
        remote_timeout = int(os.getenv('REMOTE_OCR_TIMEOUT', '300'))
        api_key = os.getenv('REMOTE_OCR_API_KEY')
        
        try:
            # å‡†å¤‡è¯·æ±‚æ•°æ®
            files = {'file': ('paper.pdf', self.pdf, 'application/pdf')}
            data = {
                'max_pages': max_pages,
                'arxiv_id': self.arxiv_id or 'unknown'
            }
            
            # å‡†å¤‡è¯·æ±‚å¤´
            headers = {}
            if api_key:
                headers['X-API-Key'] = api_key
            
            # å‘é€è¯·æ±‚åˆ°è¿œç¨‹æœåŠ¡ï¼ˆæ”¯æŒé‡è¯•æœºåˆ¶ï¼‰
            import time
            
            max_retries = 3
            base_retry_delay = 10  # åŸºç¡€å»¶è¿Ÿ10ç§’
            response = None
            
            for attempt in range(max_retries):
                try:
                    logger.info(f"å‘é€è¯·æ±‚åˆ°è¿œç¨‹OCRæœåŠ¡: {remote_endpoint} (å°è¯• {attempt + 1}/{max_retries})")
                    response = requests.post(
                        f"{remote_endpoint.rstrip('/')}/api/ocr/process",
                        files=files,
                        data=data,
                        headers=headers,
                        timeout=remote_timeout
                    )
                    
                    # å¦‚æœè¯·æ±‚æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    if response.status_code == 200:
                        break
                    else:
                        logger.warning(f"è¿œç¨‹OCRæœåŠ¡è¿”å›çŠ¶æ€ç : {response.status_code}")
                        if attempt < max_retries - 1:
                            # æŒ‡æ•°é€€é¿ï¼š10ç§’ã€20ç§’ã€40ç§’
                            retry_delay = base_retry_delay * (2 ** attempt)
                            logger.info(f"ç­‰å¾…{retry_delay}ç§’åé‡è¯•...")
                            time.sleep(retry_delay)
                            # é‡æ–°å‡†å¤‡filesï¼ˆå› ä¸ºæ–‡ä»¶æµå¯èƒ½å·²è¢«æ¶ˆè€—ï¼‰
                            files = {'file': ('paper.pdf', self.pdf, 'application/pdf')}
                            
                except requests.exceptions.ConnectionError as e:
                    error_type = "è¿æ¥è¢«æ‹’ç»" if "Connection refused" in str(e) else "è¿æ¥ä¸­æ–­"
                    
                    if attempt < max_retries - 1:
                        # ä¸æ˜¯æœ€åä¸€æ¬¡ï¼Œç­‰å¾…åç»§ç»­
                        retry_delay = base_retry_delay * (2 ** attempt)
                        logger.warning(f"{error_type}ï¼Œ{retry_delay}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(retry_delay)
                        # é‡æ–°å‡†å¤‡filesï¼ˆå› ä¸ºæ–‡ä»¶æµå¯èƒ½å·²è¢«æ¶ˆè€—ï¼‰
                        files = {'file': ('paper.pdf', self.pdf, 'application/pdf')}
                    else:
                        # æ˜¯æœ€åä¸€æ¬¡äº†ï¼Œè®°å½•è¯¦ç»†é”™è¯¯åæŠ›å‡º
                        logger.error(f"{error_type}ï¼Œå·²å°è¯•{max_retries}æ¬¡ï¼Œæ”¾å¼ƒ: {str(e)}")
                        raise
                except Exception as e:
                    if attempt < max_retries - 1:
                        # ä¸æ˜¯æœ€åä¸€æ¬¡ï¼Œç­‰å¾…åç»§ç»­
                        retry_delay = base_retry_delay * (2 ** attempt)
                        logger.warning(f"è¯·æ±‚å¼‚å¸¸ï¼Œ{retry_delay}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                        time.sleep(retry_delay)
                        # é‡æ–°å‡†å¤‡filesï¼ˆå› ä¸ºæ–‡ä»¶æµå¯èƒ½å·²è¢«æ¶ˆè€—ï¼‰
                        files = {'file': ('paper.pdf', self.pdf, 'application/pdf')}
                    else:
                        # æ˜¯æœ€åä¸€æ¬¡äº†ï¼Œè®°å½•è¯¦ç»†é”™è¯¯åæŠ›å‡º
                        logger.error(f"è¯·æ±‚å¼‚å¸¸ï¼Œå·²å°è¯•{max_retries}æ¬¡ï¼Œæ”¾å¼ƒ: {str(e)}")
                        raise
                        
            # å¦‚æœæ²¡æœ‰responseå¯¹è±¡ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†é˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
            if response is None:
                raise Exception("æ— æ³•è·å–è¿œç¨‹OCRæœåŠ¡å“åº”")
            
            if response.status_code == 200:
                result = response.json()
                
                if result.get('success', False):
                    ocr_result = result.get('ocr_result')
                    status_info = result.get('status_info', {})
                    
                    # æ›´æ–°æ–¹æ³•æ ‡è¯†
                    status_info['method'] = 'remote_paddleocr'
                    
                    # å¦‚æœæŒ‡å®šäº†ä¿å­˜è·¯å¾„ï¼Œä¿å­˜ç»“æœåˆ°æœ¬åœ°
                    if output_path and ocr_result:
                        try:
                            from pathlib import Path
                            save_dir = Path(output_path)
                            save_dir.mkdir(parents=True, exist_ok=True)
                            
                            # ä¿å­˜ä¸»è¦ç»“æœæ–‡ä»¶ï¼ˆä½¿ç”¨paddleocrå‘½åä¿æŒä¸€è‡´æ€§ï¼‰
                            result_file = save_dir / f"{self.arxiv_id or 'unknown'}_paddleocr.md"
                            with open(result_file, 'w', encoding='utf-8') as f:
                                f.write(ocr_result)
                            
                            # ä¿å­˜å›¾ç‰‡ï¼ˆå¦‚æœè¿œç¨‹æœåŠ¡è¿”å›äº†å›¾ç‰‡ï¼‰
                            local_saved_files = [str(result_file)]
                            images_data = result.get('images', {})
                            
                            if images_data:
                                # åˆ›å»ºimgsç›®å½•
                                imgs_dir = save_dir / "imgs"
                                imgs_dir.mkdir(exist_ok=True)
                                
                                # ä¿å­˜æ¯å¼ å›¾ç‰‡
                                for img_name, img_base64 in images_data.items():
                                    try:
                                        import base64
                                        img_data = base64.b64decode(img_base64)
                                        img_path = imgs_dir / img_name
                                        with open(img_path, 'wb') as f:
                                            f.write(img_data)
                                        local_saved_files.append(str(img_path))
                                    except Exception as e:
                                        logger.warning(f"ä¿å­˜å›¾ç‰‡å¤±è´¥ {img_name}: {e}")
                                
                                logger.info(f"è¿œç¨‹OCRä¿å­˜äº† {len(images_data)} å¼ å›¾ç‰‡åˆ° {imgs_dir}")
                            
                            # æ›´æ–°ä¿å­˜æ–‡ä»¶åˆ—è¡¨å’Œå›¾ç‰‡æ•°é‡
                            status_info['saved_files'] = local_saved_files
                            status_info['images_count'] = len(images_data)
                            
                            logger.info(f"è¿œç¨‹OCRç»“æœå·²ä¿å­˜åˆ°æœ¬åœ°: {result_file} (åŒ…å« {len(images_data)} å¼ å›¾ç‰‡)")
                            
                        except Exception as e:
                            logger.warning(f"ä¿å­˜è¿œç¨‹OCRç»“æœåˆ°æœ¬åœ°å¤±è´¥: {str(e)}")
                    
                    logger.info(f"è¿œç¨‹OCRå¤„ç†æˆåŠŸï¼Œæå–å­—ç¬¦æ•°: {status_info.get('char_count', 0)}")
                    return ocr_result, status_info
                    
                else:
                    error_msg = result.get('error', 'è¿œç¨‹OCRæœåŠ¡å¤„ç†å¤±è´¥')
                    logger.error(f"è¿œç¨‹OCRå¤„ç†å¤±è´¥: {error_msg}")
                    return None, {
                        'error': error_msg,
                        'total_pages': 0,
                        'processed_pages': 0,
                        'is_oversized': False,
                        'char_count': 0,
                        'method': 'remote_paddleocr',
                        'saved_files': []
                    }
                    
            else:
                error_msg = f"è¿œç¨‹OCRæœåŠ¡è¯·æ±‚å¤±è´¥: HTTP {response.status_code}"
                logger.error(error_msg)
                return None, {
                    'error': error_msg,
                    'total_pages': 0,
                    'processed_pages': 0,
                    'is_oversized': False,
                    'char_count': 0,
                    'method': 'remote_paddleocr',
                    'saved_files': []
                }
                
        except requests.exceptions.Timeout:
            error_msg = f"è¿œç¨‹OCRæœåŠ¡è¯·æ±‚è¶…æ—¶ (>{remote_timeout}ç§’)"
            logger.error(error_msg)
            return None, {
                'error': error_msg,
                'total_pages': 0,
                'processed_pages': 0,
                'is_oversized': False,
                'char_count': 0,
                'method': 'remote_paddleocr',
                'saved_files': []
            }
            
        except requests.exceptions.ConnectionError:
            error_msg = f"æ— æ³•è¿æ¥åˆ°è¿œç¨‹OCRæœåŠ¡: {remote_endpoint}"
            logger.error(error_msg)
            return None, {
                'error': error_msg,
                'total_pages': 0,
                'processed_pages': 0,
                'is_oversized': False,
                'char_count': 0,
                'method': 'remote_paddleocr',
                'saved_files': []
            }
            
        except Exception as e:
            error_msg = f"è¿œç¨‹OCRå¤„ç†å¼‚å¸¸: {str(e)}"
            logger.error(error_msg)
            return None, {
                'error': error_msg,
                'total_pages': 0,
                'processed_pages': 0,
                'is_oversized': False,
                'char_count': 0,
                'method': 'remote_paddleocr',
                'saved_files': []
            }

    def _performOCR_paddleocr(self, max_pages: int = 25, output_path: Optional[str] = None) -> tuple[Optional[str], dict]:
        """
        ä½¿ç”¨PaddleOCR 3.0 PPStructureV3è¿›è¡Œç»“æ„åŒ–æ–‡æ¡£è§£æ
        
        Args:
            max_pages: æœ€å¤§å¤„ç†é¡µæ•°ï¼Œé»˜è®¤25é¡µ
            output_path: è¾“å‡ºç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸ºä¸´æ—¶ç›®å½•
            
        Returns:
            tuple: (Markdownæ–‡æœ¬, çŠ¶æ€ä¿¡æ¯å­—å…¸)
        """
        import tempfile
        import shutil
        
        logger.info(f"å¼€å§‹ä½¿ç”¨PaddleOCR 3.0è¿›è¡Œç»“æ„åŒ–æ–‡æ¡£è§£æï¼Œæœ€å¤§å¤„ç†{max_pages}é¡µ")
        
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                # ä¿å­˜PDFåˆ°ä¸´æ—¶æ–‡ä»¶
                tmp_pdf_path = os.path.join(temp_dir, 'input.pdf')
                with open(tmp_pdf_path, 'wb') as f:
                    f.write(self.pdf)
                
                # ä½¿ç”¨PyMuPDFæ£€æŸ¥æ€»é¡µæ•°
                pdf_document = fitz.open(tmp_pdf_path)
                total_pages = len(pdf_document)
                pdf_document.close()
                
                logger.info(f"PDFæ€»é¡µæ•°: {total_pages}")
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºè¶…é•¿æ–‡æ¡£
                is_oversized = total_pages > max_pages
                if is_oversized:
                    logger.warning(f"æ–‡æ¡£é¡µæ•°({total_pages})è¶…è¿‡é™åˆ¶({max_pages})ï¼Œå°†åªå¤„ç†å‰{max_pages}é¡µ")
                
                # å†³å®šå¤„ç†çš„é¡µæ•°
                pages_to_process = min(max_pages, total_pages)
                
                # æ£€æŸ¥OCRåŠŸèƒ½æ˜¯å¦å¯ç”¨
                if not OCR_AVAILABLE:
                    logger.error("OCRåŠŸèƒ½ä¸å¯ç”¨ï¼Œç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…")
                    raise Exception("OCRåŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·å®‰è£… paddleocr å’Œç›¸å…³ä¾èµ–")
                
                # åˆå§‹åŒ–PaddleOCR PPStructureV3
                try:
                    pipeline = PPStructureV3()
                    logger.info("PaddleOCR PPStructureV3åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    logger.error(f"PaddleOCRåˆå§‹åŒ–å¤±è´¥: {e}")
                    raise Exception(f"PaddleOCRåˆå§‹åŒ–å¤±è´¥: {e}")
                
                # æ‰§è¡Œç»“æ„åŒ–è¯†åˆ«
                logger.info("å¼€å§‹æ‰§è¡Œç»“æ„åŒ–æ–‡æ¡£è¯†åˆ«...")
                output = pipeline.predict(input=tmp_pdf_path)
                
                # è®¾ç½®è¾“å‡ºç›®å½•
                if output_path is None:
                    output_md_dir = os.path.join(temp_dir, 'output')
                else:
                    output_md_dir = Path(output_path)
                
                output_md_dir = Path(output_md_dir)
                output_md_dir.mkdir(parents=True, exist_ok=True)
                
                # å¤„ç†ç»“æœå¹¶æå–markdownå’Œå›¾ç‰‡
                markdown_list = []
                markdown_images = []
                
                for res in output:
                    if hasattr(res, 'markdown'):
                        md_info = res.markdown
                        markdown_list.append(md_info)
                        markdown_images.append(md_info.get("markdown_images", {}))
                
                # åˆå¹¶markdowné¡µé¢
                if hasattr(pipeline, 'concatenate_markdown_pages'):
                    markdown_texts = pipeline.concatenate_markdown_pages(markdown_list)
                else:
                    # å¤‡ç”¨æ–¹æ³•ï¼šæ‰‹åŠ¨åˆå¹¶
                    markdown_texts = "\n\n".join([str(md) for md in markdown_list if md])
                
                # ä¿å­˜markdownæ–‡ä»¶
                if output_path is None:
                    # ä¸´æ—¶æ¨¡å¼ï¼Œå°†ç»“æœå­˜å‚¨åœ¨å±æ€§ä¸­
                    self.paddle_ocr_result = markdown_texts
                    self.paddle_ocr_images = {}
                    for item in markdown_images:
                        if item:
                            self.paddle_ocr_images.update(item)
                else:
                    # ä¿å­˜åˆ°æŒ‡å®šç›®å½•ï¼Œä½¿ç”¨æ ‡å‡†åŒ–æ–‡ä»¶å
                    saved_files = []
                    
                    # ä½¿ç”¨ arxiv_id ä½œä¸ºæ–‡ä»¶åï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    base_filename = self.arxiv_id if (self.arxiv_id and self.arxiv_id != "") else Path(tmp_pdf_path).stem
                    mkd_file_path = output_md_dir / f"{base_filename}_paddleocr.md"
                    
                    with open(mkd_file_path, "w", encoding="utf-8") as f:
                        f.write(markdown_texts)
                    saved_files.append(str(mkd_file_path))
                    
                    # ä¿å­˜å›¾ç‰‡åˆ°æ ‡å‡†åŒ–çš„å›¾ç‰‡ç›®å½•
                    images_dir = output_md_dir 
                    for item in markdown_images:
                        if item:
                            for path, image in item.items():
                                file_path = images_dir / path
                                file_path.parent.mkdir(parents=True, exist_ok=True)
                                image.save(file_path)
                                saved_files.append(str(file_path))
                    
                    logger.info(f"Markdownæ–‡ä»¶å’Œå›¾ç‰‡å·²ä¿å­˜åˆ°: {output_md_dir}")
                
                # æ„å»ºçŠ¶æ€ä¿¡æ¯
                total_chars = len(markdown_texts) if markdown_texts else 0
                status_info = {
                    'total_pages': total_pages,
                    'processed_pages': pages_to_process,
                    'is_oversized': is_oversized,
                    'char_count': total_chars,
                    'method': 'paddleocr',
                    'images_count': len(self.paddle_ocr_images) if hasattr(self, 'paddle_ocr_images') else 0
                }
                
                # æ·»åŠ ä¿å­˜æ–‡ä»¶åˆ—è¡¨ï¼ˆå¦‚æœä¿å­˜äº†æ–‡ä»¶ï¼‰
                if output_path is not None and 'saved_files' in locals():
                    status_info['saved_files'] = saved_files
                
                if markdown_texts:
                    self.ocr_result = markdown_texts
                    
                    status_msg = f"PaddleOCRç»“æ„åŒ–è¯†åˆ«å®Œæˆï¼Œå¤„ç†äº† {pages_to_process}/{total_pages} é¡µï¼Œæå–Markdownæ–‡æœ¬ {total_chars} ä¸ªå­—ç¬¦"
                    if status_info['images_count'] > 0:
                        status_msg += f"ï¼Œæå–å›¾ç‰‡ {status_info['images_count']} å¼ "
                    if is_oversized:
                        status_msg += f" (æ–‡æ¡£è¶…é•¿ï¼Œå¯èƒ½æ˜¯æ¯•ä¸šè®ºæ–‡æˆ–ä¹¦ç±)"
                    
                    logger.info(status_msg)
                    return self.ocr_result, status_info
                else:
                    logger.warning("PaddleOCRæœªæå–åˆ°ä»»ä½•å†…å®¹")
                    self.ocr_result = ""
                    return self.ocr_result, status_info
                
        except Exception as e:
            error_msg = f"PaddleOCRå¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)
    
    def getOcrResult(self) -> Optional[str]:
        """
        è·å–OCRè¯†åˆ«ç»“æœ
        
        Returns:
            str: OCRè¯†åˆ«ç»“æœï¼Œå¦‚æœæœªè¿›è¡ŒOCRè¯†åˆ«åˆ™è¿”å›None
        """
        return self.ocr_result
    
    def clearOcrResult(self):
        """
        æ¸…ç©ºOCRè¯†åˆ«ç»“æœï¼Œé‡Šæ”¾å†…å­˜
        """
        self.ocr_result = None
    
    def getPaddleOcrResult(self) -> Optional[str]:
        """
        è·å–PaddleOCRç»“æ„åŒ–è¯†åˆ«çš„Markdownç»“æœ
        
        Returns:
            str: PaddleOCRè¯†åˆ«çš„Markdownæ–‡æœ¬ï¼Œå¦‚æœæœªè¿›è¡Œè¯†åˆ«åˆ™è¿”å›None
        """
        return getattr(self, 'paddle_ocr_result', None)
    
    def getPaddleOcrImages(self) -> dict:
        """
        è·å–PaddleOCRæå–çš„å›¾ç‰‡å­—å…¸
        
        Returns:
            dict: å›¾ç‰‡è·¯å¾„åˆ°PIL Imageå¯¹è±¡çš„æ˜ å°„ï¼Œå¦‚æœæœªè¿›è¡Œè¯†åˆ«åˆ™è¿”å›ç©ºå­—å…¸
        """
        return getattr(self, 'paddle_ocr_images', {})
    
    def clearPaddleOcrResult(self):
        """
        æ¸…ç©ºPaddleOCRè¯†åˆ«ç»“æœï¼Œé‡Šæ”¾å†…å­˜
        """
        self.paddle_ocr_result = None
        if hasattr(self, 'paddle_ocr_images') and self.paddle_ocr_images:
            self.paddle_ocr_images.clear()
    
    def savePaddleOcrToFile(self, output_path: Optional[str] = None, use_standard_path: bool = False) -> bool:
        """
        å°†PaddleOCRç»“æœä¿å­˜åˆ°æ–‡ä»¶
        
        Args:
            output_path: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆä¼˜å…ˆçº§é«˜äºuse_standard_pathï¼‰
            use_standard_path: æ˜¯å¦ä½¿ç”¨æ ‡å‡†ç›®å½•ç»“æ„ï¼ˆå½“output_pathä¸ºNoneæ—¶ç”Ÿæ•ˆï¼‰
            
        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            if not self.paddle_ocr_result:
                logger.warning("æ²¡æœ‰PaddleOCRç»“æœå¯ä»¥ä¿å­˜")
                return False
            
            # ç¡®å®šè¾“å‡ºç›®å½•
            if output_path is not None:
                # ç”¨æˆ·æŒ‡å®šäº†ä¿å­˜è·¯å¾„
                output_dir = Path(output_path)
            elif use_standard_path:
                # ä½¿ç”¨æ ‡å‡†ç›®å½•ç»“æ„
                try:
                    output_dir = self.get_paper_directory()
                except ValueError as e:
                    logger.error(f"æ— æ³•ä½¿ç”¨æ ‡å‡†è·¯å¾„: {e}")
                    return False
            else:
                # æ²¡æœ‰æŒ‡å®šè·¯å¾„ä¸”ä¸ä½¿ç”¨æ ‡å‡†è·¯å¾„
                logger.error("å¿…é¡»æŒ‡å®šoutput_pathæˆ–è®¾ç½®use_standard_path=True")
                return False
            
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜markdownæ–‡ä»¶ï¼Œä½¿ç”¨æ ‡å‡†åŒ–æ–‡ä»¶å
            filename = f"{self.arxiv_id}_paddleocr.md" if (self.arxiv_id and self.arxiv_id != "") else "unknown_paddleocr.md"
            markdown_file = output_dir / filename
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(self.paddle_ocr_result)
            
            # ä¿å­˜å›¾ç‰‡åˆ°æ ‡å‡†åŒ–çš„å›¾ç‰‡ç›®å½•
            if hasattr(self, 'paddle_ocr_images') and self.paddle_ocr_images:
                images_dir = output_dir 
                for path, image in self.paddle_ocr_images.items():
                    image_path = images_dir / path
                    image_path.parent.mkdir(parents=True, exist_ok=True)
                    image.save(image_path)
            
            logger.info(f"PaddleOCRç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜PaddleOCRç»“æœå¤±è´¥: {e}")
            return False

    def get_paper_directory(self) -> Path:
        """
        è·å–è®ºæ–‡çš„æ ‡å‡†ç›®å½•è·¯å¾„
        
        Returns:
            Path: è®ºæ–‡ç›®å½•è·¯å¾„ - data/paper_analyze/{arxiv_id}/
        """
        if not self.arxiv_id or self.arxiv_id == "":
            raise ValueError("æ— æ³•åˆ›å»ºç›®å½•ï¼šArXiv ID ä¸ºç©º")
        
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºé¡¹ç›®æ ¹ç›®å½•
        project_root = Path(__file__).parent.parent.parent.parent  # å›åˆ°é¡¹ç›®æ ¹ç›®å½•
        base_dir = project_root / "data" / "paper_analyze"
        paper_dir = base_dir / self.arxiv_id
        
        return paper_dir
    
    def get_default_pdf_path(self) -> Path:
        """
        è·å– PDF æ–‡ä»¶çš„é»˜è®¤ä¿å­˜è·¯å¾„
        
        Returns:
            Path: PDF æ–‡ä»¶è·¯å¾„ - {paper_directory}/{arxiv_id}.pdf
        """
        if not self.arxiv_id or self.arxiv_id == "":
            raise ValueError("æ— æ³•ç”Ÿæˆ PDF è·¯å¾„ï¼šArXiv ID ä¸ºç©º")
        
        paper_dir = self.get_paper_directory()
        return paper_dir / f"{self.arxiv_id}.pdf"
    
    def get_default_ocr_paths(self) -> dict:
        """
        è·å– OCR ç»“æœæ–‡ä»¶çš„é»˜è®¤ä¿å­˜è·¯å¾„
        
        Returns:
            dict: åŒ…å«å„ç§ OCR æ–‡ä»¶è·¯å¾„çš„å­—å…¸
                - 'pymupdf_text': PyMuPDF æ–‡æœ¬ç»“æœè·¯å¾„
                - 'paddleocr_markdown': PaddleOCR Markdown ç»“æœè·¯å¾„
                - 'paddleocr_images_dir': PaddleOCR å›¾ç‰‡ç›®å½•è·¯å¾„
        """
        if not self.arxiv_id or self.arxiv_id == "":
            raise ValueError("æ— æ³•ç”Ÿæˆ OCR è·¯å¾„ï¼šArXiv ID ä¸ºç©º")
        
        paper_dir = self.get_paper_directory()
        
        return {
            'pymupdf_text': paper_dir / f"{self.arxiv_id}_ocr.txt",
            'paddleocr_markdown': paper_dir / f"{self.arxiv_id}_paddleocr.md",
            'paddleocr_images_dir': paper_dir / "images"
        }
    
    def save_ocr_to_standard_path(self) -> dict:
        """
        å°†æ‰€æœ‰OCRç»“æœä¿å­˜åˆ°æ ‡å‡†ç›®å½•çš„ä¾¿æ·æ–¹æ³•
        
        Returns:
            dict: ä¿å­˜æ“ä½œçš„ç»“æœä¿¡æ¯
                - 'success': bool - æ˜¯å¦æˆåŠŸ
                - 'saved_files': list - ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
                - 'errors': list - é”™è¯¯ä¿¡æ¯åˆ—è¡¨
        """
        result = {
            'success': False,
            'saved_files': [],
            'errors': []
        }
        
        try:
            if not self.arxiv_id or self.arxiv_id == "":
                result['errors'].append("ArXiv IDä¸ºç©ºï¼Œæ— æ³•ä¿å­˜åˆ°æ ‡å‡†è·¯å¾„")
                return result
            
            # ç¡®ä¿æ ‡å‡†ç›®å½•å­˜åœ¨
            paper_dir = self.get_paper_directory()
            paper_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜PyMuPDF OCRç»“æœ
            if self.ocr_result:
                try:
                    saved_files = self._save_pymupdf_result(self.ocr_result, str(paper_dir))
                    result['saved_files'].extend(saved_files)
                except Exception as e:
                    result['errors'].append(f"ä¿å­˜PyMuPDFç»“æœå¤±è´¥: {e}")
            
            # ä¿å­˜PaddleOCRç»“æœ
            if hasattr(self, 'paddle_ocr_result') and self.paddle_ocr_result:
                try:
                    success = self.savePaddleOcrToFile(use_standard_path=True)
                    if success:
                        # æ·»åŠ é¢„æœŸçš„æ–‡ä»¶è·¯å¾„åˆ°ç»“æœä¸­
                        paddle_paths = self.get_default_ocr_paths()
                        result['saved_files'].append(str(paddle_paths['paddleocr_markdown']))
                        
                        # å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ·»åŠ å›¾ç‰‡ç›®å½•
                        if hasattr(self, 'paddle_ocr_images') and self.paddle_ocr_images:
                            result['saved_files'].append(str(paddle_paths['paddleocr_images_dir']))
                    else:
                        result['errors'].append("PaddleOCRç»“æœä¿å­˜å¤±è´¥")
                except Exception as e:
                    result['errors'].append(f"ä¿å­˜PaddleOCRç»“æœå¤±è´¥: {e}")
            
            # åˆ¤æ–­æ•´ä½“æˆåŠŸçŠ¶æ€
            result['success'] = len(result['saved_files']) > 0 and len(result['errors']) == 0
            
            if result['success']:
                logger.info(f"OCRç»“æœå·²ä¿å­˜åˆ°æ ‡å‡†è·¯å¾„: {paper_dir}")
            elif result['saved_files']:
                logger.warning(f"éƒ¨åˆ†OCRç»“æœä¿å­˜æˆåŠŸï¼Œä½†æœ‰é”™è¯¯: {result['errors']}")
            else:
                logger.error(f"OCRç»“æœä¿å­˜å¤±è´¥: {result['errors']}")
                
        except Exception as e:
            result['errors'].append(f"ä¿å­˜æ“ä½œå¤±è´¥: {e}")
            logger.error(f"ä¿å­˜OCRç»“æœåˆ°æ ‡å‡†è·¯å¾„å¤±è´¥: {e}")
        
        return result

    def cleanup(self):
        """
        æ¸…ç†ArxivDataå¯¹è±¡çš„æ‰€æœ‰å†…éƒ¨æ•°æ®ï¼Œé‡Šæ”¾å†…å­˜
        åŒ…æ‹¬PDFæ•°æ®ã€OCRç»“æœã€ç»“æ„åŒ–åˆ†æå­—æ®µç­‰æ‰€æœ‰å¤§å†…å­˜å ç”¨çš„å±æ€§
        """
        # æ¸…ç†PDFç›¸å…³æ•°æ®
        self.pdf = None
        self.pdf_path = None
        
        # æ¸…ç†OCRç»“æœ
        self.ocr_result = None
        
        # æ¸…ç†PaddleOCRç»“æœ
        self.paddle_ocr_result = None
        if hasattr(self, 'paddle_ocr_images') and self.paddle_ocr_images:
            self.paddle_ocr_images.clear()
        
        # æ¸…ç†ç»“æ„åŒ–æ‘˜è¦å­—æ®µ
        self.research_background = None
        self.research_objectives = None
        self.methods = None
        self.key_findings = None
        self.conclusions = None
        self.limitations = None
        self.future_work = None
        self.keywords = None
        
        # æ¸…ç†åˆ†æç»“æœ
        self.abstract_analysis_justification = None
        self.full_paper_analysis_justification = None
        self.paper_summary = None
        
        # æ¸…ç†æ ‡ç­¾å’Œå…¶ä»–åˆ—è¡¨æ•°æ®
        if hasattr(self, 'tag') and self.tag:
            self.tag.clear()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        logger.debug(f"ArxivDataå¯¹è±¡å·²æ¸…ç†: {self.title[:50] if self.title else 'unknown'}...")

    def clear_invalid_characters(self, string: str) -> str:
        """
        å»é™¤å­—ç¬¦ä¸²ä¸­çš„éæ³•å­—ç¬¦
        """
        invalid_characters = ['/', ':', '*', '?',
                              '\\', '<', '>', '|', ' ', '"', "'"]
        for char in invalid_characters:
            string = string.replace(char, '_')
        return string


class ArxivResult:

    def __init__(self, results: list[dict]):
        """
        æœç´¢ç»“æœçš„ä¿å­˜ç±»ã€‚

        :param results: æœç´¢ç»“æœ
        :type results: list[dict]
        """
        self.results = [ArxivData(result) for result in results]

        self.num_results = len(self.results)

    def __iter__(self):
        """
        å®ç°è¿­ä»£å™¨åè®®
        """
        return iter(self.results)
    
    def display_results(self, display_range: str = "all", max_display: int = 10, 
                       show_details: bool = True, show_summary: bool = True):
        """
        ç»“æ„åŒ–æ˜¾ç¤ºæœç´¢ç»“æœ
        
        :param display_range: æ˜¾ç¤ºèŒƒå›´ "all" æˆ– "limited"
        :type display_range: str
        :param max_display: å½“display_rangeä¸º"limited"æ—¶çš„æœ€å¤§æ˜¾ç¤ºæ•°é‡
        :type max_display: int  
        :param show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        :type show_details: bool
        :param show_summary: æ˜¯å¦æ˜¾ç¤ºæ‘˜è¦ç»Ÿè®¡
        :type show_summary: bool
        """
        if self.num_results == 0:
            print("ğŸ“‹ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
            return
            
        # ç¡®å®šæ˜¾ç¤ºæ•°é‡
        if display_range == "all":
            display_count = self.num_results
            range_text = "å…¨éƒ¨"
        else:
            display_count = min(max_display, self.num_results)
            range_text = f"å‰ {display_count}"
            
        # æ˜¾ç¤ºæ ‡é¢˜
        print("=" * 80)
        print(f"ğŸ“š ArXiv æœç´¢ç»“æœ - {range_text} {display_count} ç¯‡è®ºæ–‡")
        print("=" * 80)
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        if show_details:
            for i, paper in enumerate(self.results[:display_count], 1):
                self._display_single_paper(i, paper)
                if i < display_count:  # ä¸æ˜¯æœ€åä¸€ä¸ªåˆ™æ˜¾ç¤ºåˆ†éš”çº¿
                    print("-" * 80)
        
        # æ˜¾ç¤ºæ‘˜è¦ç»Ÿè®¡
        if show_summary:
            self._display_summary(display_count)
    
    def _display_single_paper(self, index: int, paper: ArxivData):
        """æ˜¾ç¤ºå•ä¸ªè®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯"""
        print(f"\nğŸ“„ è®ºæ–‡ {index}")
        print(f"ğŸ“Œ æ ‡é¢˜: {paper.title}")
        print(f"ğŸ”— ArXiv ID: {paper.arxiv_id or 'æœªçŸ¥'}")
        print(f"ğŸ“… å‘å¸ƒæ—¶é—´: {paper.published_date}")
        print(f"ğŸ·ï¸  åˆ†ç±»: {paper.categories}")
        print(f"ğŸŒ é“¾æ¥: {paper.link}")
        if paper.snippet:
            print(f"ğŸ“ æ‘˜è¦: {paper.snippet[:200]}..." if len(paper.snippet) > 200 else f"ğŸ“ æ‘˜è¦: {paper.snippet}")
        else:
            print("ğŸ“ æ‘˜è¦: æ— ")
        print(f"ğŸ“¥ PDF: {paper.pdf_link}")
        
        # æ˜¾ç¤ºæ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
        if paper.tag:
            print(f"ğŸ·ï¸  æ ‡ç­¾: {', '.join(paper.tag)}")
    
    def _display_summary(self, displayed_count: int):
        """æ˜¾ç¤ºæ‘˜è¦ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 80)
        print("ğŸ“Š æœç´¢æ‘˜è¦")
        print("=" * 80)
        print(f"ğŸ“‹ æ€»ç»“æœæ•°: {self.num_results}")
        print(f"ğŸ–¥ï¸  å·²æ˜¾ç¤º: {displayed_count}")
        
        if displayed_count < self.num_results:
            print(f"âš ï¸  å‰©ä½™æœªæ˜¾ç¤º: {self.num_results - displayed_count}")
        
        # å‘å¸ƒæ—¶é—´ç»Ÿè®¡
        if self.num_results > 0:
            date_counts = {}
            for paper in self.results:
                date = paper.published_date
                if date and date != "æœªçŸ¥æ—¥æœŸ":
                    date_counts[date] = date_counts.get(date, 0) + 1
            
            if date_counts:
                print(f"\nğŸ“ˆ å‘å¸ƒæ—¶é—´åˆ†å¸ƒ (å‰5):")
                sorted_dates = sorted(date_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                for date, count in sorted_dates:
                    print(f"   {date}: {count} ç¯‡")
        
        # åˆ†ç±»ç»Ÿè®¡  
        if self.num_results > 0:
            category_counts = {}
            for paper in self.results:
                categories = paper.categories.split(', ') if paper.categories else ['Unknown']
                for cat in categories:
                    category_counts[cat] = category_counts.get(cat, 0) + 1
            
            if category_counts:
                print(f"\nğŸ·ï¸  åˆ†ç±»åˆ†å¸ƒ (å‰5):")
                sorted_cats = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                for cat, count in sorted_cats:
                    print(f"   {cat}: {count} ç¯‡")
        
        print("=" * 80)
    
    def display_brief(self, max_display: int = 5):
        """ç®€æ´æ˜¾ç¤ºæ¨¡å¼ï¼Œåªæ˜¾ç¤ºæ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯"""
        if self.num_results == 0:
            print("ğŸ“‹ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
            return
            
        display_count = min(max_display, self.num_results)
        
        print("=" * 60)
        print(f"ğŸ“š ArXiv æœç´¢ç»“æœæ¦‚è§ˆ - å‰ {display_count} ç¯‡")
        print("=" * 60)
        
        for i, paper in enumerate(self.results[:display_count], 1):
            print(f"{i:2d}. {paper.published_date} | {(paper.title or 'æ— æ ‡é¢˜')[:60]}...")
            print(f"    ğŸ”— {paper.arxiv_id or 'æœªçŸ¥ID'} | ğŸ·ï¸ {paper.categories or 'æ— åˆ†ç±»'}")
            print()
        
        if display_count < self.num_results:
            print(f"... è¿˜æœ‰ {self.num_results - display_count} ç¯‡è®ºæ–‡æœªæ˜¾ç¤º")
        print("=" * 60)
    
    def display_titles_only(self, max_display: int = None):
        """ä»…æ˜¾ç¤ºæ ‡é¢˜åˆ—è¡¨"""
        if self.num_results == 0:
            print("ğŸ“‹ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
            return
            
        display_count = max_display if max_display else self.num_results
        display_count = min(display_count, self.num_results)
        
        print(f"ğŸ“œ è®ºæ–‡æ ‡é¢˜åˆ—è¡¨ ({display_count}/{self.num_results}):")
        print("-" * 50)
        
        for i, paper in enumerate(self.results[:display_count], 1):
            print(f"{i:3d}. {paper.title or 'æ— æ ‡é¢˜'}")
        
        if display_count < self.num_results:
            print(f"\n... è¿˜æœ‰ {self.num_results - display_count} ç¯‡è®ºæ–‡")
    
    def get_papers_by_date_range(self, start_year: int = None, end_year: int = None):
        """æ ¹æ®å‘å¸ƒå¹´ä»½ç­›é€‰è®ºæ–‡"""
        filtered_papers = []
        
        for paper in self.results:
            if paper.published_date and paper.published_date != "æœªçŸ¥æ—¥æœŸ":
                # æå–å¹´ä»½
                try:
                    year_match = re.search(r'(\d{4})å¹´', paper.published_date)
                    if year_match:
                        year = int(year_match.group(1))
                        
                        # æ£€æŸ¥å¹´ä»½èŒƒå›´
                        if start_year and year < start_year:
                            continue
                        if end_year and year > end_year:
                            continue
                        
                        filtered_papers.append(paper)
                except:
                    continue
        
        # åˆ›å»ºæ–°çš„ç»“æœå¯¹è±¡
        filtered_results = []
        for paper in filtered_papers:
            result_dict = {
                'title': paper.title,
                'link': paper.link, 
                'snippet': paper.snippet,
                'categories': paper.categories
            }
            filtered_results.append(result_dict)
        
        return ArxivResult(filtered_results)


class ArxivTool:
    def __init__(self, search_host: str = None):
        """
        ç›´æ¥ä½¿ç”¨ ArXiv API è¿›è¡Œæœç´¢ï¼Œä¸å†ä¾èµ– SearxNGã€‚

        :param search_host: ä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼Œä½†ä¸å†ä½¿ç”¨
        :type search_host: str
        """
        # ä¿ç•™å‚æ•°ä½†ä¸å†ä½¿ç”¨ï¼Œé¿å…ç ´åç°æœ‰è°ƒç”¨ä»£ç 
        self.search_host = search_host
        
        # å¤§è§„æ¨¡æœç´¢çš„é»˜è®¤é…ç½®
        self.default_chunk_size = 2000  # æ¯æ‰¹æ¬¡å¤§å°
        self.default_delay_seconds = 1.0  # è¯·æ±‚é—´å»¶æ—¶ï¼ˆç§’ï¼‰
        self.default_max_retries = 3  # é‡è¯•æ¬¡æ•°

    def _paginated_search(self, query: str, total_results: int,
                         sort_by: str = "relevance", order: str = "descending",
                         chunk_size: int = None, delay_seconds: float = None,
                         max_retries: int = None, show_progress: bool = False) -> ArxivResult:
        """
        å†…éƒ¨åˆ†é¡µæœç´¢æ–¹æ³•ï¼Œé€æ˜å¤„ç†å¤§é‡ç»“æœè·å–
        
        :param query: æœç´¢æŸ¥è¯¢
        :param total_results: æ€»ç›®æ ‡ç»“æœæ•°é‡
        :param sort_by: æ’åºæ–¹å¼
        :param order: æ’åºé¡ºåº
        :param chunk_size: æ¯æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤ä½¿ç”¨self.default_chunk_size
        :param delay_seconds: è¯·æ±‚é—´å»¶æ—¶ï¼Œé»˜è®¤ä½¿ç”¨self.default_delay_seconds
        :param max_retries: é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä½¿ç”¨self.default_max_retries
        :param show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        :return: åˆå¹¶çš„æœç´¢ç»“æœ
        """
        # ä½¿ç”¨é»˜è®¤é…ç½®
        if chunk_size is None:
            chunk_size = self.default_chunk_size
        if delay_seconds is None:
            delay_seconds = self.default_delay_seconds
        if max_retries is None:
            max_retries = self.default_max_retries
        
        # ç¡®ä¿chunk_sizeä¸è¶…è¿‡ArXiv APIé™åˆ¶
        chunk_size = min(chunk_size, 2000)
        
        logger.info(f"å¼€å§‹åˆ†é¡µæœç´¢ï¼Œç›®æ ‡: {total_results} ç¯‡ï¼Œåˆ†å—å¤§å°: {chunk_size}")
        
        all_results = []
        total_fetched = 0
        
        # åˆå§‹åŒ–è¿›åº¦æ¡
        progress_bar = None
        if show_progress:
            try:
                progress_bar = tqdm(total=total_results, desc="ArXivæœç´¢è¿›åº¦", unit="ç¯‡")
            except:
                # å¦‚æœtqdmä¸å¯ç”¨ï¼Œå¿½ç•¥è¿›åº¦æ¡
                pass
        
        try:
            # åˆ†æ‰¹è·å–ç»“æœ
            while total_fetched < total_results:
                # è®¡ç®—å½“å‰æ‰¹æ¬¡å¤§å°
                current_chunk = min(chunk_size, total_results - total_fetched)
                
                # å°è¯•è·å–å½“å‰æ‰¹æ¬¡
                retry_count = 0
                batch_results = None
                
                while retry_count <= max_retries:
                    try:
                        # æ„å»ºAPIè¯·æ±‚
                        base_url = "http://export.arxiv.org/api/query"
                        sort_map = {
                            "relevance": "relevance",
                            "lastUpdatedDate": "lastUpdatedDate", 
                            "submittedDate": "submittedDate"
                        }
                        
                        params = {
                            "search_query": query,
                            "start": total_fetched,
                            "max_results": current_chunk,
                            "sortBy": sort_map.get(sort_by, "relevance"),
                            "sortOrder": order
                        }
                        
                        logger.debug(f"è¯·æ±‚æ‰¹æ¬¡: start={total_fetched}, max_results={current_chunk}")
                        
                        # å‘èµ·è¯·æ±‚
                        response = requests.get(base_url, params=params, timeout=30)
                        response.raise_for_status()
                        
                        # è§£æå“åº”
                        feed = feedparser.parse(response.content)
                        
                        if not feed.entries:
                            logger.warning(f"æ‰¹æ¬¡ {total_fetched}-{total_fetched+current_chunk} æ— ç»“æœï¼Œå¯èƒ½å·²åˆ°è¾¾ç»“æœé›†æœ«å°¾")
                            break
                        
                        # è½¬æ¢ç»“æœæ ¼å¼
                        batch_results = []
                        for entry in feed.entries:
                            # æå–åˆ†ç±»
                            categories = []
                            if hasattr(entry, 'tags'):
                                categories = [tag.term for tag in entry.tags]
                            elif hasattr(entry, 'arxiv_primary_category'):
                                categories = [entry.arxiv_primary_category['term']]
                            
                            # æå–ä½œè€…ä¿¡æ¯
                            authors = []
                            if hasattr(entry, 'authors'):
                                authors = [author.name for author in entry.authors]
                            elif hasattr(entry, 'author'):
                                authors = [entry.author]
                            
                            result = {
                                'title': entry.title,
                                'link': entry.link,
                                'snippet': entry.summary,
                                'categories': ', '.join(categories) if categories else 'Unknown',
                                'authors': ', '.join(authors) if authors else 'Unknown'
                            }
                            batch_results.append(result)
                        
                        # æˆåŠŸè·å–æ‰¹æ¬¡ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        break
                        
                    except requests.exceptions.RequestException as e:
                        retry_count += 1
                        if retry_count <= max_retries:
                            logger.warning(f"è¯·æ±‚å¤±è´¥ï¼Œé‡è¯• {retry_count}/{max_retries}: {str(e)}")
                            time.sleep(delay_seconds * retry_count)  # é€’å¢å»¶æ—¶
                        else:
                            logger.error(f"æ‰¹æ¬¡è¯·æ±‚å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {str(e)}")
                            raise Exception(f"åˆ†é¡µæœç´¢å¤±è´¥: {str(e)}")
                    except Exception as e:
                        retry_count += 1
                        if retry_count <= max_retries:
                            logger.warning(f"è§£æå¤±è´¥ï¼Œé‡è¯• {retry_count}/{max_retries}: {str(e)}")
                            time.sleep(delay_seconds * retry_count)
                        else:
                            logger.error(f"æ‰¹æ¬¡è§£æå¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {str(e)}")
                            raise Exception(f"åˆ†é¡µæœç´¢è§£æå¤±è´¥: {str(e)}")
                
                # å¦‚æœæ²¡æœ‰è·å–åˆ°ç»“æœï¼Œå¯èƒ½å·²åˆ°è¾¾ç»“æœé›†æœ«å°¾
                if not batch_results:
                    logger.info(f"å·²è·å–æ‰€æœ‰å¯ç”¨ç»“æœï¼Œæ€»è®¡: {total_fetched} ç¯‡")
                    break
                
                # æ·»åŠ åˆ°æ€»ç»“æœé›†
                all_results.extend(batch_results)
                total_fetched += len(batch_results)
                
                # æ›´æ–°è¿›åº¦æ¡
                if progress_bar:
                    progress_bar.update(len(batch_results))
                
                logger.debug(f"å·²è·å– {total_fetched}/{total_results} ç¯‡æ–‡ç« ")
                
                # å¦‚æœè·å–çš„ç»“æœå°‘äºè¯·æ±‚æ•°é‡ï¼Œå¯èƒ½å·²åˆ°è¾¾ç»“æœé›†æœ«å°¾
                if len(batch_results) < current_chunk:
                    logger.info(f"ç»“æœé›†å·²è€—å°½ï¼Œæ€»è®¡è·å–: {total_fetched} ç¯‡")
                    break
                
                # è¯·æ±‚é—´å»¶æ—¶ï¼ˆé™¤äº†æœ€åä¸€æ¬¡è¯·æ±‚ï¼‰
                if total_fetched < total_results and delay_seconds > 0:
                    time.sleep(delay_seconds)
            
            logger.info(f"åˆ†é¡µæœç´¢å®Œæˆï¼Œæ€»è®¡è·å–: {len(all_results)} ç¯‡æ–‡ç« ")
            return ArxivResult(all_results)
            
        finally:
            # å…³é—­è¿›åº¦æ¡
            if progress_bar:
                progress_bar.close()

    def arxivSearch(self, query: str,
                    num_results: int = 20,
                    sort_by: str = "relevance",
                    order: str = "desc",
                    max_results: int = None,
                    kwargs: dict = None,
                    use_direct_api: bool = True,
                    chunk_size: int = None,
                    delay_seconds: float = None,
                    max_retries: int = None,
                    show_progress: bool = False
                    ) -> ArxivResult:
        """
        ä½¿ç”¨ ArXiv API ç›´æ¥æœç´¢ï¼Œç°åœ¨æ”¯æŒå¤§è§„æ¨¡æœç´¢ï¼ˆ1000+/10000+ç¯‡æ–‡ç« ï¼‰

        :param query: æœç´¢çš„æŸ¥è¯¢
        :type query: str
        :param num_results: è¿”å›çš„ç»“æœæ•°é‡ï¼ˆæ— ä¸Šé™ï¼Œè‡ªåŠ¨åˆ†é¡µå¤„ç†ï¼‰
        :type num_results: int
        :param sort_by: æ’åºæ–¹å¼ï¼Œå¯é€‰ "relevance", "lastUpdatedDate", "submittedDate"
        :type sort_by: str
        :param order: æ’åºé¡ºåºï¼Œå¯é€‰ "asc" (å‡åº) æˆ– "desc" (é™åº)
        :type order: str
        :param max_results: ä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼Œä½†ä¸å†ä½¿ç”¨
        :type max_results: int
        :param kwargs: ä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼Œä½†ä¸å†ä½¿ç”¨
        :type kwargs: dict
        :param use_direct_api: ä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼Œæ€»æ˜¯ä½¿ç”¨ç›´æ¥API
        :type use_direct_api: bool
        :param chunk_size: åˆ†é¡µå—å¤§å°ï¼Œä»…åœ¨å¤§è§„æ¨¡æœç´¢æ—¶ä½¿ç”¨
        :type chunk_size: int
        :param delay_seconds: è¯·æ±‚é—´å»¶æ—¶ï¼Œä»…åœ¨å¤§è§„æ¨¡æœç´¢æ—¶ä½¿ç”¨
        :type delay_seconds: float
        :param max_retries: é‡è¯•æ¬¡æ•°ï¼Œä»…åœ¨å¤§è§„æ¨¡æœç´¢æ—¶ä½¿ç”¨
        :type max_retries: int
        :param show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œå¤§è§„æ¨¡æœç´¢æ—¶æ¨èå¯ç”¨
        :type show_progress: bool
        :return: æœç´¢ç»“æœ
        :rtype: ArxivResult
        """
        
        # ç°åœ¨æ€»æ˜¯ä½¿ç”¨ç›´æ¥ArXiv APIï¼Œæ”¯æŒå¤§è§„æ¨¡æœç´¢
        logger.info(f"ä½¿ç”¨ç›´æ¥ArXiv APIæœç´¢: {query} ({num_results}ç¯‡)")
        
        return self.directArxivSearch(
            query=query,
            num_results=num_results,
            sort_by=sort_by,
            order="descending" if order == "desc" else "ascending",
            chunk_size=chunk_size,
            delay_seconds=delay_seconds,
            max_retries=max_retries,
            show_progress=show_progress
        )

    # ç§»é™¤åˆ†é¡µæœç´¢æ–¹æ³•ï¼Œç›´æ¥APIæ”¯æŒå¤§é‡ç»“æœ

    def getLatestPapers(self, query: str, num_results: int = 20) -> ArxivResult:
        """
        è·å–æœ€æ–°çš„è®ºæ–‡ï¼ŒæŒ‰æäº¤æ—¥æœŸé™åºæ’åˆ—
        
        :param query: æœç´¢çš„æŸ¥è¯¢
        :type query: str
        :param num_results: è¿”å›çš„ç»“æœæ•°é‡
        :type num_results: int
        :return: æœç´¢ç»“æœ
        :rtype: ArxivResult
        """
        return self.arxivSearch(query=query, 
                               num_results=num_results,
                               sort_by="submittedDate", 
                               order="desc")

    def getRecentlyUpdated(self, query: str, num_results: int = 20) -> ArxivResult:
        """
        è·å–æœ€è¿‘æ›´æ–°çš„è®ºæ–‡ï¼ŒæŒ‰æ›´æ–°æ—¥æœŸé™åºæ’åˆ—
        
        :param query: æœç´¢çš„æŸ¥è¯¢
        :type query: str
        :param num_results: è¿”å›çš„ç»“æœæ•°é‡
        :type num_results: int
        :return: æœç´¢ç»“æœ
        :rtype: ArxivResult
        """
        return self.arxivSearch(query=query, 
                               num_results=num_results,
                               sort_by="lastUpdatedDate", 
                               order="desc")

    def searchWithHighLimit(self, query: str, num_results: int = 50, 
                           sort_by: str = "relevance", order: str = "desc",
                           max_single_request: int = 20,
                           chunk_size: int = None,
                           delay_seconds: float = None,
                           max_retries: int = None,
                           show_progress: bool = True) -> ArxivResult:
        """
        é«˜é™åˆ¶æœç´¢æ–¹æ³•ï¼Œä¸“é—¨ä¼˜åŒ–å¤§è§„æ¨¡æœç´¢ï¼ˆ1000+/10000+ç¯‡æ–‡ç« ï¼‰
        
        :param query: æœç´¢æŸ¥è¯¢
        :param num_results: ç›®æ ‡ç»“æœæ•°é‡ï¼ˆæ— ä¸Šé™ï¼Œè‡ªåŠ¨åˆ†é¡µå¤„ç†ï¼‰
        :param sort_by: æ’åºæ–¹å¼
        :param order: æ’åºé¡ºåº
        :param max_single_request: ä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼Œä½†ä¸å†ä½¿ç”¨
        :param chunk_size: åˆ†é¡µå—å¤§å°ï¼Œé»˜è®¤ä½¿ç”¨ä¼˜åŒ–åçš„å¤§å—ï¼ˆ1500ï¼‰
        :param delay_seconds: è¯·æ±‚é—´å»¶æ—¶ï¼Œé»˜è®¤ä½¿ç”¨æ›´çŸ­å»¶æ—¶ï¼ˆ0.5ç§’ï¼‰
        :param max_retries: é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤æ›´å¤šé‡è¯•ï¼ˆ5æ¬¡ï¼‰
        :param show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œé»˜è®¤å¼€å¯
        :return: æœç´¢ç»“æœ
        """
        # ä¸ºå¤§è§„æ¨¡æœç´¢ä¼˜åŒ–çš„é»˜è®¤é…ç½®
        if chunk_size is None:
            chunk_size = 1500  # æ›´å¤§çš„åˆ†é¡µå—ï¼Œå‡å°‘è¯·æ±‚æ¬¡æ•°
        if delay_seconds is None:
            delay_seconds = 0.5  # æ›´çŸ­çš„å»¶æ—¶ï¼Œæé«˜æ•ˆç‡
        if max_retries is None:
            max_retries = 5  # æ›´å¤šé‡è¯•ï¼Œç¡®ä¿ç¨³å®šæ€§
        
        logger.info(f"é«˜é™åˆ¶æœç´¢æ¨¡å¼ï¼Œç›®æ ‡: {num_results} ç¯‡æ–‡ç« ")
        
        return self.arxivSearch(
            query=query, 
            num_results=num_results,
            sort_by=sort_by,
            order=order,
            chunk_size=chunk_size,
            delay_seconds=delay_seconds,
            max_retries=max_retries,
            show_progress=show_progress
        )

    def directArxivSearch(self, query: str, num_results: int = 20,
                         sort_by: str = "relevance", order: str = "descending",
                         chunk_size: int = None, delay_seconds: float = None,
                         max_retries: int = None, show_progress: bool = False) -> ArxivResult:
        """
        ç›´æ¥ä½¿ç”¨ArXiv APIè¿›è¡Œæœç´¢ï¼Œè·å–æœ€æ–°æ•°æ®
        ç°åœ¨æ”¯æŒå¤§è§„æ¨¡æœç´¢ï¼ˆ1000+/10000+ç¯‡æ–‡ç« ï¼‰
        
        :param query: æœç´¢æŸ¥è¯¢
        :param num_results: ç»“æœæ•°é‡ï¼ˆæ— ä¸Šé™ï¼Œè‡ªåŠ¨åˆ†é¡µå¤„ç†ï¼‰
        :param sort_by: æ’åºæ–¹å¼ ("relevance", "lastUpdatedDate", "submittedDate")
        :param order: æ’åºé¡ºåº ("ascending", "descending")
        :param chunk_size: åˆ†é¡µå—å¤§å°ï¼Œé»˜è®¤ä½¿ç”¨ç±»é…ç½®
        :param delay_seconds: è¯·æ±‚é—´å»¶æ—¶ï¼Œé»˜è®¤ä½¿ç”¨ç±»é…ç½®
        :param max_retries: é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä½¿ç”¨ç±»é…ç½®
        :param show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆå¤§è§„æ¨¡æœç´¢æ—¶æ¨èï¼‰
        :return: æœç´¢ç»“æœ
        """
        # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†é¡µæœç´¢
        if num_results > 2000:
            logger.info(f"æ£€æµ‹åˆ°å¤§è§„æ¨¡æœç´¢è¯·æ±‚({num_results}ç¯‡)ï¼Œå¯ç”¨åˆ†é¡µæ¨¡å¼")
            return self._paginated_search(
                query=query,
                total_results=num_results,
                sort_by=sort_by,
                order=order,
                chunk_size=chunk_size,
                delay_seconds=delay_seconds,
                max_retries=max_retries,
                show_progress=show_progress
            )
        
        # æ ‡å‡†å•æ¬¡è¯·æ±‚ï¼ˆ<=2000ç¯‡ï¼‰
        base_url = "http://export.arxiv.org/api/query"
        
        # æ˜ å°„æ’åºå‚æ•°
        sort_map = {
            "relevance": "relevance",
            "lastUpdatedDate": "lastUpdatedDate", 
            "submittedDate": "submittedDate"
        }
        
        params = {
            "search_query": query,
            "start": 0,
            "max_results": min(num_results, 2000),  # ArXiv APIå•æ¬¡é™åˆ¶
            "sortBy": sort_map.get(sort_by, "relevance"),
            "sortOrder": order
        }
        
        try:
            logger.info(f"ç›´æ¥è°ƒç”¨ArXiv APIæœç´¢: {query} (å•æ¬¡è¯·æ±‚ï¼Œ{num_results}ç¯‡)")
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
            
            # è§£æRSS/Atomæ ¼å¼å“åº”
            feed = feedparser.parse(response.content)
            
            if not feed.entries:
                logger.warning("ArXiv APIæœªè¿”å›ç»“æœ")
                return ArxivResult([])
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            results = []
            for entry in feed.entries[:num_results]:
                # æå–åˆ†ç±»
                categories = []
                if hasattr(entry, 'tags'):
                    categories = [tag.term for tag in entry.tags]
                elif hasattr(entry, 'arxiv_primary_category'):
                    categories = [entry.arxiv_primary_category['term']]
                
                # æå–ä½œè€…ä¿¡æ¯
                authors = []
                if hasattr(entry, 'authors'):
                    authors = [author.name for author in entry.authors]
                elif hasattr(entry, 'author'):
                    authors = [entry.author]
                
                result = {
                    'title': entry.title,
                    'link': entry.link,
                    'snippet': entry.summary,
                    'categories': ', '.join(categories) if categories else 'Unknown',
                    'authors': ', '.join(authors) if authors else 'Unknown'
                }
                results.append(result)
            
            logger.info(f"ArXiv APIè¿”å› {len(results)} ä¸ªç»“æœ")
            return ArxivResult(results)
            
        except requests.exceptions.RequestException as e:
            logger.error(f"ArXiv APIè¯·æ±‚å¤±è´¥: {str(e)}")
            # å›é€€åˆ°SearxNGæœç´¢
            logger.info("å›é€€åˆ°SearxNGæœç´¢")
            return self.arxivSearch(query, num_results, sort_by, "desc" if order == "descending" else "asc")
        except Exception as e:
            logger.error(f"ArXiv APIè§£æå¤±è´¥: {str(e)}")
            return self.arxivSearch(query, num_results, sort_by, "desc" if order == "descending" else "asc")

    def getLatestPapersDirectly(self, query: str, num_results: int = 20) -> ArxivResult:
        """
        ç›´æ¥ä»ArXiv APIè·å–æœ€æ–°è®ºæ–‡
        """
        return self.directArxivSearch(query, num_results, "submittedDate", "descending")

    def getMostRelevantPapers(self, query: str, num_results: int = 20) -> ArxivResult:
        """
        è·å–æœ€ç›¸å…³çš„è®ºæ–‡ï¼ŒæŒ‰ç›¸å…³æ€§æ’åº
        
        :param query: æœç´¢çš„æŸ¥è¯¢
        :type query: str
        :param num_results: è¿”å›çš„ç»“æœæ•°é‡
        :type num_results: int
        :return: æœç´¢ç»“æœ
        :rtype: ArxivResult
        """
        return self.directArxivSearch(query, num_results, "relevance", "descending")

    def searchPapersByDateRange(self, query: str, start_year: int, end_year: int, num_results: int = 20) -> ArxivResult:
        """
        æœç´¢æŒ‡å®šå¹´ä»½èŒƒå›´å†…çš„è®ºæ–‡
        
        :param query: æœç´¢çš„æŸ¥è¯¢
        :type query: str
        :param start_year: å¼€å§‹å¹´ä»½
        :type start_year: int
        :param end_year: ç»“æŸå¹´ä»½
        :type end_year: int
        :param num_results: è¿”å›çš„ç»“æœæ•°é‡
        :type num_results: int
        :return: æœç´¢ç»“æœ
        :rtype: ArxivResult
        """
        # æ„é€ å¸¦å¹´ä»½èŒƒå›´çš„æŸ¥è¯¢
        # ArXiv APIæ”¯æŒsubmittedDateèŒƒå›´æŸ¥è¯¢
        date_query = f"{query} AND submittedDate:[{start_year}0101* TO {end_year}1231*]"
        
        logger.info(f"æœç´¢å¹´ä»½èŒƒå›´ {start_year}-{end_year} çš„è®ºæ–‡: {query}")
        return self.directArxivSearch(date_query, num_results, "submittedDate", "descending")

    def searchPapersAfterYear(self, query: str, after_year: int, num_results: int = 20) -> ArxivResult:
        """
        æœç´¢æŸå¹´ä¹‹åçš„è®ºæ–‡
        
        :param query: æœç´¢çš„æŸ¥è¯¢
        :type query: str
        :param after_year: èµ·å§‹å¹´ä»½ï¼ˆåŒ…å«è¯¥å¹´ï¼‰
        :type after_year: int
        :param num_results: è¿”å›çš„ç»“æœæ•°é‡
        :type num_results: int
        :return: æœç´¢ç»“æœ
        :rtype: ArxivResult
        """
        from datetime import datetime
        current_year = datetime.now().year
        
        # æ„é€ å¸¦å¹´ä»½èŒƒå›´çš„æŸ¥è¯¢ï¼Œä»æŒ‡å®šå¹´ä»½åˆ°å½“å‰å¹´ä»½
        date_query = f"{query} AND submittedDate:[{after_year}0101* TO {current_year}1231*]"
        
        logger.info(f"æœç´¢ {after_year} å¹´ä¹‹åçš„è®ºæ–‡: {query}")
        return self.directArxivSearch(date_query, num_results, "submittedDate", "descending")

    def searchPapersByMode(self, query: str, mode: ArxivSearchMode, num_results: int = 20, 
                          start_year: int = None, end_year: int = None, after_year: int = None) -> ArxivResult:
        """
        æ ¹æ®æœç´¢æ¨¡å¼æœç´¢è®ºæ–‡çš„ç»Ÿä¸€æ¥å£
        
        :param query: æœç´¢çš„æŸ¥è¯¢
        :type query: str
        :param mode: æœç´¢æ¨¡å¼
        :type mode: ArxivSearchMode
        :param num_results: è¿”å›çš„ç»“æœæ•°é‡
        :type num_results: int
        :param start_year: å¼€å§‹å¹´ä»½ï¼ˆä»…ç”¨äºDATE_RANGEæ¨¡å¼ï¼‰
        :type start_year: int
        :param end_year: ç»“æŸå¹´ä»½ï¼ˆä»…ç”¨äºDATE_RANGEæ¨¡å¼ï¼‰
        :type end_year: int
        :param after_year: èµ·å§‹å¹´ä»½ï¼ˆä»…ç”¨äºAFTER_YEARæ¨¡å¼ï¼‰
        :type after_year: int
        :return: æœç´¢ç»“æœ
        :rtype: ArxivResult
        """
        if mode == ArxivSearchMode.LATEST:
            return self.getLatestPapers(query, num_results)
        elif mode == ArxivSearchMode.MOST_RELEVANT:
            return self.getMostRelevantPapers(query, num_results)
        elif mode == ArxivSearchMode.RECENTLY_UPDATED:
            return self.getRecentlyUpdated(query, num_results)
        elif mode == ArxivSearchMode.DATE_RANGE:
            if start_year is None or end_year is None:
                raise ValueError("DATE_RANGEæ¨¡å¼éœ€è¦æä¾›start_yearå’Œend_yearå‚æ•°")
            return self.searchPapersByDateRange(query, start_year, end_year, num_results)
        elif mode == ArxivSearchMode.AFTER_YEAR:
            if after_year is None:
                raise ValueError("AFTER_YEARæ¨¡å¼éœ€è¦æä¾›after_yearå‚æ•°")
            return self.searchPapersAfterYear(query, after_year, num_results)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢æ¨¡å¼: {mode}")

    # ç§»é™¤SearxNGç›¸å…³æ–¹æ³•ï¼Œç°åœ¨å®Œå…¨ä½¿ç”¨ç›´æ¥API


if __name__ == "__main__":
    # ArXiv API å·¥å…·æµ‹è¯•å’Œç»“æ„åŒ–æ˜¾ç¤ºåŠŸèƒ½æ¼”ç¤º
    arxiv_tool = ArxivTool()
    
    print("=== ArXiv API å·¥å…·åŠŸèƒ½æµ‹è¯• ===")
    
    # æµ‹è¯•1: åŸºç¡€æœç´¢
    print("ğŸ” æµ‹è¯•1: åŸºç¡€æœç´¢ - machine learning (10ä¸ªç»“æœ)")
    results = arxiv_tool.arxivSearch(query="machine learning", num_results=10)
    print(f"âœ… æœç´¢å®Œæˆ: {results.num_results} ä¸ªç»“æœ\n")
    
    # æ¼”ç¤ºç»“æ„åŒ–æ˜¾ç¤º - é™åˆ¶æ˜¾ç¤ºå‰3ä¸ª
    print("ğŸ“‹ ç»“æ„åŒ–æ˜¾ç¤ºæ¼”ç¤º - å‰3ä¸ªç»“æœ:")
    results.display_results(display_range="limited", max_display=3)
    
    print("\n" + "="*80 + "\n")
    
    # æµ‹è¯•2: ç®€æ´æ˜¾ç¤ºæ¨¡å¼
    print("ğŸ” æµ‹è¯•2: æœ€æ–°è®ºæ–‡æœç´¢ - deep learning")
    latest_papers = arxiv_tool.getLatestPapersDirectly(query="deep learning", num_results=15)
    
    print("ğŸ“‹ ç®€æ´æ˜¾ç¤ºæ¼”ç¤º:")
    latest_papers.display_brief(max_display=5)
    
    print("\n" + "="*80 + "\n")
    
    # æµ‹è¯•3: ä»…æ ‡é¢˜æ¨¡å¼
    print("ğŸ” æµ‹è¯•3: ç¥ç»ç½‘ç»œæœç´¢")
    nn_results = arxiv_tool.arxivSearch(query="neural networks", num_results=20)
    
    print("ğŸ“‹ ä»…æ ‡é¢˜æ˜¾ç¤ºæ¼”ç¤º:")
    nn_results.display_titles_only(max_display=8)
    
    print("\n" + "="*80 + "\n")
    
    # æµ‹è¯•4: å®Œæ•´æ˜¾ç¤ºæ¨¡å¼ï¼ˆå°æ•°æ®é›†ï¼‰
    print("ğŸ” æµ‹è¯•4: è®¡ç®—æœºè§†è§‰æœç´¢")
    cv_results = arxiv_tool.arxivSearch(query="computer vision", num_results=5)
    
    print("ğŸ“‹ å®Œæ•´æ˜¾ç¤ºæ¼”ç¤º - æ˜¾ç¤ºå…¨éƒ¨:")
    cv_results.display_results(display_range="all", show_summary=True)
    
    print("\n" + "="*80 + "\n")
    
    # æ¼”ç¤ºå¹´ä»½ç­›é€‰åŠŸèƒ½
    if latest_papers.num_results > 0:
        print("ğŸ“‹ å¹´ä»½ç­›é€‰æ¼”ç¤º - ç­›é€‰2020å¹´åçš„è®ºæ–‡:")
        recent_papers = latest_papers.get_papers_by_date_range(start_year=2020)
        if recent_papers.num_results > 0:
            recent_papers.display_brief(max_display=50)
        else:
            print("   æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡")
    
    print("\n=== ğŸ‰ ArXiv API é‡æ„å®Œæˆï¼ç°åœ¨æ”¯æŒä¸°å¯Œçš„ç»“æ„åŒ–æ˜¾ç¤ºåŠŸèƒ½ ===")
    
    # OCR åŠŸèƒ½æµ‹è¯•
    print("\n" + "="*60)
    print("ğŸ” OCRåŠŸèƒ½æµ‹è¯• - PyMuPDF + PaddleOCR 3.0")
    print("="*60)
    
    if results.num_results > 0:
        test_paper = results.results[2]
        print(f"ğŸ“„ æµ‹è¯•è®ºæ–‡: {test_paper.title[:60]}...")
        
        try:
            # ä¸‹è½½PDF
            print("ğŸ“¥ ä¸‹è½½PDFä¸­...")
            test_paper.downloadPdf()
            
            # æµ‹è¯•1: é»˜è®¤PyMuPDFæ–¹æ³•
            print("\nğŸ” æµ‹è¯•1: PyMuPDFå¿«é€Ÿæ–‡æœ¬æå–...")
            ocr_result, status_info = test_paper.performOCR(use_paddleocr=False)
            
            if ocr_result:
                print(f"âœ… PyMuPDFå®Œæˆï¼Œæå–æ–‡æœ¬: {len(ocr_result)} å­—ç¬¦")
                print(f"ğŸ“„ å¤„ç†äº† {status_info['processed_pages']}/{status_info['total_pages']} é¡µ")
                if status_info['is_oversized']:
                    print("âš ï¸ æ–‡æ¡£è¶…é•¿ï¼Œå¯èƒ½æ˜¯æ¯•ä¸šè®ºæ–‡æˆ–ä¹¦ç±")
                print(f"ğŸ“ PyMuPDFç»“æœé¢„è§ˆ: {ocr_result[:200]}..." if len(ocr_result) > 200 else f"ğŸ“ PyMuPDFç»“æœ: {ocr_result}")
            else:
                print("âŒ PyMuPDFæœªæå–åˆ°æ–‡æœ¬")
            
            # æµ‹è¯•2: PaddleOCRç»“æ„åŒ–è¯†åˆ«
            print("\nğŸ” æµ‹è¯•2: PaddleOCR 3.0ç»“æ„åŒ–è¯†åˆ«...")
            paddle_result, paddle_status = test_paper.performOCR(use_paddleocr=True)
            
            if paddle_result:
                print(f"âœ… PaddleOCRå®Œæˆï¼Œæå–Markdown: {len(paddle_result)} å­—ç¬¦")
                print(f"ğŸ“„ å¤„ç†äº† {paddle_status['processed_pages']}/{paddle_status['total_pages']} é¡µ")
                if paddle_status.get('images_count', 0) > 0:
                    print(f"ğŸ–¼ï¸ æå–å›¾ç‰‡: {paddle_status['images_count']} å¼ ")
                if paddle_status['is_oversized']:
                    print("âš ï¸ æ–‡æ¡£è¶…é•¿ï¼Œå¯èƒ½æ˜¯æ¯•ä¸šè®ºæ–‡æˆ–ä¹¦ç±")
                print(f"ğŸ“ PaddleOCR Markdowné¢„è§ˆ: {paddle_result[:300]}..." if len(paddle_result) > 300 else f"ğŸ“ PaddleOCRç»“æœ: {paddle_result}")
                
                # æ˜¾ç¤ºPaddleOCRç‰¹æœ‰åŠŸèƒ½
                paddle_markdown = test_paper.getPaddleOcrResult()
                paddle_images = test_paper.getPaddleOcrImages()
                print(f"ğŸ¯ PaddleOCRç‰¹è‰²åŠŸèƒ½:")
                print(f"   - ç»“æ„åŒ–Markdown: {len(paddle_markdown)} å­—ç¬¦" if paddle_markdown else "   - ç»“æ„åŒ–Markdown: æ— ")
                print(f"   - å›¾ç‰‡æå–: {len(paddle_images)} å¼ å›¾ç‰‡")
                
            else:
                print("âŒ PaddleOCRæœªæå–åˆ°å†…å®¹")
                
        except Exception as e:
            print(f"âŒ OCRæµ‹è¯•å¤±è´¥: {str(e)}")
    
    print("="*60)
    
    # ä½¿ç”¨æŒ‡å—
    print("\nğŸ“– ArXivå·¥å…·ä½¿ç”¨æŒ‡å—:")
    print("="*40)
    print("ğŸ” æœç´¢å’Œæ˜¾ç¤º:")
    print("   results.display_results()           # å®Œæ•´æ˜¾ç¤ºæ‰€æœ‰ç»“æœ")
    print("   results.display_results('limited')  # é™åˆ¶æ˜¾ç¤ºå‰Nä¸ª") 
    print("   results.display_brief()             # ç®€æ´æ¨¡å¼")
    print("   results.display_titles_only()       # ä»…æ˜¾ç¤ºæ ‡é¢˜")
    print("   results.get_papers_by_date_range()  # æŒ‰å¹´ä»½ç­›é€‰")
    print("\nğŸ“„ OCRåŠŸèƒ½:")
    print("   paper.performOCR()                  # é»˜è®¤PyMuPDFå¿«é€Ÿæå–")
    print("   paper.performOCR(use_paddleocr=True) # PaddleOCRç»“æ„åŒ–è¯†åˆ«")
    print("   paper.getOcrResult()                # è·å–OCRæ–‡æœ¬ç»“æœ")
    print("   paper.getPaddleOcrResult()          # è·å–PaddleOCR Markdown")
    print("   paper.getPaddleOcrImages()          # è·å–æå–çš„å›¾ç‰‡")
    print("   paper.savePaddleOcrToFile(path)     # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶")
    print("   paper.clearPaddleOcrResult()        # æ¸…ç†PaddleOCRæ•°æ®")
    print("\nğŸ’¡ ä¾èµ–è¦æ±‚:")
    print("   - PaddlePaddle >= 3.0.0 (CUDA 12.6)")
    print("   - PaddleOCR >= 3.0.0")
    print("   - PyMuPDF >= 1.20.0")
