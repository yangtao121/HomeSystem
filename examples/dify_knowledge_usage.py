#!/usr/bin/env python3
"""
Dify Knowledge Base Usage Examples

This example demonstrates how to use the unified dify_knowledge module
for managing datasets, documents, and segments in Dify Knowledge Base.
"""

import os
import time
from pathlib import Path
import sys

# Add the parent directory to the path so we can import HomeSystem
sys.path.append(str(Path(__file__).parent.parent))

from HomeSystem.integrations.dify import (
    # å®¢æˆ·ç«¯
    DifyKnowledgeBaseClient,
    
    # é…ç½®ç±»
    DifyKnowledgeBaseConfig,
    get_config,
    UploadConfig,
    ProcessRule,
    IndexingTechnique,
    ProcessMode,
    DocumentType,
    
    # æ•°æ®æ¨¡å‹
    DifyDatasetModel,
    DifyDocumentModel,
    DifySegmentModel,
    DatasetStatus,
    DocumentStatus,
    IndexingStatus,
    
    # å¼‚å¸¸ç±»
    DifyKnowledgeBaseError,
    DatasetNotFoundError,
    DocumentUploadError,
    DatasetCreationError
)

# Additional imports for the example
try:
    from HomeSystem.integrations.dify.dify_knowledge import TimeoutConfig
except ImportError:
    # Fallback if TimeoutConfig is not available
    TimeoutConfig = None


def create_and_setup_knowledge_base_example():
    """å®Œæ•´çš„çŸ¥è¯†åº“åˆ›å»ºå’Œè®¾ç½®ç¤ºä¾‹"""
    print("=== çŸ¥è¯†åº“åˆ›å»ºå’Œè®¾ç½®ç¤ºä¾‹ ===")
    
    # 1. ä»ç¯å¢ƒå˜é‡è·å–é…ç½®ï¼ˆæ¨èæ–¹å¼ï¼‰
    try:
        config = get_config()
        print("âœ… æˆåŠŸä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®")
    except Exception:
        # å¦‚æœç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆéœ€è¦æ‰‹åŠ¨è®¾ç½®APIå¯†é’¥ï¼‰
        if TimeoutConfig:
            timeout_config = TimeoutConfig(
                connect_timeout=30,
                read_timeout=60,
                upload_timeout=300
            )
        else:
            timeout_config = None
            
        config = DifyKnowledgeBaseConfig(
            api_key="your-dify-api-key",  # è¯·æ›¿æ¢ä¸ºå®é™…APIå¯†é’¥
            base_url="https://api.dify.ai",  # ä¸åŒ…å«/v1ï¼Œå› ä¸º_make_requestä¼šè‡ªåŠ¨æ·»åŠ 
        )
        if timeout_config:
            config.timeout_config = timeout_config
        print("âš ï¸  ä½¿ç”¨é»˜è®¤é…ç½®ï¼Œè¯·è®¾ç½®æ­£ç¡®çš„APIå¯†é’¥")
    
    # 2. åˆ›å»ºå®¢æˆ·ç«¯å¹¶è¿›è¡Œå¥åº·æ£€æŸ¥
    client = DifyKnowledgeBaseClient(config)
    
    print("ğŸ” è¿›è¡Œè¿æ¥å¥åº·æ£€æŸ¥...")
    if not client.health_check():
        print("âŒ è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return None
    print("âœ… è¿æ¥å¥åº·æ£€æŸ¥é€šè¿‡")
    
    try:
        # 3. åˆ›å»ºçŸ¥è¯†åº“
        print("\nğŸ“ åˆ›å»ºæ–°çŸ¥è¯†åº“...")
        dataset = client.create_dataset(
            name="AIç ”ç©¶çŸ¥è¯†åº“",
            description="åŒ…å«äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç›¸å…³çš„ç ”ç©¶æ–‡æ¡£å’Œè®ºæ–‡",
            permission="only_me"  # ä»…è‡ªå·±å¯è§
        )
        print(f"âœ… çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸ:")
        print(f"   - åç§°: {dataset.name}")
        print(f"   - ID: {dataset.dify_dataset_id}")
        print(f"   - æè¿°: {dataset.description}")
        print(f"   - æƒé™: {dataset.permission}")
        
        # 4. é…ç½®ä¸Šä¼ å‚æ•°
        print("\nâš™ï¸  é…ç½®æ–‡æ¡£å¤„ç†è§„åˆ™...")
        upload_config = UploadConfig(
            indexing_technique=IndexingTechnique.HIGH_QUALITY,
            process_rule=ProcessRule(
                mode=ProcessMode.CUSTOM,
                pre_processing_rules=[
                    {"id": "remove_extra_spaces", "enabled": True},
                    {"id": "remove_urls_emails", "enabled": False}
                ],
                segmentation={
                    "separator": "\\n\\n",  # æŒ‰æ®µè½åˆ†å‰²
                    "max_tokens": 500,     # æ¯ä¸ªåˆ†ç‰‡æœ€å¤§tokenæ•°
                    "chunk_overlap": 50    # åˆ†ç‰‡é‡å tokenæ•°
                }
            ),
            duplicate_check=True  # å¯ç”¨é‡å¤æ£€æŸ¥
        )
        print("âœ… å¤„ç†è§„åˆ™é…ç½®å®Œæˆ:")
        print(f"   - ç´¢å¼•æŠ€æœ¯: {upload_config.indexing_technique.value}")
        print(f"   - å¤„ç†æ¨¡å¼: {upload_config.process_rule.mode.value}")
        print(f"   - åˆ†ç‰‡è®¾ç½®: æœ€å¤§{upload_config.process_rule.segmentation['max_tokens']}tokens")
        
        return dataset, client, upload_config
        
    except DatasetCreationError as e:
        print(f"âŒ çŸ¥è¯†åº“åˆ›å»ºå¤±è´¥: {e}")
        print(f"   é”™è¯¯ç : {e.error_code}")
        return None
    except DifyKnowledgeBaseError as e:
        print(f"âŒ æ“ä½œå¤±è´¥: {e}")
        return None


def upload_operations_example():
    """è¯¦ç»†çš„æ–‡æ¡£ä¸Šä¼ æ“ä½œç¤ºä¾‹"""
    print("\n=== æ–‡æ¡£ä¸Šä¼ æ“ä½œç¤ºä¾‹ ===")
    
    # è·å–çŸ¥è¯†åº“è®¾ç½®
    try:
        result = create_and_setup_knowledge_base_example()
        if not result:
            print("âŒ æ— æ³•ç»§ç»­ä¸Šä¼ ç¤ºä¾‹ï¼ŒçŸ¥è¯†åº“åˆ›å»ºå¤±è´¥")
            return
    except Exception as e:
        print(f"âŒ çŸ¥è¯†åº“åˆ›å»ºå¤±è´¥: {e}")
        return
    
    dataset, client, upload_config = result
    
    try:
        print("\nğŸ“ å¼€å§‹æ–‡æ¡£ä¸Šä¼ æ“ä½œ...")
        
        # 1. ä¸Šä¼ æ–‡æœ¬æ–‡æ¡£
        print("\n1ï¸âƒ£ ä¸Šä¼ æ–‡æœ¬æ–‡æ¡£...")
        text_content = """
        # Transformeræ¶æ„è¯¦è§£
        
        Transformeræ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„ï¼Œç”±Vaswaniç­‰äººåœ¨2017å¹´çš„è®ºæ–‡"Attention Is All You Need"ä¸­æå‡ºã€‚
        
        ## æ ¸å¿ƒç‰¹ç‚¹
        - å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‘’å¼ƒäº†å¾ªç¯å’Œå·ç§¯
        - å¹¶è¡Œè®¡ç®—èƒ½åŠ›å¼ºï¼Œè®­ç»ƒæ•ˆç‡é«˜
        - åœ¨æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šå–å¾—çªç ´æ€§è¿›å±•
        
        ## ä¸»è¦ç»„ä»¶
        1. **å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šå…è®¸æ¨¡å‹å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒä½ç½®
        2. **ä½ç½®ç¼–ç **ï¼šä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®æä¾›ä½ç½®ä¿¡æ¯
        3. **å‰é¦ˆç¥ç»ç½‘ç»œ**ï¼šå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹è¿›è¡Œå˜æ¢
        4. **æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–**ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹
        
        ## åº”ç”¨é¢†åŸŸ
        - è‡ªç„¶è¯­è¨€å¤„ç†ï¼šBERTã€GPTã€T5ç­‰
        - è®¡ç®—æœºè§†è§‰ï¼šVision Transformer (ViT)
        - å¤šæ¨¡æ€ï¼šCLIPã€DALL-Eç­‰
        """
        
        document1 = client.upload_document_text(
            dataset_id=dataset.dify_dataset_id,
            name="Transformeræ¶æ„è¯¦è§£",
            text=text_content.strip(),
            upload_config=upload_config
        )
        print(f"âœ… æ–‡æœ¬æ–‡æ¡£ä¸Šä¼ æˆåŠŸ:")
        print(f"   - æ–‡æ¡£å: {document1.name}")
        print(f"   - æ–‡æ¡£ID: {document1.dify_document_id}")
        print(f"   - å­—ç¬¦æ•°: {document1.character_count}")
        print(f"   - è¯æ•°: {document1.word_count}")
        
        # 2. åˆ›å»ºå¹¶ä¸Šä¼ æ–‡ä»¶æ–‡æ¡£ï¼ˆæ¨¡æ‹Ÿï¼‰
        print("\n2ï¸âƒ£ æ¨¡æ‹Ÿæ–‡ä»¶ä¸Šä¼ ...")
        # è¿™é‡Œæ¨¡æ‹Ÿæ–‡ä»¶ä¸Šä¼ ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦çœŸå®æ–‡ä»¶è·¯å¾„
        sample_file_content = """
        # æ·±åº¦å­¦ä¹ åŸºç¡€æ¦‚å¿µ
        
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯æ·±å±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®è¡¨ç¤ºã€‚
        
        ## åŸºæœ¬æ¦‚å¿µ
        - ç¥ç»å…ƒï¼šå¤„ç†ä¿¡æ¯çš„åŸºæœ¬å•å…ƒ
        - å±‚ï¼šç¥ç»å…ƒçš„ç»„ç»‡ç»“æ„
        - æ¿€æ´»å‡½æ•°ï¼šå¼•å…¥éçº¿æ€§
        - åå‘ä¼ æ’­ï¼šè®­ç»ƒç®—æ³•
        
        ## å¸¸è§æ¶æ„
        1. å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
        2. å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰
        3. é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰
        4. Transformer
        """
        
        # å¦‚æœæœ‰å®é™…æ–‡ä»¶ï¼Œå¯ä»¥è¿™æ ·ä¸Šä¼ ï¼š
        # document2 = client.upload_document_file(
        #     dataset_id=dataset.dify_dataset_id,
        #     file_path="/path/to/your/document.pdf",
        #     upload_config=upload_config
        # )
        
        # è¿™é‡Œç”¨æ–‡æœ¬æ¨¡æ‹Ÿæ–‡ä»¶ä¸Šä¼ 
        document2 = client.upload_document_text(
            dataset_id=dataset.dify_dataset_id,
            name="æ·±åº¦å­¦ä¹ åŸºç¡€æ¦‚å¿µ.md",
            text=sample_file_content.strip(),
            upload_config=upload_config
        )
        print(f"âœ… æ¨¡æ‹Ÿæ–‡ä»¶ä¸Šä¼ æˆåŠŸ:")
        print(f"   - æ–‡æ¡£å: {document2.name}")
        print(f"   - æ–‡æ¡£ID: {document2.dify_document_id}")
        
        # 3. ç›‘æ§æ–‡æ¡£å¤„ç†çŠ¶æ€
        print("\nâ³ ç›‘æ§æ–‡æ¡£å¤„ç†çŠ¶æ€...")
        documents_to_monitor = [document1, document2]
        
        for i, doc in enumerate(documents_to_monitor, 1):
            print(f"\næ£€æŸ¥æ–‡æ¡£ {i}: {doc.name}")
            max_attempts = 10
            attempt = 0
            
            while attempt < max_attempts:
                try:
                    # è·å–æœ€æ–°æ–‡æ¡£çŠ¶æ€
                    updated_doc = client.get_document(dataset.dify_dataset_id, doc.dify_document_id)
                    
                    print(f"   çŠ¶æ€: {updated_doc.status}")
                    print(f"   ç´¢å¼•çŠ¶æ€: {updated_doc.indexing_status}")
                    
                    if updated_doc.indexing_status == IndexingStatus.COMPLETED.value:
                        print(f"   âœ… æ–‡æ¡£å¤„ç†å®Œæˆ")
                        print(f"   - åˆ†ç‰‡æ•°é‡: {updated_doc.segment_count}")
                        print(f"   - Tokenæ•°: {updated_doc.tokens}")
                        break
                    elif updated_doc.indexing_status == IndexingStatus.ERROR.value:
                        print(f"   âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {updated_doc.error}")
                        break
                    else:
                        print(f"   â³ å¤„ç†ä¸­... (å°è¯• {attempt + 1}/{max_attempts})")
                        import time; time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                        
                except Exception as e:
                    print(f"   âš ï¸ çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
                    break
                    
                attempt += 1
            
            if attempt >= max_attempts:
                print(f"   âš ï¸ è¶…æ—¶ï¼šæ–‡æ¡£å¯èƒ½ä»åœ¨å¤„ç†ä¸­")
        
        # 4. è·å–æ–‡æ¡£åˆ†ç‰‡ä¿¡æ¯
        print("\nğŸ“„ è·å–æ–‡æ¡£åˆ†ç‰‡ä¿¡æ¯...")
        for i, doc in enumerate(documents_to_monitor, 1):
            try:
                segments = client.get_document_segments(
                    dataset_id=dataset.dify_dataset_id,
                    document_id=doc.dify_document_id
                )
                print(f"\næ–‡æ¡£ {i} ({doc.name}) çš„åˆ†ç‰‡:")
                print(f"   - æ€»åˆ†ç‰‡æ•°: {len(segments)}")
                
                for j, segment in enumerate(segments[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªåˆ†ç‰‡
                    print(f"   åˆ†ç‰‡ {j}:")
                    print(f"     - å†…å®¹é•¿åº¦: {segment.word_count} è¯")
                    print(f"     - Tokenæ•°: {segment.tokens}")
                    print(f"     - å…³é”®è¯: {', '.join(segment.keywords) if segment.keywords else 'æ— '}")
                    if len(segment.content) > 100:
                        print(f"     - å†…å®¹é¢„è§ˆ: {segment.content[:100]}...")
                    else:
                        print(f"     - å†…å®¹: {segment.content}")
                
                if len(segments) > 3:
                    print(f"   ... è¿˜æœ‰ {len(segments) - 3} ä¸ªåˆ†ç‰‡")
                    
            except Exception as e:
                print(f"   âš ï¸ è·å–åˆ†ç‰‡å¤±è´¥: {e}")
        
        return dataset, client, documents_to_monitor
        
    except DocumentUploadError as e:
        print(f"âŒ æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {e}")
        return None
    except DifyKnowledgeBaseError as e:
        print(f"âŒ æ“ä½œå¤±è´¥: {e}")
        return None


def batch_operations_example():
    """æ‰¹é‡æ“ä½œç¤ºä¾‹"""
    print("\n=== æ‰¹é‡æ“ä½œç¤ºä¾‹ ===")
    
    # è·å–ä¸Šä¼ ç¤ºä¾‹çš„ç»“æœ
    try:
        result = upload_operations_example()
        if not result:
            print("âŒ æ— æ³•ç»§ç»­æ‰¹é‡æ“ä½œç¤ºä¾‹")
            return
    except Exception as e:
        print(f"âŒ ä¸Šä¼ æ“ä½œå¤±è´¥: {e}")
        return
    
    dataset, client, existing_docs = result
    
    try:
        print("\nğŸ“š æ‰¹é‡æ–‡æ¡£ä¸Šä¼ ç¤ºä¾‹...")
        
        # 1. å‡†å¤‡æ‰¹é‡æ–‡æœ¬æ–‡æ¡£
        batch_texts = [
            ("æœºå™¨å­¦ä¹ æ¦‚è¿°", """
            æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚
            
            ä¸»è¦ç±»å‹ï¼š
            1. ç›‘ç£å­¦ä¹ ï¼šä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹
            2. æ— ç›‘ç£å­¦ä¹ ï¼šå‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼
            3. å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥
            
            åº”ç”¨é¢†åŸŸï¼šå›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚
            """),
            
            ("ç¥ç»ç½‘ç»œåŸºç¡€", """
            ç¥ç»ç½‘ç»œæ˜¯æ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒå·¥ä½œæ–¹å¼çš„è®¡ç®—æ¨¡å‹ã€‚
            
            åŸºæœ¬ç»„æˆï¼š
            - è¾“å…¥å±‚ï¼šæ¥æ”¶å¤–éƒ¨æ•°æ®
            - éšè—å±‚ï¼šè¿›è¡Œç‰¹å¾æå–å’Œå˜æ¢
            - è¾“å‡ºå±‚ï¼šäº§ç”Ÿæœ€ç»ˆç»“æœ
            
            å…³é”®æ¦‚å¿µï¼š
            - æƒé‡å’Œåç½®ï¼šå¯å­¦ä¹ çš„å‚æ•°
            - æ¿€æ´»å‡½æ•°ï¼šå¼•å…¥éçº¿æ€§
            - æŸå¤±å‡½æ•°ï¼šè¡¡é‡é¢„æµ‹ä¸çœŸå®å€¼çš„å·®å¼‚
            """),
            
            ("è®¡ç®—æœºè§†è§‰", """
            è®¡ç®—æœºè§†è§‰æ˜¯è®©è®¡ç®—æœº"çœ‹æ‡‚"å›¾åƒå’Œè§†é¢‘çš„æŠ€æœ¯ã€‚
            
            æ ¸å¿ƒä»»åŠ¡ï¼š
            1. å›¾åƒåˆ†ç±»ï¼šè¯†åˆ«å›¾åƒä¸­çš„ä¸»è¦å¯¹è±¡
            2. ç›®æ ‡æ£€æµ‹ï¼šå®šä½å¹¶è¯†åˆ«å›¾åƒä¸­çš„å¤šä¸ªå¯¹è±¡
            3. è¯­ä¹‰åˆ†å‰²ï¼šå¯¹å›¾åƒä¸­æ¯ä¸ªåƒç´ è¿›è¡Œåˆ†ç±»
            4. å®ä¾‹åˆ†å‰²ï¼šåŒºåˆ†åŒç±»å¯¹è±¡çš„ä¸åŒå®ä¾‹
            
            ä¸»è¦ç®—æ³•ï¼šCNNã€R-CNNã€YOLOã€U-Netç­‰ã€‚
            """)
        ]
        
        print(f"ğŸ“ å‡†å¤‡æ‰¹é‡ä¸Šä¼  {len(batch_texts)} ä¸ªæ–‡æ¡£...")
        
        # 2. æ‰§è¡Œæ‰¹é‡ä¸Šä¼ 
        batch_results = client.batch_upload_texts(
            dataset_id=dataset.dify_dataset_id,
            documents=batch_texts,
            upload_config=UploadConfig(
                indexing_technique=IndexingTechnique.HIGH_QUALITY,
                process_rule=ProcessRule(
                    mode=ProcessMode.AUTOMATIC,
                    pre_processing_rules=[
                        {"id": "remove_extra_spaces", "enabled": True}
                    ],
                    segmentation={
                        "separator": "\\n",
                        "max_tokens": 300
                    }
                )
            )
        )
        
        print(f"âœ… æ‰¹é‡ä¸Šä¼ å®Œæˆ:")
        print(f"   - æˆåŠŸä¸Šä¼ : {len(batch_results)} ä¸ªæ–‡æ¡£")
        for doc in batch_results:
            print(f"     * {doc.name} (ID: {doc.dify_document_id})")
        
        # 3. æ‰¹é‡çŠ¶æ€ç›‘æ§
        print(f"\nâ³ æ‰¹é‡ç›‘æ§æ–‡æ¡£å¤„ç†çŠ¶æ€...")
        all_completed = False
        max_wait_time = 60  # æœ€å¤§ç­‰å¾…60ç§’
        wait_time = 0
        
        while not all_completed and wait_time < max_wait_time:
            completed_count = 0
            
            for doc in batch_results:
                try:
                    updated_doc = client.get_document(dataset.dify_dataset_id, doc.dify_document_id)
                    if updated_doc.indexing_status == IndexingStatus.COMPLETED.value:
                        completed_count += 1
                except Exception:
                    pass
            
            print(f"   è¿›åº¦: {completed_count}/{len(batch_results)} æ–‡æ¡£å®Œæˆå¤„ç†")
            
            if completed_count == len(batch_results):
                all_completed = True
                print("   âœ… æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆ")
            else:
                time.sleep(3)
                wait_time += 3
        
        if not all_completed:
            print("   âš ï¸ éƒ¨åˆ†æ–‡æ¡£å¯èƒ½ä»åœ¨å¤„ç†ä¸­")
        
        # 4. è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯:")
        updated_dataset = client.get_dataset(dataset.dify_dataset_id, use_cache=False)
        print(f"   - çŸ¥è¯†åº“åç§°: {updated_dataset.name}")
        print(f"   - æ–‡æ¡£æ€»æ•°: {updated_dataset.document_count}")
        print(f"   - å­—ç¬¦æ€»æ•°: {updated_dataset.character_count}")
        
        # 5. åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£
        print(f"\nğŸ“‹ çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£:")
        all_documents = client.list_documents(dataset.dify_dataset_id, limit=50)
        for i, doc in enumerate(all_documents, 1):
            print(f"   {i}. {doc.name}")
            print(f"      - ID: {doc.dify_document_id}")
            print(f"      - çŠ¶æ€: {doc.status} / {doc.indexing_status}")
            print(f"      - åˆ†ç‰‡æ•°: {doc.segment_count}")
        
        return dataset, client, all_documents
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡æ“ä½œå¤±è´¥: {e}")
        return None


async def basic_usage_example():
    """åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    print("\n=== åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹ ===")
    
    # è¿™ä¸ªç¤ºä¾‹ç°åœ¨ä¸»è¦ç”¨äºæ¼”ç¤ºç®€å•çš„æŸ¥è¯¢æ“ä½œ
    try:
        result = batch_operations_example()
        if not result:
            print("âŒ æ— æ³•è¿è¡ŒåŸºæœ¬ä½¿ç”¨ç¤ºä¾‹")
            return
    except Exception as e:
        print(f"âŒ æ‰¹é‡æ“ä½œå¤±è´¥: {e}")
        return
    
    dataset, client, documents = result
    
    try:
        print("\nğŸ” çŸ¥è¯†åº“æŸ¥è¯¢ç¤ºä¾‹...")
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "ä»€ä¹ˆæ˜¯Transformerï¼Ÿ",
            "æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
            "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ",
            "ç¥ç»ç½‘ç»œçš„ç»„æˆéƒ¨åˆ†"
        ]
        
        for query in test_queries:
            print(f"\næŸ¥è¯¢: '{query}'")
            try:
                query_result = client.query_dataset(
                    dataset_id=dataset.dify_dataset_id,
                    query=query,
                    retrieval_model={
                        "search_method": "semantic_search",
                        "top_k": 3,
                        "score_threshold_enabled": True,
                        "score_threshold": 0.3
                    }
                )
                
                results = query_result.get('data', [])
                print(f"   æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ:")
                
                for i, result in enumerate(results, 1):
                    score = result.get('score', 0)
                    content = result.get('content', '')
                    print(f"   {i}. ç›¸å…³åº¦: {score:.3f}")
                    if len(content) > 150:
                        print(f"      å†…å®¹: {content[:150]}...")
                    else:
                        print(f"      å†…å®¹: {content}")
                        
            except Exception as e:
                print(f"   âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        
        print(f"\nâœ… åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹å®Œæˆ")
        
    except DifyKnowledgeBaseError as e:
        print(f"âŒ æ“ä½œå¤±è´¥: {e}")


def file_upload_example():
    """æ–‡ä»¶ä¸Šä¼ ç¤ºä¾‹"""
    print("\n=== æ–‡ä»¶ä¸Šä¼ ç¤ºä¾‹ ===")
    
    config = get_config()
    client = DifyKnowledgeBaseClient(config)
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        dataset = client.create_dataset(
            name="æ–‡æ¡£çŸ¥è¯†åº“",
            description="åŒ…å«å„ç§æ–‡æ¡£æ ¼å¼çš„çŸ¥è¯†åº“"
        )
        print(f"åˆ›å»ºæ•°æ®é›†: {dataset.name}")
        
        # å‡†å¤‡ä¸Šä¼ é…ç½®
        upload_config = UploadConfig(
            indexing_technique=IndexingTechnique.HIGH_QUALITY,
            process_rule=ProcessRule(
                mode=ProcessMode.CUSTOM,
                pre_processing_rules=[
                    {"id": "remove_extra_spaces", "enabled": True},
                    {"id": "remove_urls_emails", "enabled": False}
                ],
                segmentation={
                    "separator": "\\n\\n",
                    "max_tokens": 800,
                    "chunk_overlap": 50
                }
            )
        )
        
        # ç¤ºä¾‹ï¼šä¸Šä¼ PDFæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        pdf_path = "/path/to/sample.pdf"
        if os.path.exists(pdf_path):
            document = client.upload_document_file(
                dataset_id=dataset.dify_dataset_id,
                file_path=pdf_path,
                upload_config=upload_config
            )
            print(f"ä¸Šä¼ PDFæˆåŠŸ: {document.name}")
        
        # ç¤ºä¾‹ï¼šæ‰¹é‡ä¸Šä¼ æ–‡æœ¬
        texts_data = [
            ("æ–‡æ¡£1", "ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„å†…å®¹..."),
            ("æ–‡æ¡£2", "ç¬¬äºŒä¸ªæ–‡æ¡£çš„å†…å®¹..."),
            ("æ–‡æ¡£3", "ç¬¬ä¸‰ä¸ªæ–‡æ¡£çš„å†…å®¹...")
        ]
        
        batch_results = client.batch_upload_texts(
            dataset_id=dataset.dify_dataset_id,
            documents=texts_data,
            upload_config=upload_config
        )
        print(f"æ‰¹é‡ä¸Šä¼ å®Œæˆ: {len(batch_results)} ä¸ªæ–‡æ¡£")
        
    except Exception as e:
        print(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")


def data_model_example():
    """æ•°æ®æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹"""
    print("\n=== æ•°æ®æ¨¡å‹ç¤ºä¾‹ ===")
    
    # 1. åˆ›å»ºæ•°æ®é›†æ¨¡å‹
    dataset_model = DifyDatasetModel()
    dataset_model.dify_dataset_id = "dataset_123"
    dataset_model.name = "AIç ”ç©¶çŸ¥è¯†åº“"
    dataset_model.description = "åŒ…å«AIç›¸å…³ç ”ç©¶æ–‡æ¡£"
    dataset_model.status = DatasetStatus.ACTIVE
    dataset_model.document_count = 15
    dataset_model.word_count = 50000
    
    print("æ•°æ®é›†æ¨¡å‹:")
    print(f"  ID: {dataset_model.dify_dataset_id}")
    print(f"  åç§°: {dataset_model.name}")
    print(f"  çŠ¶æ€: {dataset_model.status}")
    print(f"  æ–‡æ¡£æ•°: {dataset_model.document_count}")
    
    # 2. åˆ›å»ºæ–‡æ¡£æ¨¡å‹
    document_model = DifyDocumentModel()
    document_model.dify_document_id = "doc_456"
    document_model.dify_dataset_id = dataset_model.dify_dataset_id
    document_model.name = "Transformerè®ºæ–‡"
    document_model.file_type = DocumentType.TXT.value
    document_model.status = DocumentStatus.COMPLETED.value
    document_model.indexing_status = IndexingStatus.COMPLETED.value
    document_model.word_count = 8000
    document_model.segment_count = 12
    
    print("\næ–‡æ¡£æ¨¡å‹:")
    print(f"  ID: {document_model.dify_document_id}")
    print(f"  åç§°: {document_model.name}")
    print(f"  ç±»å‹: {document_model.file_type}")
    print(f"  çŠ¶æ€: {document_model.status}")
    print(f"  æ®µè½æ•°: {document_model.segment_count}")
    
    # 3. åˆ›å»ºæ®µè½æ¨¡å‹
    segment_model = DifySegmentModel()
    segment_model.dify_segment_id = "seg_789"
    segment_model.dify_document_id = document_model.dify_document_id
    segment_model.position = 1
    segment_model.content = "Transformeræ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¨¡å‹..."
    segment_model.word_count = 150
    segment_model.tokens = 200
    segment_model.keywords = ["transformer", "attention", "æ·±åº¦å­¦ä¹ "]
    
    print("\næ®µè½æ¨¡å‹:")
    print(f"  ID: {segment_model.dify_segment_id}")
    print(f"  ä½ç½®: {segment_model.position}")
    print(f"  è¯æ•°: {segment_model.word_count}")
    print(f"  å…³é”®è¯: {', '.join(segment_model.keywords)}")
    
    # 4. è½¬æ¢ä¸ºå­—å…¸
    dataset_dict = dataset_model.to_dict()
    print(f"\næ•°æ®é›†å­—å…¸keys: {list(dataset_dict.keys())}")


def error_handling_example():
    """é”™è¯¯å¤„ç†ç¤ºä¾‹"""
    print("\n=== é”™è¯¯å¤„ç†ç¤ºä¾‹ ===")
    
    config = DifyKnowledgeBaseConfig(
        api_key="invalid-key",  # æ•…æ„ä½¿ç”¨æ— æ•ˆå¯†é’¥
        base_url="https://api.dify.ai"
    )
    client = DifyKnowledgeBaseClient(config)
    
    try:
        # å°è¯•åˆ›å»ºæ•°æ®é›†ï¼ˆä¼šå¤±è´¥ï¼‰
        client.create_dataset(name="æµ‹è¯•æ•°æ®é›†")
    except DifyKnowledgeBaseError as e:
        print(f"æ•è·åˆ°çŸ¥è¯†åº“é”™è¯¯: {e}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
    
    try:
        # å°è¯•è·å–ä¸å­˜åœ¨çš„æ•°æ®é›†
        client.get_dataset("nonexistent_id")
    except DatasetNotFoundError as e:
        print(f"æ•°æ®é›†æœªæ‰¾åˆ°: {e}")
    except DifyKnowledgeBaseError as e:
        print(f"å…¶ä»–çŸ¥è¯†åº“é”™è¯¯: {e}")


def advanced_features_example():
    """é«˜çº§åŠŸèƒ½ç¤ºä¾‹"""
    print("\n=== é«˜çº§åŠŸèƒ½ç¤ºä¾‹ ===")
    
    config = get_config()
    client = DifyKnowledgeBaseClient(config)
    
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆç¼“å­˜åŠŸèƒ½å·²å†…ç½®ï¼‰
        cached_client = DifyKnowledgeBaseClient(config)
        
        # æ³¨æ„ï¼šé‡è¯•ç­–ç•¥åœ¨å®¢æˆ·ç«¯åˆå§‹åŒ–æ—¶å·²é…ç½®ï¼Œè¿™é‡Œä»…ä½œæ¼”ç¤º
        print("é‡è¯•ç­–ç•¥å·²åœ¨å®¢æˆ·ç«¯åˆå§‹åŒ–æ—¶é…ç½®")
        
        # è·å–æ‰€æœ‰æ•°æ®é›†
        datasets = cached_client.list_datasets()
        print(f"æ‰¾åˆ° {len(datasets)} ä¸ªæ•°æ®é›†")
        
        if datasets:
            dataset_id = datasets[0].dify_dataset_id
            
            # è·å–æ•°æ®é›†è¯¦æƒ…
            dataset_detail = cached_client.get_dataset(dataset_id)
            print(f"æ•°æ®é›†è¯¦æƒ…: {dataset_detail.name}")
            
            print("ç¼“å­˜åŠŸèƒ½å·²å†…ç½®åœ¨å®¢æˆ·ç«¯ä¸­")
            
            print("æ³¨æ„ï¼šæ­¤æ¼”ç¤ºç‰ˆæœ¬ä¸åŒ…å«æ•°æ®é›†æ›´æ–°åŠŸèƒ½")
        
    except Exception as e:
        print(f"é«˜çº§åŠŸèƒ½ç¤ºä¾‹å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("Dify Knowledge Base ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("DIFY_KB_API_KEY") and not os.getenv("DIFY_API_KEY"):
        print("âš ï¸  è­¦å‘Š: æœªè®¾ç½® Dify API å¯†é’¥ç¯å¢ƒå˜é‡")
        print("è¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ä¹‹ä¸€:")
        print("  export DIFY_KB_API_KEY=your-api-key")
        print("  export DIFY_API_KEY=your-api-key")
        print("  export DIFY_BASE_URL=https://api.dify.ai/v1  # å¯é€‰ï¼Œé»˜è®¤ä¸ºæœ¬åœ°")
        print()
        print("ğŸ”§ å¦‚æœæ‚¨æƒ³è¿è¡Œå®Œæ•´ç¤ºä¾‹ï¼Œè¯·å…ˆé…ç½®APIå¯†é’¥")
        print("ğŸ”§ å¦‚æœåªæƒ³æŸ¥çœ‹æ•°æ®æ¨¡å‹å’Œé”™è¯¯å¤„ç†ç¤ºä¾‹ï¼Œå¯ä»¥ç»§ç»­è¿è¡Œ")
        print()
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¿è¡Œå®Œæ•´ç¤ºä¾‹
    run_full_examples = False
    if os.getenv("DIFY_KB_API_KEY") or os.getenv("DIFY_API_KEY"):
        run_full_examples = True
        print("âœ… æ£€æµ‹åˆ°APIå¯†é’¥ï¼Œå°†è¿è¡Œå®Œæ•´ç¤ºä¾‹")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°APIå¯†é’¥ï¼Œå°†åªè¿è¡Œæ•°æ®æ¨¡å‹å’Œé”™è¯¯å¤„ç†ç¤ºä¾‹")
    
    try:
        if run_full_examples:
            print("\n" + "=" * 60)
            print("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„ Dify çŸ¥è¯†åº“æ“ä½œç¤ºä¾‹")
            print("=" * 60)
            
            # ä¸»è¦ç¤ºä¾‹ï¼šåˆ›å»ºçŸ¥è¯†åº“ -> ä¸Šä¼ æ–‡æ¡£ -> æ‰¹é‡æ“ä½œ -> æŸ¥è¯¢æµ‹è¯•
            print("\nğŸ“š è¿™ä¸ªç¤ºä¾‹å°†æ¼”ç¤ºå®Œæ•´çš„çŸ¥è¯†åº“ä½¿ç”¨æµç¨‹:")
            print("   1. åˆ›å»ºçŸ¥è¯†åº“å’Œé…ç½®")
            print("   2. ä¸Šä¼ ä¸ªåˆ«æ–‡æ¡£ï¼ˆæ–‡æœ¬å’Œæ–‡ä»¶ï¼‰")
            print("   3. æ‰¹é‡ä¸Šä¼ å¤šä¸ªæ–‡æ¡£")
            print("   4. æŸ¥è¯¢çŸ¥è¯†åº“å†…å®¹")
            print("   5. ç›‘æ§å¤„ç†çŠ¶æ€")
            
            # è¿è¡Œä¸»è¦çš„çŸ¥è¯†åº“æ“ä½œç¤ºä¾‹
            # Note: basic_usage_example is currently async but calls sync functions
            print("æ³¨æ„ï¼šç”±äºAPIæ–¹æ³•æ˜¯åŒæ­¥çš„ï¼Œç¤ºä¾‹å·²ä¿®æ”¹ä¸ºç›´æ¥è°ƒç”¨")
            
            print("\n" + "=" * 60)
            print("ğŸ“ æ–‡ä»¶ä¸Šä¼ ä¸“é¡¹ç¤ºä¾‹")
            print("=" * 60)
            # è¿è¡Œæ–‡ä»¶ä¸Šä¼ ç¤ºä¾‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            file_upload_example()
            
        # æ€»æ˜¯è¿è¡Œæ•°æ®æ¨¡å‹å’Œé”™è¯¯å¤„ç†ç¤ºä¾‹ï¼ˆä¸éœ€è¦APIè¿æ¥ï¼‰
        print("\n" + "=" * 60)
        print("ğŸ“Š æ•°æ®æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹")
        print("=" * 60)
        data_model_example()
        
        print("\n" + "=" * 60)
        print("ğŸ› ï¸  é”™è¯¯å¤„ç†ç¤ºä¾‹")
        print("=" * 60)
        error_handling_example()
        
        if run_full_examples:
            print("\n" + "=" * 60)
            print("ğŸ”§ é«˜çº§åŠŸèƒ½ç¤ºä¾‹")
            print("=" * 60)
            advanced_features_example()
    
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­äº†ç¤ºä¾‹è¿è¡Œ")
    except Exception as e:
        print(f"\nâŒ ç¤ºä¾‹è¿è¡Œå‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("âœ… ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("=" * 60)
    
    if run_full_examples:
        print("\nğŸ“‹ è¿è¡Œæ€»ç»“:")
        print("   âœ… çŸ¥è¯†åº“åˆ›å»ºå’Œè®¾ç½®")
        print("   âœ… æ–‡æ¡£ä¸Šä¼ æ“ä½œï¼ˆæ–‡æœ¬å’Œæ–‡ä»¶ï¼‰")
        print("   âœ… æ‰¹é‡æ–‡æ¡£å¤„ç†")
        print("   âœ… çŸ¥è¯†åº“æŸ¥è¯¢æµ‹è¯•")
        print("   âœ… é«˜çº§åŠŸèƒ½æ¼”ç¤º")
    else:
        print("\nğŸ“‹ è¿è¡Œæ€»ç»“:")
        print("   âœ… æ•°æ®æ¨¡å‹ä½¿ç”¨æ¼”ç¤º")
        print("   âœ… é”™è¯¯å¤„ç†æœºåˆ¶æ¼”ç¤º")
        print("   âš ï¸  å®Œæ•´APIç¤ºä¾‹éœ€è¦è®¾ç½®APIå¯†é’¥")
    
    print("\nğŸ’¡ æç¤º:")
    print("   - è®¾ç½® DIFY_KB_API_KEY ç¯å¢ƒå˜é‡æ¥è¿è¡Œå®Œæ•´ç¤ºä¾‹")
    print("   - æŸ¥çœ‹ä»£ç æ³¨é‡Šäº†è§£æ›´å¤šé…ç½®é€‰é¡¹")
    print("   - å‚è€ƒ HomeSystem æ–‡æ¡£è·å–è¯¦ç»†ä½¿ç”¨æŒ‡å—")


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    main()