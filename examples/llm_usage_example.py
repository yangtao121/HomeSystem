#!/usr/bin/env python3
"""
LLM Factory ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•åœ¨HomeSystemä¸­ä½¿ç”¨ä¸åŒå‚å•†çš„LLMæ¨¡å‹
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from HomeSystem.graph.llm_factory import get_llm, get_embedding, list_available_llm_models
from langchain_core.messages import HumanMessage


def demo_basic_usage():
    """åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    print("=" * 60)
    print("ğŸš€ LLM Factory åŸºç¡€ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    # 1. æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹
    print("\nğŸ“‹ å¯ç”¨çš„LLMæ¨¡å‹:")
    models = list_available_llm_models()
    for i, model in enumerate(models, 1):
        print(f"  {i:2d}. {model}")
    
    # 2. ä½¿ç”¨é»˜è®¤æ¨¡å‹
    print(f"\nğŸ”§ ä½¿ç”¨é»˜è®¤æ¨¡å‹:")
    default_llm = get_llm()
    print(f"  é»˜è®¤LLMç±»å‹: {type(default_llm).__name__}")
    
    # 3. ä½¿ç”¨æŒ‡å®šæ¨¡å‹
    print(f"\nğŸ¯ ä½¿ç”¨æŒ‡å®šæ¨¡å‹:")
    if "deepseek.DeepSeek_V3" in models:
        deepseek_llm = get_llm("deepseek.DeepSeek_V3")
        print(f"  DeepSeek V3: {type(deepseek_llm).__name__}")
    
    if "siliconflow.Qwen2_5_72B" in models:
        qwen_llm = get_llm("siliconflow.Qwen2_5_72B")
        print(f"  é€šä¹‰åƒé—® 2.5-72B: {type(qwen_llm).__name__}")
    
    # 4. ä½¿ç”¨embeddingæ¨¡å‹
    print(f"\nğŸ” ä½¿ç”¨Embeddingæ¨¡å‹:")
    embedding = get_embedding("ollama.BGE_M3")
    print(f"  BGE-M3 Embedding: {type(embedding).__name__}")


def demo_multi_model_conversation():
    """å¤šæ¨¡å‹å¯¹è¯ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ğŸ’¬ å¤šæ¨¡å‹å¯¹è¯ç¤ºä¾‹")
    print("=" * 60)
    
    # å‡†å¤‡æµ‹è¯•æ¶ˆæ¯
    test_message = "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±"
    message = HumanMessage(content=test_message)
    
    # æµ‹è¯•ä¸åŒæ¨¡å‹
    models_to_test = [
        "deepseek.DeepSeek_V3",
        "siliconflow.DeepSeek_V3", 
        "moonshot.Kimi_K2",
        "zhipuai.GLM_4_5",
        "zhipuai.GLM_4_5_Air"
    ]
    
    available_models = list_available_llm_models()
    
    for model_name in models_to_test:
        if model_name in available_models:
            try:
                print(f"\nğŸ¤– {model_name}:")
                llm = get_llm(model_name, temperature=0.7, max_tokens=100)
                print(f"  æ¨¡å‹å·²åˆ›å»º: {type(llm).__name__}")
                print(f"  æµ‹è¯•æ¶ˆæ¯: {test_message}")
                print("  æ³¨æ„: éœ€è¦æœ‰æ•ˆçš„API Keyæ‰èƒ½å®é™…å‘é€è¯·æ±‚")
                
            except Exception as e:
                print(f"  âŒ åˆ›å»ºå¤±è´¥: {e}")
        else:
            print(f"\nâš ï¸  {model_name}: æ¨¡å‹ä¸å¯ç”¨ï¼ˆå¯èƒ½ç¼ºå°‘API Keyï¼‰")


def demo_graph_integration():
    """Graphé›†æˆç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ğŸ”— Graphé›†æˆç¤ºä¾‹")
    print("=" * 60)
    
    print("""
åœ¨Graphä¸­ä½¿ç”¨LLMçš„ç¤ºä¾‹ä»£ç :

```python
from HomeSystem.graph.llm_factory import get_llm, get_embedding

class MyGraph(BaseGraph):
    def __init__(self):
        super().__init__()
        
        # å¯ä»¥åœ¨éœ€è¦æ—¶åŠ¨æ€è·å–ä¸åŒçš„LLM
        self.main_llm = get_llm("deepseek.DeepSeek_V3")       # ä¸»è¦æ¨ç†
        self.code_llm = get_llm("siliconflow.Qwen2_5_72B")   # ä»£ç ç”Ÿæˆ
        self.reasoning_llm = get_llm("deepseek.DeepSeek_R1") # æ·±åº¦æ¨ç†
        self.agent_llm = get_llm("zhipuai.GLM_4_5")          # æ™ºèƒ½ä½“ä»»åŠ¡
        self.efficient_llm = get_llm("zhipuai.GLM_4_5_Air")  # é«˜æ•ˆå¤„ç†
        
        # è·å–embeddingæ¨¡å‹
        self.embedding = get_embedding("ollama.BGE_M3")
    
    def process_query(self, query: str):
        # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©ä¸åŒçš„æ¨¡å‹
        if "ä»£ç " in query or "code" in query.lower():
            return self.code_llm.invoke([HumanMessage(content=query)])
        elif "æ¨ç†" in query or "reasoning" in query.lower():
            return self.reasoning_llm.invoke([HumanMessage(content=query)])
        elif "æ™ºèƒ½ä½“" in query or "agent" in query.lower():
            return self.agent_llm.invoke([HumanMessage(content=query)])
        elif "å¿«é€Ÿ" in query or "efficient" in query.lower():
            return self.efficient_llm.invoke([HumanMessage(content=query)])
        else:
            return self.main_llm.invoke([HumanMessage(content=query)])
```

âœ¨ ä¼˜åŠ¿:
  - åŒä¸€ä¸ªgraphå¯ä»¥ä½¿ç”¨å¤šä¸ªä¸åŒçš„LLM
  - æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹
  - æ”¯æŒæ‰€æœ‰å‚å•†: DeepSeek, ç¡…åŸºæµåŠ¨, ç«å±±å¼•æ“, æœˆä¹‹æš—é¢, æ™ºè°±AI, Ollama
  - ç»Ÿä¸€çš„provider.modelå‘½åæ ¼å¼ï¼Œæ˜“äºåŒºåˆ†å’Œç®¡ç†
  - æ™ºè°±AI GLM-4.5: ä¸“ä¸ºæ™ºèƒ½ä½“ä»»åŠ¡ä¼˜åŒ–ï¼Œå…¨çƒæ’åç¬¬3
  - GLM-4.5-Air: é«˜æ•ˆè½»é‡ç‰ˆæœ¬ï¼Œæˆæœ¬ä¼˜åŠ¿æ˜æ˜¾
""")


def main():
    """ä¸»å‡½æ•°"""
    try:
        demo_basic_usage()
        demo_multi_model_conversation()
        demo_graph_integration()
        
        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("ğŸ’¡ æç¤º: ç¡®ä¿.envæ–‡ä»¶ä¸­é…ç½®äº†ç›¸åº”çš„API Key")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()