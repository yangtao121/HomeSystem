#!/usr/bin/env python3
"""
æ–‡æœ¬åˆ†å—ç´¢å¼•å·¥å…·ä½¿ç”¨ç¤ºä¾‹

ç®€åŒ–ç‰ˆæ¼”ç¤ºï¼Œå±•ç¤º TextChunkIndexerTool çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
- æ™ºèƒ½æ–‡æœ¬åˆ†å—
- è¯­ä¹‰æœç´¢ï¼ˆå¦‚æœæœ‰embeddingæ¨¡å‹ï¼‰
"""
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from HomeSystem.graph.tool.text_chunk_indexer import TextChunkIndexerTool
import json


def main():
    print("æ–‡æœ¬åˆ†å—ç´¢å¼•å·¥å…·ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # åˆ›å»ºå·¥å…·å®ä¾‹ï¼ˆè‡ªåŠ¨åŠ è½½æœ¬åœ°embeddingæ¨¡å‹ï¼‰
    print("ğŸš€ åˆå§‹åŒ–å·¥å…·...")
    indexer = TextChunkIndexerTool()
    
    # æ£€æŸ¥embeddingæ¨¡å‹çŠ¶æ€
    embedding_model = getattr(indexer, 'embeddings_model', None)
    if embedding_model:
        print("âœ… è‡ªåŠ¨åŠ è½½äº†embeddingæ¨¡å‹ï¼Œæ”¯æŒè¯­ä¹‰æœç´¢")
    else:
        print("âš ï¸ æœªåŠ è½½embeddingæ¨¡å‹ï¼Œä»…æ”¯æŒæ–‡æœ¬åˆ†å—")
    
    # ç¤ºä¾‹æ–‡æ¡£ï¼šæŠ€æœ¯è¯´æ˜
    document = """
# APIæ¥å£æ–‡æ¡£

## ç”¨æˆ·è®¤è¯
ç”¨æˆ·è®¤è¯é‡‡ç”¨JWTä»¤ç‰Œæœºåˆ¶ã€‚å®¢æˆ·ç«¯éœ€è¦åœ¨è¯·æ±‚å¤´ä¸­åŒ…å«Authorizationå­—æ®µï¼Œ
æ ¼å¼ä¸º "Bearer <token>"ã€‚ä»¤ç‰Œæœ‰æ•ˆæœŸä¸º24å°æ—¶ã€‚

## æ•°æ®æŸ¥è¯¢API
GET /api/data - è·å–æ•°æ®åˆ—è¡¨
æ”¯æŒåˆ†é¡µå‚æ•°ï¼špageï¼ˆé¡µç ï¼‰å’Œlimitï¼ˆæ¯é¡µå¤§å°ï¼‰
è¿”å›æ ¼å¼ä¸ºJSONï¼ŒåŒ…å«dataæ•°ç»„å’Œpaginationä¿¡æ¯ã€‚

## æ•°æ®åˆ›å»ºAPI  
POST /api/data - åˆ›å»ºæ–°æ•°æ®
è¯·æ±‚ä½“éœ€è¦åŒ…å«nameã€typeã€valueç­‰å¿…å¡«å­—æ®µã€‚
åˆ›å»ºæˆåŠŸè¿”å›201çŠ¶æ€ç å’Œæ–°åˆ›å»ºçš„æ•°æ®å¯¹è±¡ã€‚

## é”™è¯¯å¤„ç†
APIä½¿ç”¨æ ‡å‡†HTTPçŠ¶æ€ç è¡¨ç¤ºç»“æœï¼š
- 200: æˆåŠŸ
- 400: è¯·æ±‚å‚æ•°é”™è¯¯
- 401: è®¤è¯å¤±è´¥
- 403: æƒé™ä¸è¶³
- 500: æœåŠ¡å™¨é”™è¯¯

æ‰€æœ‰é”™è¯¯å“åº”éƒ½åŒ…å«errorå­—æ®µï¼Œæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ã€‚

## é™æµç­–ç•¥
APIé‡‡ç”¨ä»¤ç‰Œæ¡¶ç®—æ³•è¿›è¡Œé™æµï¼Œæ¯ä¸ªç”¨æˆ·æ¯åˆ†é’Ÿæœ€å¤š100æ¬¡è¯·æ±‚ã€‚
è¶…å‡ºé™åˆ¶å°†è¿”å›429çŠ¶æ€ç ï¼Œéœ€è¦ç­‰å¾…ä¸€åˆ†é’Ÿåé‡è¯•ã€‚
"""
    
    print("ğŸ“„ å¤„ç†æ–‡æ¡£å¹¶ç”Ÿæˆåˆ†å—...")
    
    # 1. åŸºæœ¬åˆ†å—
    result = indexer._run(text_content=document)
    data = json.loads(result)
    
    print(f"âœ… ç”Ÿæˆäº† {data.get('total_chunks', 0)} ä¸ªåˆ†å—")
    print(f"åˆ†å—ç­–ç•¥: {data.get('chunk_strategy', 'unknown')}")
    
    # æ˜¾ç¤ºåˆ†å—ä¿¡æ¯
    chunks = data.get('chunks', [])
    for i, chunk in enumerate(chunks):
        content_preview = chunk.get('content', '')[:80].replace('\n', '\\n')
        print(f"  åˆ†å— {i+1}: {content_preview}...")
    
    # 2. è¯­ä¹‰æœç´¢æ¼”ç¤º
    print(f"\nğŸ” è¯­ä¹‰æœç´¢æ¼”ç¤º:")
    
    search_queries = [
        "å¦‚ä½•è¿›è¡Œç”¨æˆ·è®¤è¯ï¼Ÿ",
        "APIé™æµæ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ",
        "åˆ›å»ºæ•°æ®éœ€è¦ä»€ä¹ˆå‚æ•°ï¼Ÿ"
    ]
    
    for query in search_queries:
        print(f"\næŸ¥è¯¢: {query}")
        
        search_result = indexer._run(
            text_content=document,
            query=query
        )
        
        search_data = json.loads(search_result)
        results = search_data.get('search_results', [])
        
        if results:
            best_match = results[0]
            similarity = best_match.get('similarity_score', 0)
            content = best_match.get('content', '')[:120]
            print(f"  ğŸ“„ åŒ¹é…å†…å®¹ (ç›¸ä¼¼åº¦: {similarity:.3f}): {content}...")
        else:
            if embedding_model:
                print(f"  âŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
            else:
                print(f"  âš ï¸ æ— æ³•è¿›è¡Œè¯­ä¹‰æœç´¢ï¼ˆembeddingæ¨¡å‹æœªåŠ è½½ï¼‰")
    
    print(f"\n{'=' * 50}")
    print("âœ… ç¤ºä¾‹å®Œæˆï¼")
    
    # 3. å·¥å…·é›†æˆæç¤º
    print(f"\nğŸ’¡ LangGraphé›†æˆä½¿ç”¨æ–¹æ³•:")
    print(f"```python")
    print(f"from HomeSystem.graph.tool import TextChunkIndexerTool")
    print(f"")
    print(f"# æ–¹å¼1: è‡ªåŠ¨æ¨¡å¼ï¼ˆæ¨èï¼‰- è‡ªåŠ¨åŠ è½½ollama.BGE_M3")
    print(f"tool = TextChunkIndexerTool()")
    print(f"")
    print(f"# æ–¹å¼2: æ‰‹åŠ¨æŒ‡å®šembeddingæ¨¡å‹")
    print(f"from HomeSystem.graph.llm_factory import LLMFactory")
    print(f"factory = LLMFactory()")
    print(f"embedding_model = factory.create_embedding('ollama.BGE_M3')")
    print(f"tool = TextChunkIndexerTool(embeddings_model=embedding_model)")
    print(f"")
    print(f"# æ–¹å¼3: ç¦ç”¨è‡ªåŠ¨embeddingï¼ˆä»…åˆ†å—åŠŸèƒ½ï¼‰")
    print(f"tool = TextChunkIndexerTool(auto_embedding=False)")
    print(f"")
    print(f"# åœ¨LangGraphèŠ‚ç‚¹ä¸­ä½¿ç”¨")
    print(f"result = tool._run(")
    print(f"    text_content='æ‚¨çš„æ–‡æ¡£å†…å®¹',")
    print(f"    query='æœç´¢æŸ¥è¯¢'  # å¯é€‰ï¼Œéœ€è¦embeddingæ”¯æŒ")
    print(f")")
    print(f"```")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()