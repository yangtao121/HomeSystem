"""
ç»Ÿä¸€APIè·¯ç”± - æ•´åˆä¸¤ä¸ªåº”ç”¨çš„APIæ¥å£
æä¾›RESTful APIæ¥å£ç”¨äºå‰ç«¯è°ƒç”¨å’Œç¬¬ä¸‰æ–¹é›†æˆ
"""
from flask import Blueprint, request, jsonify, send_file
from services.task_service import paper_gather_service
from services.paper_gather_service import paper_data_service
from services.paper_explore_service import PaperService
from services.dify_service import DifyService
from HomeSystem.integrations.paper_analysis.analysis_service import PaperAnalysisService
from HomeSystem.integrations.database import ArxivPaperModel
import logging
import os
import sys
import json
import tempfile
import zipfile
import re
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

# å®šä¹‰é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.join(os.path.dirname(__file__), '..', '..', '..')
sys.path.append(PROJECT_ROOT)

api_bp = Blueprint('api', __name__, url_prefix='/api')

# åˆå§‹åŒ–æœåŠ¡
paper_explore_service = PaperService()
dify_service = DifyService()

# åˆå§‹åŒ–Redisè¿æ¥
try:
    import redis
    from config import REDIS_CONFIG
    redis_client = redis.Redis(
        host=REDIS_CONFIG['host'],
        port=REDIS_CONFIG['port'],
        db=REDIS_CONFIG['db'],
        decode_responses=True
    )
    redis_client.ping()
except Exception as e:
    logger.warning(f"APIæ¨¡å—Redisè¿æ¥å¤±è´¥: {e}")
    redis_client = None

# åˆå§‹åŒ–åˆ†ææœåŠ¡
paper_analysis_service = PaperAnalysisService()

# åˆ†ææœåŠ¡é€‚é…å™¨ç±» - æ¡¥æ¥PaperAnalysisServiceå’ŒWeb APIæ¥å£
class AnalysisServiceAdapter:
    """Web APIåˆ†ææœåŠ¡é€‚é…å™¨"""
    
    def __init__(self, paper_service: PaperService, redis_client=None):
        self.paper_service = paper_service
        self.redis_client = redis_client
        self.analysis_threads = {}  # å­˜å‚¨æ­£åœ¨è¿›è¡Œçš„åˆ†æçº¿ç¨‹
        
        # é»˜è®¤é…ç½®
        self.default_config = {
            'analysis_model': 'deepseek.DeepSeek_V3',
            'vision_model': 'ollama.Qwen2_5_VL_7B', 
            'timeout': 600
        }
    
    def load_config(self) -> Dict[str, Any]:
        """ä»RedisåŠ è½½é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨æ–°çš„ç³»ç»Ÿè®¾ç½®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨æ—§é…ç½®å’Œé»˜è®¤é…ç½®"""
        config = self.default_config.copy()
        
        if self.redis_client:
            try:
                # ä¼˜å…ˆå°è¯•åŠ è½½æ–°çš„ç³»ç»Ÿè®¾ç½®
                system_config_key = "system_settings:global"
                saved_system_config = self.redis_client.get(system_config_key)
                if saved_system_config:
                    import json
                    system_data = json.loads(saved_system_config)
                    
                    # ä»ç³»ç»Ÿè®¾ç½®ä¸­æå–æ·±åº¦åˆ†æç›¸å…³é…ç½®
                    analysis_config = {}
                    
                    # æ¨¡å‹é…ç½®æ˜ å°„
                    if system_data.get('deep_analysis_model'):
                        analysis_config['analysis_model'] = system_data['deep_analysis_model']
                    elif system_data.get('llm_model_name'):
                        analysis_config['analysis_model'] = system_data['llm_model_name']
                    
                    if system_data.get('vision_model'):
                        analysis_config['vision_model'] = system_data['vision_model']
                    
                    if system_data.get('analysis_timeout'):
                        analysis_config['timeout'] = system_data['analysis_timeout']
                    
                    # æ·±åº¦åˆ†æç›¸å…³é…ç½®
                    if 'enable_deep_analysis' in system_data:
                        analysis_config['enable_deep_analysis'] = system_data['enable_deep_analysis']
                    if 'deep_analysis_threshold' in system_data:
                        analysis_config['deep_analysis_threshold'] = system_data['deep_analysis_threshold']
                    if 'ocr_char_limit_for_analysis' in system_data:
                        analysis_config['ocr_char_limit_for_analysis'] = system_data['ocr_char_limit_for_analysis']
                    if 'relevance_threshold' in system_data:
                        analysis_config['relevance_threshold'] = system_data['relevance_threshold']
                    
                    config.update(analysis_config)
                    logger.info(f"ä»ç³»ç»Ÿè®¾ç½®åŠ è½½æ·±åº¦åˆ†æé…ç½®: {analysis_config}")
                
                else:
                    # å¦‚æœç³»ç»Ÿè®¾ç½®ä¸å­˜åœ¨ï¼Œå°è¯•åŠ è½½æ—§çš„åˆ†æé…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
                    old_config_key = "analysis_config:global"
                    saved_old_config = self.redis_client.get(old_config_key)
                    if saved_old_config:
                        import json
                        old_data = json.loads(saved_old_config)
                        config.update(old_data)
                        logger.info(f"ä»æ—§é…ç½®åŠ è½½æ·±åº¦åˆ†æé…ç½®: {old_data}")
                
            except Exception as e:
                logger.warning(f"ä»RedisåŠ è½½é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return config
    
    def start_analysis(self, arxiv_id: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """å¯åŠ¨è®ºæ–‡æ·±åº¦åˆ†æ"""
        try:
            import threading
            from pathlib import Path
            
            logger.info(f"ğŸš€ å¼€å§‹å¯åŠ¨æ·±åº¦åˆ†æ - ArXiv ID: {arxiv_id}")
            
            # æ£€æŸ¥è®ºæ–‡æ˜¯å¦å­˜åœ¨
            paper = self.paper_service.get_paper_detail(arxiv_id)
            if not paper:
                logger.error(f"âŒ è®ºæ–‡ä¸å­˜åœ¨: {arxiv_id}")
                return {
                    'success': False,
                    'error': f'è®ºæ–‡ {arxiv_id} ä¸å­˜åœ¨'
                }
            
            # æ£€æŸ¥æ˜¯å¦å·²åœ¨åˆ†æä¸­
            if arxiv_id in self.analysis_threads:
                thread = self.analysis_threads[arxiv_id]
                if thread.is_alive():
                    logger.warning(f"âš ï¸ è®ºæ–‡å·²åœ¨åˆ†æä¸­: {arxiv_id}")
                    return {
                        'success': False,
                        'error': 'è¯¥è®ºæ–‡æ­£åœ¨åˆ†æä¸­ï¼Œè¯·ç¨å'
                    }
                else:
                    del self.analysis_threads[arxiv_id]
            
            # æ›´æ–°åˆ†æçŠ¶æ€ä¸ºå¤„ç†ä¸­
            self.paper_service.update_analysis_status(arxiv_id, 'processing')
            
            # åŠ è½½å½“å‰é…ç½®
            current_config = self.load_config()
            analysis_config = {**current_config, **(config or {})}
            
            # åˆ›å»ºå¹¶å¯åŠ¨åˆ†æçº¿ç¨‹
            thread = threading.Thread(
                target=self._run_analysis,
                args=(arxiv_id, paper, analysis_config),
                daemon=True
            )
            thread.start()
            self.analysis_threads[arxiv_id] = thread
            
            logger.info(f"Started deep analysis for paper {arxiv_id}")
            
            return {
                'success': True,
                'message': 'æ·±åº¦åˆ†æå·²å¯åŠ¨',
                'status': 'processing'
            }
            
        except Exception as e:
            logger.error(f"Failed to start analysis for {arxiv_id}: {e}")
            try:
                self.paper_service.update_analysis_status(arxiv_id, 'failed')
            except:
                pass
            
            return {
                'success': False,
                'error': f'å¯åŠ¨åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _run_analysis(self, arxiv_id: str, paper: Dict[str, Any], config: Dict[str, Any]):
        """æ‰§è¡Œè®ºæ–‡åˆ†æï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        try:
            from pathlib import Path
            import re
            
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ·±åº¦åˆ†æ - ArXiv ID: {arxiv_id}")
            
            # åˆ›å»ºåˆ†ææœåŠ¡å®ä¾‹
            analysis_service = PaperAnalysisService(default_config=config)
            
            # è®¡ç®—è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
            project_root = Path(__file__).parent.parent.parent.parent
            paper_folder_path = str(project_root / "data" / "paper_analyze" / arxiv_id)
            
            # å‡†å¤‡è®ºæ–‡æ•°æ®ï¼ˆç”¨äºPDFä¸‹è½½ï¼‰
            paper_data = {
                'title': paper.get('title', ''),
                'link': f"https://arxiv.org/abs/{arxiv_id}",
                'snippet': paper.get('abstract', ''),
                'categories': paper.get('categories', ''),
                'arxiv_id': arxiv_id
            }
            
            # æ‰§è¡Œå®Œæ•´çš„æ·±åº¦åˆ†ææµç¨‹
            result = analysis_service.perform_deep_analysis(
                arxiv_id=arxiv_id,
                paper_folder_path=paper_folder_path,
                config=config,
                paper_data=paper_data
            )
            
            if result['success']:
                # ä½¿ç”¨Webåº”ç”¨ç‰¹æœ‰çš„å›¾ç‰‡è·¯å¾„å¤„ç†
                processed_content = self._process_image_paths(
                    result['analysis_result'], 
                    arxiv_id
                )
                
                # ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“
                self.paper_service.save_analysis_result(arxiv_id, processed_content)
                
                # é‡æ–°ä¿å­˜å¤„ç†åçš„å†…å®¹åˆ°æ–‡ä»¶
                with open(result['analysis_file_path'], 'w', encoding='utf-8') as f:
                    f.write(processed_content)
                
                logger.info(f"Analysis completed for {arxiv_id}, saved {len(processed_content)} characters")
                self.paper_service.update_analysis_status(arxiv_id, 'completed')
            else:
                logger.error(f"âŒ æ·±åº¦åˆ†æå¤±è´¥: {arxiv_id}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                self.paper_service.update_analysis_status(arxiv_id, 'failed')
            
        except Exception as e:
            logger.error(f"ğŸ’¥ åˆ†æè¿‡ç¨‹å¤±è´¥ {arxiv_id}: {e}")
            try:
                self.paper_service.update_analysis_status(arxiv_id, 'failed')
            except:
                pass
        finally:
            if arxiv_id in self.analysis_threads:
                del self.analysis_threads[arxiv_id]
    
    def _process_image_paths(self, content: str, arxiv_id: str) -> str:
        """å¤„ç†Markdownå†…å®¹ä¸­çš„å›¾ç‰‡è·¯å¾„ï¼Œå°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºå¯è®¿é—®çš„URLè·¯å¾„"""
        try:
            logger.info(f"ğŸ–¼ï¸ Starting image path processing for {arxiv_id}")
            
            # åŒ¹é… ![alt](imgs/filename) æ ¼å¼
            img_pattern = r'!\[([^\]]*)\]\((imgs/[^)]+)\)'
            
            def replace_image_path(match):
                alt_text = match.group(1)
                relative_path = match.group(2)
                filename = relative_path.replace('imgs/', '')
                new_path = f"/paper/{arxiv_id}/analysis_images/{filename}"
                logger.debug(f"  ğŸ“¸ Converting: {relative_path} â†’ {new_path}")
                return f"![{alt_text}]({new_path})"
            
            # ç»Ÿè®¡å’Œæ›¿æ¢
            original_matches = re.findall(img_pattern, content)
            logger.info(f"  ğŸ“Š Found {len(original_matches)} image references for {arxiv_id}")
            
            processed_content = re.sub(img_pattern, replace_image_path, content)
            
            # éªŒè¯å¤„ç†ç»“æœ
            processed_matches = re.findall(r'!\[([^\]]*)\]\((/paper/[^)]+)\)', processed_content)
            logger.info(f"  âœ… Successfully processed {len(processed_matches)} image paths for {arxiv_id}")
            
            return processed_content
            
        except Exception as e:
            logger.error(f"âŒ Failed to process image paths for {arxiv_id}: {e}")
            return content
    
    def get_analysis_status(self, arxiv_id: str) -> Dict[str, Any]:
        """è·å–åˆ†æçŠ¶æ€"""
        try:
            status_info = self.paper_service.get_analysis_status(arxiv_id)
            
            if not status_info:
                return {
                    'success': True,
                    'status': 'not_started',
                    'message': 'å°šæœªå¼€å§‹åˆ†æ'
                }
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„çº¿ç¨‹
            if arxiv_id in self.analysis_threads:
                thread = self.analysis_threads[arxiv_id]
                if thread.is_alive():
                    status_info['is_running'] = True
                else:
                    del self.analysis_threads[arxiv_id]
                    status_info['is_running'] = False
            else:
                status_info['is_running'] = False
            
            return {
                'success': True,
                **status_info
            }
            
        except Exception as e:
            logger.error(f"Failed to get analysis status for {arxiv_id}: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_analysis_result(self, arxiv_id: str) -> Dict[str, Any]:
        """è·å–åˆ†æç»“æœ"""
        try:
            result = self.paper_service.get_analysis_result(arxiv_id)
            
            if not result:
                return {
                    'success': False,
                    'error': 'åˆ†æç»“æœä¸å­˜åœ¨'
                }
            
            return {
                'success': True,
                **result
            }
            
        except Exception as e:
            logger.error(f"Failed to get analysis result for {arxiv_id}: {e}")
            return {
                'success': False,
                'error': str(e)
            }

# åˆ›å»ºé€‚é…å™¨å®ä¾‹
analysis_service = AnalysisServiceAdapter(paper_explore_service, redis_client)


# === è®ºæ–‡æ”¶é›†ç›¸å…³API (æ¥è‡ªPaperGather) ===

@api_bp.route('/collect/models')
def get_available_models():
    """è·å–å¯ç”¨çš„LLMæ¨¡å‹"""
    try:
        models = paper_gather_service.get_available_models()
        return jsonify({
            'success': True,
            'models': models
        })
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/collect/search_modes')
def get_search_modes():
    """è·å–å¯ç”¨çš„æœç´¢æ¨¡å¼"""
    try:
        modes = paper_gather_service.get_available_search_modes()
        return jsonify({
            'success': True,
            'search_modes': modes
        })
    except Exception as e:
        logger.error(f"è·å–æœç´¢æ¨¡å¼å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/collect/task/<task_id>/status')
def get_task_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        result = paper_gather_service.get_task_result(task_id)
        if not result:
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡ä¸å­˜åœ¨'
            }), 404
        
        return jsonify({
            'success': True,
            'task_result': result
        })
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/collect/task/<task_id>/stop', methods=['POST'])
def stop_task(task_id):
    """åœæ­¢ä»»åŠ¡"""
    try:
        success = paper_gather_service.stop_task(task_id)
        return jsonify({
            'success': success,
            'message': 'ä»»åŠ¡åœæ­¢æˆåŠŸ' if success else 'ä»»åŠ¡åœæ­¢å¤±è´¥'
        })
    except Exception as e:
        logger.error(f"åœæ­¢ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# === è®ºæ–‡æµè§ˆç›¸å…³API (æ¥è‡ªExplorePaperData) ===

@api_bp.route('/explore/search')
def api_search():
    """æœç´¢è®ºæ–‡"""
    try:
        query = request.args.get('q', '').strip()
        task_name = request.args.get('task_name', '').strip()
        task_id = request.args.get('task_id', '').strip()
        page = int(request.args.get('page', 1))
        per_page = min(int(request.args.get('per_page', 10)), 50)
        
        papers, total = paper_explore_service.search_papers(
            query=query,
            task_name=task_name,
            task_id=task_id,
            page=page, 
            per_page=per_page
        )
        
        return jsonify({
            'success': True,
            'data': papers,
            'total': total,
            'page': page,
            'per_page': per_page,
            'total_pages': (total + per_page - 1) // per_page
        })
    
    except Exception as e:
        logger.error(f"APIæœç´¢å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/explore/stats')
def api_stats():
    """è·å–ç»Ÿè®¡æ•°æ®"""
    try:
        stats = paper_explore_service.get_overview_stats()
        return jsonify({'success': True, 'data': stats})
    
    except Exception as e:
        logger.error(f"APIç»Ÿè®¡æ•°æ®è·å–å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/explore/tasks')
def api_tasks():
    """è·å–å¯ç”¨ä»»åŠ¡åˆ—è¡¨"""
    try:
        tasks_data = paper_explore_service.get_available_tasks()
        # å°†å¤æ‚çš„æ•°æ®ç»“æ„è½¬æ¢ä¸ºç®€å•çš„ä»»åŠ¡æ•°ç»„ï¼Œä¾›JavaScriptè¿­ä»£ä½¿ç”¨
        all_tasks = []
        
        # æ·»åŠ åŸºäºä»»åŠ¡åç§°çš„ä»»åŠ¡
        for task in tasks_data.get('task_names', []):
            all_tasks.append({
                'task_name': task['task_name'],
                'task_id': '',  # task_namesä¸­æ²¡æœ‰task_id
                'paper_count': task['paper_count']
            })
        
        # æ·»åŠ åŸºäºä»»åŠ¡IDçš„ä»»åŠ¡ï¼ˆé¿å…é‡å¤ï¼‰
        task_name_set = {task['task_name'] for task in all_tasks}
        for task in tasks_data.get('task_ids', []):
            if task['task_name'] not in task_name_set:
                all_tasks.append({
                    'task_name': task['task_name'],
                    'task_id': task['task_id'],
                    'paper_count': task['paper_count'],
                    'first_created': task.get('first_created'),
                    'last_created': task.get('last_created')
                })
        
        return jsonify({'success': True, 'data': {'tasks': all_tasks}})
    
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/explore/update_task_name', methods=['POST'])
def api_update_task_name():
    """æ›´æ–°ä»»åŠ¡åç§°"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_id = data.get('arxiv_id')
        new_task_name = (data.get('new_task_name') or '').strip()
        
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
            
        success = paper_explore_service.update_task_name(arxiv_id, new_task_name)
        
        if success:
            return jsonify({'success': True, 'message': 'ä»»åŠ¡åç§°æ›´æ–°æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'æ›´æ–°å¤±è´¥ï¼Œè®ºæ–‡ä¸å­˜åœ¨'}), 404
    
    except Exception as e:
        logger.error(f"æ›´æ–°ä»»åŠ¡åç§°å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/delete_paper', methods=['POST'])
def api_delete_paper_post():
    """åˆ é™¤å•ä¸ªè®ºæ–‡ (POSTè¯·æ±‚ï¼Œä»JSON bodyè·å–arxiv_id)"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'}), 400
        
        arxiv_id = data.get('arxiv_id')
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
        
        success = paper_explore_service.delete_paper(arxiv_id)
        
        if success:
            return jsonify({'success': True, 'message': 'è®ºæ–‡åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'åˆ é™¤å¤±è´¥ï¼Œè®ºæ–‡ä¸å­˜åœ¨'}), 404
    
    except Exception as e:
        logger.error(f"åˆ é™¤è®ºæ–‡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/explore/delete_paper/<arxiv_id>', methods=['DELETE'])
def api_delete_paper(arxiv_id):
    """åˆ é™¤å•ä¸ªè®ºæ–‡"""
    try:
        success = paper_explore_service.delete_paper(arxiv_id)
        
        if success:
            return jsonify({'success': True, 'message': 'è®ºæ–‡åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'åˆ é™¤å¤±è´¥ï¼Œè®ºæ–‡ä¸å­˜åœ¨'}), 404
    
    except Exception as e:
        logger.error(f"åˆ é™¤è®ºæ–‡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# === æ·±åº¦åˆ†æç›¸å…³API ===

@api_bp.route('/analysis/paper/<arxiv_id>/analyze', methods=['POST'])
def api_start_analysis(arxiv_id):
    """å¯åŠ¨æ·±åº¦è®ºæ–‡åˆ†æ"""
    try:
        logger.info(f"ğŸ¯ æ”¶åˆ°æ·±åº¦åˆ†æè¯·æ±‚ - ArXiv ID: {arxiv_id}")
        
        # è·å–é…ç½®å‚æ•°
        data = request.get_json() if request.is_json else {}
        config = data.get('config', {})
        
        # å¯åŠ¨åˆ†æ
        result = analysis_service.start_analysis(arxiv_id, config)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify(result), 400
            
    except Exception as e:
        logger.error(f"å¯åŠ¨æ·±åº¦åˆ†æå¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"å¯åŠ¨åˆ†æå¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/analysis/paper/<arxiv_id>/status')
def api_analysis_status(arxiv_id):
    """æŸ¥è¯¢åˆ†æçŠ¶æ€"""
    try:
        result = analysis_service.get_analysis_status(arxiv_id)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify(result), 404
            
    except Exception as e:
        logger.error(f"è·å–åˆ†æçŠ¶æ€å¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/analysis/paper/<arxiv_id>/result')
def api_analysis_result(arxiv_id):
    """è·å–åˆ†æç»“æœ"""
    try:
        result = analysis_service.get_analysis_result(arxiv_id)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify(result), 404
            
    except Exception as e:
        logger.error(f"è·å–åˆ†æç»“æœå¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"è·å–ç»“æœå¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/analysis/config', methods=['GET'])
def get_analysis_config():
    """è·å–æ·±åº¦åˆ†æé…ç½®å’Œå¯ç”¨æ¨¡å‹"""
    try:
        # å¯¼å…¥LLMFactory
        from HomeSystem.graph.llm_factory import LLMFactory
        factory = LLMFactory()
        
        # ä»Redisè·å–å½“å‰é…ç½®
        config_key = "analysis_config:global"
        current_config = {
            "analysis_model": "deepseek.DeepSeek_V3",
            "vision_model": "ollama.Qwen2_5_VL_7B",
            "timeout": 600
        }
        
        if redis_client:
            try:
                saved_config = redis_client.get(config_key)
                if saved_config:
                    current_config.update(json.loads(saved_config))
            except Exception as e:
                logger.warning(f"è¯»å–Redisé…ç½®å¤±è´¥: {e}")
        
        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        available_models = factory.get_available_llm_models()
        vision_models = factory.get_available_vision_models()
        
        return jsonify({
            'success': True,
            'data': {
                'current_config': current_config,
                'available_models': {
                    'analysis_models': available_models,
                    'vision_models': vision_models
                }
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–åˆ†æé…ç½®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"è·å–é…ç½®å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/analysis/config', methods=['POST'])
def save_analysis_config():
    """ä¿å­˜æ·±åº¦åˆ†æé…ç½®"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # éªŒè¯å¿…è¦å­—æ®µ
        analysis_model = data.get('analysis_model')
        vision_model = data.get('vision_model')
        timeout = data.get('timeout', 600)
        
        if not analysis_model or not vision_model:
            return jsonify({
                'success': False,
                'error': 'åˆ†ææ¨¡å‹å’Œè§†è§‰æ¨¡å‹ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # æ„å»ºé…ç½®
        config = {
            'analysis_model': analysis_model,
            'vision_model': vision_model,
            'timeout': timeout
        }
        
        # ä¿å­˜åˆ°Redis
        config_key = "analysis_config:global"
        if redis_client:
            try:
                redis_client.set(config_key, json.dumps(config))
                logger.info(f"é…ç½®å·²ä¿å­˜åˆ°Redis: {config}")
            except Exception as e:
                logger.error(f"ä¿å­˜é…ç½®åˆ°Rediså¤±è´¥: {e}")
                return jsonify({
                    'success': False,
                    'error': f'ä¿å­˜é…ç½®å¤±è´¥: {str(e)}'
                }), 500
        
        return jsonify({
            'success': True,
            'message': 'é…ç½®ä¿å­˜æˆåŠŸ',
            'config': config
        })
        
    except Exception as e:
        logger.error(f"ä¿å­˜åˆ†æé…ç½®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"ä¿å­˜é…ç½®å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/settings/save', methods=['POST'])
def save_settings():
    """ä¿å­˜ç³»ç»Ÿè®¾ç½®ï¼ˆæ¨¡å‹è®¾ç½®å’Œæ·±åº¦åˆ†æè®¾ç½®ï¼‰"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # éªŒè¯å¿…è¦å­—æ®µ
        llm_model_name = data.get('llm_model_name')
        if not llm_model_name:
            return jsonify({
                'success': False,
                'error': 'LLMæ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # æ„å»ºé…ç½®
        config = {
            # LLMé…ç½®
            'llm_model_name': llm_model_name,
            'relevance_threshold': data.get('relevance_threshold', 0.7),
            'abstract_analysis_model': data.get('abstract_analysis_model'),
            'full_paper_analysis_model': data.get('full_paper_analysis_model'),
            'deep_analysis_model': data.get('deep_analysis_model'),
            'vision_model': data.get('vision_model'),
            
            # æ·±åº¦åˆ†æé…ç½®
            'enable_deep_analysis': data.get('enable_deep_analysis', True),
            'deep_analysis_threshold': data.get('deep_analysis_threshold', 0.8),
            'ocr_char_limit_for_analysis': data.get('ocr_char_limit_for_analysis', 10000),
            'analysis_timeout': data.get('analysis_timeout', 600)
        }
        
        # ä¿å­˜åˆ°Redis
        config_key = "system_settings:global"
        if redis_client:
            try:
                redis_client.set(config_key, json.dumps(config))
                logger.info(f"ç³»ç»Ÿè®¾ç½®å·²ä¿å­˜åˆ°Redis: {config}")
            except Exception as e:
                logger.error(f"ä¿å­˜ç³»ç»Ÿè®¾ç½®åˆ°Rediså¤±è´¥: {e}")
                return jsonify({
                    'success': False,
                    'error': f'ä¿å­˜è®¾ç½®å¤±è´¥: {str(e)}'
                }), 500
        
        return jsonify({
            'success': True,
            'message': 'è®¾ç½®ä¿å­˜æˆåŠŸ',
            'config': config
        })
        
    except Exception as e:
        logger.error(f"ä¿å­˜ç³»ç»Ÿè®¾ç½®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"ä¿å­˜è®¾ç½®å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/settings/load', methods=['GET'])
def load_settings():
    """åŠ è½½ç³»ç»Ÿè®¾ç½®"""
    try:
        config_key = "system_settings:global"
        config = {}
        
        if redis_client:
            try:
                config_data = redis_client.get(config_key)
                if config_data:
                    config = json.loads(config_data)
                    logger.info(f"ä»RedisåŠ è½½ç³»ç»Ÿè®¾ç½®: {config}")
            except Exception as e:
                logger.error(f"ä»RedisåŠ è½½ç³»ç»Ÿè®¾ç½®å¤±è´¥: {e}")
        
        return jsonify({
            'success': True,
            'config': config
        })
        
    except Exception as e:
        logger.error(f"åŠ è½½ç³»ç»Ÿè®¾ç½®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"åŠ è½½è®¾ç½®å¤±è´¥: {str(e)}"
        }), 500


# === ä»»åŠ¡å†å²ç®¡ç†ç›¸å…³API ===

@api_bp.route('/task/details/<task_id>')
def get_task_details(task_id):
    """è·å–ä»»åŠ¡è¯¦ç»†ä¿¡æ¯ - ç”¨äºå‰ç«¯æ¨¡æ€æ¡†æ˜¾ç¤º"""
    try:
        result = paper_gather_service.get_task_result(task_id)
        if not result:
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡ä¸å­˜åœ¨'
            }), 404
        
        return jsonify({
            'success': True,
            'data': result
        })
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500



@api_bp.route('/task/history')
def get_task_history():
    """è·å–ä»»åŠ¡å†å²åˆ—è¡¨"""
    try:
        page = int(request.args.get('page', 1))
        per_page = min(int(request.args.get('per_page', 50)), 100)
        
        # è·å–ä»»åŠ¡å†å²
        tasks = paper_gather_service.get_task_history(limit=per_page * 5)  # è·å–æ›´å¤šæ•°æ®ç”¨äºåˆ†é¡µ
        
        # è®¡ç®—åˆ†é¡µ
        total = len(tasks)
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        paginated_tasks = tasks[start_idx:end_idx]
        
        return jsonify({
            'success': True,
            'data': {
                'tasks': paginated_tasks,
                'pagination': {
                    'page': page,
                    'per_page': per_page,
                    'total': total,
                    'total_pages': (total + per_page - 1) // per_page
                }
            }
        })
    
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡å†å²å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/task/history/<task_id>', methods=['DELETE'])
def delete_task_history(task_id):
    """åˆ é™¤å†å²ä»»åŠ¡"""
    try:
        success, error_msg = paper_gather_service.delete_task_history(task_id)
        
        if success:
            return jsonify({
                'success': True,
                'message': 'ä»»åŠ¡åˆ é™¤æˆåŠŸ'
            })
        else:
            return jsonify({
                'success': False,
                'error': error_msg or 'åˆ é™¤å¤±è´¥'
            }), 400
    
    except Exception as e:
        logger.error(f"åˆ é™¤ä»»åŠ¡å†å²å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/task/config/<task_id>')
def get_task_config(task_id):
    """è·å–ç‰¹å®šä»»åŠ¡çš„é…ç½®"""
    try:
        # è·å–ä»»åŠ¡é…ç½®
        task_config = paper_gather_service.get_task_config_by_id(task_id)
        
        if not task_config:
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡ä¸å­˜åœ¨'
            }), 404
        
        return jsonify({
            'success': True,
            'data': {
                'task_id': task_id,
                'config': task_config
            }
        })
    
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡é…ç½®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/config/presets')
def get_config_presets():
    """è·å–é…ç½®é¢„è®¾åˆ—è¡¨"""
    try:
        # è¿”å›ç©ºçš„é¢„è®¾åˆ—è¡¨ï¼ˆå¯ä»¥åç»­æ‰©å±•ï¼‰
        return jsonify({
            'success': True,
            'data': {
                'presets': []
            }
        })
    
    except Exception as e:
        logger.error(f"è·å–é…ç½®é¢„è®¾å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/config/presets', methods=['POST'])
def create_config_preset():
    """åˆ›å»ºé…ç½®é¢„è®¾"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # ç›®å‰è¿”å›æˆåŠŸä½†ä¸å®é™…ä¿å­˜ï¼ˆå¯ä»¥åç»­æ‰©å±•ï¼‰
        return jsonify({
            'success': True,
            'message': 'é¢„è®¾åˆ›å»ºæˆåŠŸï¼ˆåŠŸèƒ½æš‚æœªå®Œå…¨å®ç°ï¼‰',
            'preset_id': 'temp_id'
        })
    
    except Exception as e:
        logger.error(f"åˆ›å»ºé…ç½®é¢„è®¾å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/config/presets/<preset_id>', methods=['DELETE'])
def delete_config_preset(preset_id):
    """åˆ é™¤é…ç½®é¢„è®¾"""
    try:
        # ç›®å‰è¿”å›æˆåŠŸä½†ä¸å®é™…åˆ é™¤ï¼ˆå¯ä»¥åç»­æ‰©å±•ï¼‰
        return jsonify({
            'success': True,
            'message': 'é¢„è®¾åˆ é™¤æˆåŠŸï¼ˆåŠŸèƒ½æš‚æœªå®Œå…¨å®ç°ï¼‰'
        })
    
    except Exception as e:
        logger.error(f"åˆ é™¤é…ç½®é¢„è®¾å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/models')
def get_models():
    """è·å–å¯ç”¨çš„LLMæ¨¡å‹åˆ—è¡¨"""
    try:
        models = paper_gather_service.get_available_models()
        return jsonify({
            'success': True,
            'data': models
        })
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# === ç³»ç»ŸçŠ¶æ€ç›¸å…³API ===

@api_bp.route('/running_tasks')
def get_running_tasks():
    """è·å–è¿è¡Œä¸­ä»»åŠ¡çŠ¶æ€ - ç”¨äºé¦–é¡µå®æ—¶æ›´æ–°"""
    try:
        # è·å–è¿è¡Œä¸­ä»»åŠ¡æ•°é‡å’Œè¯¦æƒ…
        running_count = paper_gather_service.get_running_tasks_count()
        running_details = paper_gather_service.get_running_tasks_detail()
        
        return jsonify({
            'success': True,
            'data': {
                'count': running_count,
                'details': running_details
            }
        })
    except Exception as e:
        logger.error(f"è·å–è¿è¡Œä¸­ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/system_stats')
def get_system_stats():
    """è·å–ç³»ç»Ÿæ ¸å¿ƒç»Ÿè®¡æ•°æ® - ç”¨äºé¦–é¡µå±•ç¤ºå¡ç‰‡"""
    try:
        # è·å–è®ºæ–‡æ”¶é›†ç»Ÿè®¡
        collect_stats = paper_data_service.get_paper_statistics()
        
        # è·å–è¿è¡Œä¸­ä»»åŠ¡æ•°é‡
        running_tasks_count = paper_gather_service.get_running_tasks_count()
        
        # è·å–å®šæ—¶ä»»åŠ¡æ•°é‡
        scheduled_tasks = paper_gather_service.get_scheduled_tasks()
        scheduled_count = len(scheduled_tasks) if scheduled_tasks else 0
        
        # æ„å»ºæ ¸å¿ƒç»Ÿè®¡æ•°æ®
        core_stats = {
            'total_papers': collect_stats.get('total_papers', 0),
            'analyzed_papers': collect_stats.get('analyzed_papers', 0),
            'running_tasks': running_tasks_count,
            'scheduled_tasks': scheduled_count
        }
        
        return jsonify({
            'success': True,
            'data': core_stats
        })
    except Exception as e:
        logger.error(f"è·å–ç³»ç»Ÿç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/tasks/status')
def get_tasks_status():
    """è·å–æ‰€æœ‰æ´»åŠ¨ä»»åŠ¡çš„çŠ¶æ€ - ç”¨äºå‰ç«¯å®šæ—¶åˆ·æ–°"""
    try:
        # è·å–è¿è¡Œä¸­ä»»åŠ¡è¯¦æƒ…
        running_tasks = paper_gather_service.get_running_tasks_detail()
        
        # æ ¼å¼åŒ–ä»»åŠ¡çŠ¶æ€æ•°æ®ä¾›å‰ç«¯ä½¿ç”¨
        status_data = []
        for task in running_tasks:
            status_data.append({
                'task_id': task.get('task_id', ''),
                'status': task.get('status', 'unknown'),
                'progress': task.get('progress', 0.0),
                'start_time': task.get('start_time'),
                'duration': task.get('duration')
            })
        
        return jsonify({
            'success': True,
            'data': status_data
        })
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ========== è¿ç§»å’Œä»»åŠ¡ç®¡ç†ç›¸å…³API ==========

@api_bp.route('/tasks/available_for_migration')
def api_get_available_tasks_for_migration():
    """è·å–å¯ç”¨äºè¿ç§»çš„ä»»åŠ¡åˆ—è¡¨"""
    try:
        tasks = paper_explore_service.get_available_tasks()
        return jsonify({'success': True, 'data': {'tasks': tasks}})
    
    except Exception as e:
        logger.error(f"è·å–è¿ç§»ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/get_tasks')
def api_get_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨ï¼ˆç”¨äºæ‰¹é‡æ“ä½œï¼‰"""
    try:
        tasks_data = paper_explore_service.get_available_tasks()
        # å°†å¤æ‚çš„æ•°æ®ç»“æ„è½¬æ¢ä¸ºç®€å•çš„ä»»åŠ¡æ•°ç»„ï¼Œä¾›æ‰¹é‡æ“ä½œä½¿ç”¨
        all_tasks = []
        
        # æ·»åŠ åŸºäºä»»åŠ¡åç§°çš„ä»»åŠ¡
        for task in tasks_data.get('task_names', []):
            all_tasks.append({
                'task_name': task['task_name'],
                'task_id': '',  # task_namesä¸­æ²¡æœ‰task_id
                'paper_count': task['paper_count'],
                'created_at': None  # task_namesä¸­æ²¡æœ‰æ—¶é—´ä¿¡æ¯
            })
        
        # æ·»åŠ åŸºäºä»»åŠ¡IDçš„ä»»åŠ¡ï¼ˆé¿å…é‡å¤ï¼‰
        task_name_set = {task['task_name'] for task in all_tasks}
        for task in tasks_data.get('task_ids', []):
            if task['task_name'] not in task_name_set:
                all_tasks.append({
                    'task_name': task['task_name'],
                    'task_id': task['task_id'],
                    'paper_count': task['paper_count'],
                    'created_at': task.get('last_created')  # ä½¿ç”¨last_createdä½œä¸ºå±•ç¤ºæ—¶é—´
                })
        
        return jsonify({'success': True, 'tasks': all_tasks})
    
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/migrate_paper_to_task', methods=['POST'])
def api_migrate_paper_to_task():
    """å•ä¸ªè®ºæ–‡è¿ç§»åˆ°æ–°ä»»åŠ¡"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_id = data.get('arxiv_id')
        target_task_name = data.get('target_task_name', '').strip()
        target_task_id = data.get('target_task_id', '').strip()
        
        if not arxiv_id or not target_task_name:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        success = paper_explore_service.migrate_paper_to_task(
            arxiv_id, target_task_name, target_task_id or None
        )
        
        if success:
            return jsonify({'success': True, 'message': 'è®ºæ–‡è¿ç§»æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'è¿ç§»å¤±è´¥ï¼Œè®ºæ–‡ä¸å­˜åœ¨'}), 404
    
    except Exception as e:
        logger.error(f"è®ºæ–‡è¿ç§»å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/batch_migrate_to_task', methods=['POST'])
def api_batch_migrate_to_task():
    """æ‰¹é‡è®ºæ–‡è¿ç§»åˆ°æ–°ä»»åŠ¡"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_ids = data.get('arxiv_ids', [])
        target_task_name = data.get('target_task_name', '').strip()
        target_task_id = data.get('target_task_id', '').strip()
        
        if not arxiv_ids or not target_task_name:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        affected_rows = paper_explore_service.batch_migrate_papers_to_task(
            arxiv_ids, target_task_name, target_task_id or None
        )
        
        return jsonify({
            'success': True, 
            'affected_rows': affected_rows,
            'message': f'æˆåŠŸè¿ç§» {affected_rows} ç¯‡è®ºæ–‡'
        })
    
    except Exception as e:
        logger.error(f"æ‰¹é‡è¿ç§»å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/assign_task_to_paper', methods=['POST'])
def api_assign_task_to_paper():
    """ä¸ºè®ºæ–‡åˆ†é…ä»»åŠ¡"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_id = data.get('arxiv_id')
        task_name = data.get('task_name', '').strip()
        task_id = data.get('task_id', '').strip()
        
        if not arxiv_id or not task_name:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        success = paper_explore_service.assign_task_to_paper(
            arxiv_id, task_name, task_id or None
        )
        
        if success:
            return jsonify({'success': True, 'message': 'ä»»åŠ¡åˆ†é…æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'åˆ†é…å¤±è´¥ï¼Œè®ºæ–‡ä¸å­˜åœ¨'}), 404
    
    except Exception as e:
        logger.error(f"ä»»åŠ¡åˆ†é…å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/batch_assign_task', methods=['POST'])
def api_batch_assign_task():
    """æ‰¹é‡åˆ†é…ä»»åŠ¡"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_ids = data.get('arxiv_ids', [])
        task_name = data.get('task_name', '').strip()
        task_id = data.get('task_id', '').strip()
        
        if not arxiv_ids or not task_name:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        affected_rows = paper_explore_service.batch_assign_task_to_papers(
            arxiv_ids, task_name, task_id or None
        )
        
        return jsonify({
            'success': True, 
            'affected_rows': affected_rows,
            'message': f'æˆåŠŸä¸º {affected_rows} ç¯‡è®ºæ–‡åˆ†é…ä»»åŠ¡'
        })
    
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ†é…ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# === ä¸‹è½½åˆ†æç›¸å…³API ===

@api_bp.route('/paper/<arxiv_id>/download_analysis')
def api_download_analysis(arxiv_id):
    """APIæ¥å£ - ä¸‹è½½åˆ†æç»“æœï¼ˆMarkdown + å›¾ç‰‡æ‰“åŒ…ä¸ºZIPï¼‰"""
    try:
        # è·å–åˆ†æç»“æœ
        result = analysis_service.get_analysis_result(arxiv_id)
        
        if not result['success']:
            return jsonify({
                'success': False,
                'error': 'åˆ†æç»“æœä¸å­˜åœ¨'
            }), 404
        
        # åˆ›å»ºä¸´æ—¶ZIPæ–‡ä»¶
        temp_zip = tempfile.NamedTemporaryFile(delete=False, suffix='.zip')
        
        try:
            with zipfile.ZipFile(temp_zip.name, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                # æ·»åŠ Markdownæ–‡ä»¶
                markdown_content = result['content']
                
                # å¤„ç†å›¾ç‰‡è·¯å¾„ï¼Œè½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
                processed_markdown = _process_markdown_for_download(markdown_content, arxiv_id)
                
                zip_file.writestr(f"{arxiv_id}_analysis.md", processed_markdown)
                
                # æ·»åŠ å›¾ç‰‡æ–‡ä»¶
                images_dir = os.path.join(PROJECT_ROOT, "data/paper_analyze", arxiv_id, "imgs")
                if os.path.exists(images_dir):
                    for filename in os.listdir(images_dir):
                        if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                            image_path = os.path.join(images_dir, filename)
                            if os.path.isfile(image_path):
                                zip_file.write(image_path, f"imgs/{filename}")
            
            # è¿”å›ZIPæ–‡ä»¶
            return send_file(
                temp_zip.name,
                as_attachment=True,
                download_name=f"{arxiv_id}_deep_analysis.zip",
                mimetype='application/zip'
            )
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆåœ¨å‘é€åä¼šè¢«è‡ªåŠ¨åˆ é™¤ï¼‰
            pass
            
    except Exception as e:
        logger.error(f"ä¸‹è½½åˆ†æç»“æœå¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"ä¸‹è½½å¤±è´¥: {str(e)}"
        }), 500


def _process_markdown_for_download(content: str, arxiv_id: str) -> str:
    """
    å¤„ç†Markdownå†…å®¹ï¼Œå°†ç½‘é¡µURLè·¯å¾„è½¬æ¢ä¸ºæœ¬åœ°ç›¸å¯¹è·¯å¾„
    
    Args:
        content: åŸå§‹Markdownå†…å®¹
        arxiv_id: ArXivè®ºæ–‡ID
        
    Returns:
        str: å¤„ç†åçš„Markdownå†…å®¹
    """
    try:
        # å°†ç½‘é¡µURLè·¯å¾„è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
        pattern = rf'/paper/{re.escape(arxiv_id)}/analysis_images/([^)]+)'
        replacement = r'imgs/\1'
        
        processed_content = re.sub(pattern, replacement, content)
        
        return processed_content
        
    except Exception as e:
        logger.error(f"å¤„ç†Markdownä¸‹è½½å†…å®¹å¤±è´¥: {e}")
        return content


# === ä¸­æ–‡æœç´¢åŠ©æ‰‹API ===

@api_bp.route('/search/translate', methods=['POST'])
def translate_chinese_search():
    """ä¸­æ–‡æœç´¢éœ€æ±‚è½¬æ¢ä¸ºè‹±æ–‡æœç´¢å…³é”®è¯å’Œéœ€æ±‚æè¿°"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æä¾›JSONæ•°æ®'
            }), 400
        
        chinese_input = data.get('chinese_input', '').strip()
        model_name = data.get('model_name', 'ollama.Qwen3_30B')
        
        if not chinese_input:
            return jsonify({
                'success': False,
                'error': 'è¯·è¾“å…¥ä¸­æ–‡æœç´¢éœ€æ±‚'
            }), 400
        
        # å¯¼å…¥å¹¶åˆ›å»ºä¸­æ–‡æœç´¢åŠ©æ‰‹
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
        from HomeSystem.workflow.paper_gather_task.chinese_search_assistant import ChineseSearchAssistantLLM
        
        assistant = ChineseSearchAssistantLLM(model_name=model_name)
        result = assistant.convert_chinese_to_english_search(chinese_input)
        
        return jsonify({
            'success': True,
            'data': {
                'search_keywords': result.search_keywords,
                'user_requirements': result.user_requirements,
                'suggested_task_name': result.suggested_task_name,
                'confidence': result.confidence,
                'notes': result.notes,
                'model_used': model_name
            }
        })
    
    except Exception as e:
        logger.error(f"ä¸­æ–‡æœç´¢è½¬æ¢å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"è½¬æ¢å¤±è´¥: {str(e)}"
        }), 500


# === DifyçŸ¥è¯†åº“ç›¸å…³API ===

@api_bp.route('/dify_upload_all_eligible', methods=['POST'])
def api_dify_upload_all_eligible():
    """ä¸€é”®ä¸Šä¼ å…¨éƒ¨ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡åˆ°DifyçŸ¥è¯†åº“"""
    try:
        data = request.get_json() if request.is_json else {}
        filters = data.get('filters', {})
        
        # è°ƒç”¨æ‰¹é‡ä¸Šä¼ æœåŠ¡
        result = dify_service.upload_all_eligible_papers_with_summary(filters)
        
        if result.get('success'):
            return jsonify(result)
        else:
            return jsonify(result), 400
            
    except Exception as e:
        logger.error(f"ä¸€é”®ä¸Šä¼ å…¨éƒ¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"ä¸Šä¼ å¤±è´¥: {str(e)}",
            'total_eligible': 0,
            'success_count': 0,
            'failed_count': 0,
            'progress': 0
        }), 500


@api_bp.route('/dify_batch_verify', methods=['POST'])
def api_dify_batch_verify():
    """ä¸€é”®éªŒè¯çŸ¥è¯†åº“ä¸­æ‰€æœ‰æ–‡æ¡£çš„çŠ¶æ€"""
    try:
        # è°ƒç”¨æ‰¹é‡éªŒè¯æœåŠ¡
        result = dify_service.batch_verify_all_documents()
        
        if result.get('success'):
            return jsonify(result)
        else:
            return jsonify(result), 400
            
    except Exception as e:
        logger.error(f"ä¸€é”®éªŒè¯çŸ¥è¯†åº“å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"éªŒè¯å¤±è´¥: {str(e)}",
            'total': 0,
            'verified': 0,
            'failed': 0,
            'missing': 0,
            'progress': 0
        }), 500


@api_bp.route('/dify_upload/<arxiv_id>', methods=['POST'])
def api_dify_upload_single(arxiv_id):
    """ä¸Šä¼ å•ä¸ªè®ºæ–‡åˆ°DifyçŸ¥è¯†åº“"""
    try:
        result = dify_service.upload_paper_to_dify(arxiv_id)
        
        if result.get('success'):
            return jsonify(result)
        else:
            return jsonify(result), 400
            
    except Exception as e:
        logger.error(f"ä¸Šä¼ è®ºæ–‡åˆ°Difyå¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"ä¸Šä¼ å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/dify_remove/<arxiv_id>', methods=['POST', 'DELETE'])
def api_dify_remove_single(arxiv_id):
    """ç§»é™¤å•ä¸ªè®ºæ–‡ä»DifyçŸ¥è¯†åº“ï¼ˆæ— /api/å‰ç¼€ç‰ˆæœ¬ï¼‰"""
    return api_dify_remove_single_paper(arxiv_id)


@api_bp.route('/dify_status/<arxiv_id>')
def api_dify_status(arxiv_id):
    """æ£€æŸ¥å•ä¸ªè®ºæ–‡åœ¨Difyä¸­çš„çŠ¶æ€"""
    try:
        result = dify_service.verify_dify_document(arxiv_id)
        
        if result.get('success'):
            return jsonify(result)
        else:
            return jsonify(result), 404
    except Exception as e:
        logger.error(f"æ£€æŸ¥DifyçŠ¶æ€å¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"çŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}"
        }), 500

@api_bp.route('/api/upload_to_dify', methods=['POST'])
def api_upload_to_dify():
    """æ‰¹é‡ä¸Šä¼ è®ºæ–‡åˆ°DifyçŸ¥è¯†åº“ - å…¼å®¹å‰ç«¯è°ƒç”¨"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_ids = data.get('arxiv_ids', [])
        if not arxiv_ids:
            return jsonify({'success': False, 'error': 'æœªæä¾›è®ºæ–‡IDåˆ—è¡¨'}), 400
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªIDï¼Œç›´æ¥è°ƒç”¨å•ç¯‡ä¸Šä¼ 
        if len(arxiv_ids) == 1:
            result = dify_service.upload_paper_to_dify(arxiv_ids[0])
            return jsonify(result)
        
        # å¤šä¸ªIDçš„æƒ…å†µï¼Œè°ƒç”¨æ‰¹é‡ä¸Šä¼ 
        results = {
            'success_count': 0,
            'failed_count': 0,
            'results': []
        }
        
        for arxiv_id in arxiv_ids:
            try:
                result = dify_service.upload_paper_to_dify(arxiv_id)
                if result.get('success'):
                    results['success_count'] += 1
                    results['results'].append({
                        'arxiv_id': arxiv_id,
                        'status': 'success',
                        'message': 'ä¸Šä¼ æˆåŠŸ'
                    })
                else:
                    results['failed_count'] += 1
                    results['results'].append({
                        'arxiv_id': arxiv_id,
                        'status': 'failed',
                        'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
                    })
            except Exception as e:
                results['failed_count'] += 1
                results['results'].append({
                    'arxiv_id': arxiv_id,
                    'status': 'failed',
                    'error': str(e)
                })
        
        # å¦‚æœå…¨éƒ¨æˆåŠŸ
        if results['failed_count'] == 0:
            return jsonify({
                'success': True,
                'message': f'æˆåŠŸä¸Šä¼ {results["success_count"]}ç¯‡è®ºæ–‡',
                'data': results
            })
        else:
            return jsonify({
                'success': False,
                'error': f'ä¸Šä¼ å¤±è´¥{results["failed_count"]}ç¯‡ï¼ŒæˆåŠŸ{results["success_count"]}ç¯‡',
                'data': results
            }), 400
            
    except Exception as e:
        logger.error(f"æ‰¹é‡ä¸Šä¼ è®ºæ–‡åˆ°Difyå¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@api_bp.route('/api/remove_from_dify', methods=['POST'])
def api_remove_from_dify():
    """ä»DifyçŸ¥è¯†åº“ç§»é™¤è®ºæ–‡ - å…¼å®¹å‰ç«¯è°ƒç”¨"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_ids = data.get('arxiv_ids', [])
        if not arxiv_ids:
            return jsonify({'success': False, 'error': 'æœªæä¾›è®ºæ–‡IDåˆ—è¡¨'}), 400
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç§»é™¤åŠŸèƒ½
        if not hasattr(dify_service, 'remove_paper_from_dify'):
            return jsonify({
                'success': False,
                'error': 'æš‚ä¸æ”¯æŒä»Difyç§»é™¤è®ºæ–‡åŠŸèƒ½'
            }), 501
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªID
        if len(arxiv_ids) == 1:
            result = dify_service.remove_paper_from_dify(arxiv_ids[0])
            return jsonify(result)
        
        # å¤šä¸ªIDçš„æƒ…å†µ
        results = {
            'success_count': 0,
            'failed_count': 0,
            'results': []
        }
        
        for arxiv_id in arxiv_ids:
            try:
                result = dify_service.remove_paper_from_dify(arxiv_id)
                if result.get('success'):
                    results['success_count'] += 1
                    results['results'].append({
                        'arxiv_id': arxiv_id,
                        'status': 'success',
                        'message': 'ç§»é™¤æˆåŠŸ'
                    })
                else:
                    results['failed_count'] += 1
                    results['results'].append({
                        'arxiv_id': arxiv_id,
                        'status': 'failed',
                        'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
                    })
            except Exception as e:
                results['failed_count'] += 1
                results['results'].append({
                    'arxiv_id': arxiv_id,
                    'status': 'failed',
                    'error': str(e)
                })
        
        # è¿”å›ç»“æœ
        if results['failed_count'] == 0:
            return jsonify({
                'success': True,
                'message': f'æˆåŠŸç§»é™¤{results["success_count"]}ç¯‡è®ºæ–‡',
                'data': results
            })
        else:
            return jsonify({
                'success': False,
                'error': f'ç§»é™¤å¤±è´¥{results["failed_count"]}ç¯‡ï¼ŒæˆåŠŸ{results["success_count"]}ç¯‡',
                'data': results
            }), 400
            
    except Exception as e:
        logger.error(f"ä»Difyç§»é™¤è®ºæ–‡å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/dify_statistics')
def api_dify_statistics():
    """è·å–DifyçŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # è·å–åŸºæœ¬ç»Ÿè®¡
        eligible_papers = dify_service.get_eligible_papers_for_upload()
        
        # è®¡ç®—å·²ä¸Šä¼ çš„è®ºæ–‡æ•°é‡
        all_papers = paper_explore_service.get_paper_statistics()
        uploaded_count = all_papers.get('dify_uploaded', 0) if all_papers else 0
        
        statistics = {
            'total_papers': all_papers.get('total_papers', 0) if all_papers else 0,
            'eligible_for_upload': len(eligible_papers),
            'already_uploaded': uploaded_count,
            'dify_service_available': dify_service.is_available()
        }
        
        return jsonify({
            'success': True,
            'data': statistics
        })
        
    except Exception as e:
        logger.error(f"è·å–Difyç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/dify_upload/<arxiv_id>', methods=['POST'])
def api_dify_upload_single_paper(arxiv_id):
    """å•ä¸ªè®ºæ–‡ä¸Šä¼ åˆ°DifyçŸ¥è¯†åº“ - å…¼å®¹ExplorePaperDataè°ƒç”¨æ¨¡å¼"""
    try:
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
        
        # è°ƒç”¨shared serviceä¸Šä¼ 
        result = dify_service.upload_paper_to_dify(arxiv_id)
        
        if result.get('success'):
            return jsonify({
                'success': True,
                'message': 'è®ºæ–‡ä¸Šä¼ æˆåŠŸ',
                'data': {
                    'arxiv_id': arxiv_id,
                    'dataset_id': result.get('dataset_id'),
                    'document_id': result.get('document_id'),
                    'document_name': result.get('document_name')
                }
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'ä¸Šä¼ å¤±è´¥')
            }), 400
            
    except Exception as e:
        logger.error(f"ä¸Šä¼ å•ä¸ªè®ºæ–‡åˆ°Difyå¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/dify_remove/<arxiv_id>', methods=['POST', 'DELETE'])
def api_dify_remove_single_paper(arxiv_id):
    """ä»DifyçŸ¥è¯†åº“ç§»é™¤å•ä¸ªè®ºæ–‡ - å…¼å®¹ExplorePaperDataè°ƒç”¨æ¨¡å¼"""
    try:
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
        
        # è°ƒç”¨shared serviceç§»é™¤
        result = dify_service.remove_paper_from_dify(arxiv_id)
        
        if result.get('success'):
            return jsonify({
                'success': True,
                'message': 'è®ºæ–‡ç§»é™¤æˆåŠŸ',
                'data': {
                    'arxiv_id': arxiv_id
                }
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'ç§»é™¤å¤±è´¥')
            }), 400
            
    except Exception as e:
        logger.error(f"ä»Difyç§»é™¤å•ä¸ªè®ºæ–‡å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/dify_verify/<arxiv_id>', methods=['POST'])
def api_dify_verify_single_paper(arxiv_id):
    """éªŒè¯è®ºæ–‡æ˜¯å¦å­˜åœ¨äºDifyæœåŠ¡å™¨ - å…¼å®¹ExplorePaperDataè°ƒç”¨æ¨¡å¼"""
    try:
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
        
        # è°ƒç”¨shared serviceéªŒè¯
        result = dify_service.verify_dify_document(arxiv_id)
        
        if result.get('success'):
            return jsonify({
                'success': True,
                'data': result
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'éªŒè¯å¤±è´¥')
            }), 404
            
    except Exception as e:
        logger.error(f"éªŒè¯Difyæ–‡æ¡£å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/dify_clean/<arxiv_id>', methods=['POST'])
def api_dify_clean_single_paper(arxiv_id):
    """æ¸…ç†æ— æ•ˆçš„Difyæ–‡æ¡£è®°å½• - å…¼å®¹ExplorePaperDataè°ƒç”¨æ¨¡å¼"""
    try:
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
        
        # è·å–è®ºæ–‡æ•°æ®å¹¶æ¸…ç†æ— æ•ˆè®°å½•
        paper_dict = dify_service._get_paper_data(arxiv_id)
        if not paper_dict:
            return jsonify({'success': False, 'error': 'è®ºæ–‡ä¸å­˜åœ¨'}), 400
        
        # æ¸…ç†æ•°æ®åº“ä¸­çš„Difyä¿¡æ¯
        if dify_service.db_ops:
            paper = dify_service.db_ops.get_by_field(ArxivPaperModel, 'arxiv_id', arxiv_id)
            if paper:
                arxiv_paper = paper if isinstance(paper, ArxivPaperModel) else ArxivPaperModel.from_dict(paper.to_dict())
                arxiv_paper.clear_dify_info()
                
                clear_data = {
                    'dify_dataset_id': arxiv_paper.dify_dataset_id,
                    'dify_document_id': arxiv_paper.dify_document_id,
                    'dify_document_name': arxiv_paper.dify_document_name,
                    'dify_character_count': arxiv_paper.dify_character_count,
                    'dify_segment_count': arxiv_paper.dify_segment_count,
                    'dify_upload_time': arxiv_paper.dify_upload_time,
                    'dify_metadata': json.dumps(arxiv_paper.dify_metadata) if arxiv_paper.dify_metadata else '{}'
                }
                
                success = dify_service.db_ops.update(arxiv_paper, clear_data)
                if success:
                    return jsonify({
                        'success': True,
                        'message': 'æ— æ•ˆè®°å½•å·²æ¸…ç†',
                        'data': {'arxiv_id': arxiv_id}
                    })
                else:
                    return jsonify({'success': False, 'error': 'æ¸…ç†å¤±è´¥'}), 500
            else:
                return jsonify({'success': False, 'error': 'è®ºæ–‡ä¸å­˜åœ¨'}), 400
        else:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“æœåŠ¡ä¸å¯ç”¨'}), 500
            
    except Exception as e:
        logger.error(f"æ¸…ç†Difyè®°å½•å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/dify_validate_upload/<arxiv_id>')
def api_dify_validate_upload_single_paper(arxiv_id):
    """éªŒè¯è®ºæ–‡ä¸Šä¼ å‰ç½®æ¡ä»¶ - å…¼å®¹ExplorePaperDataè°ƒç”¨æ¨¡å¼"""
    try:
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
        
        # è°ƒç”¨shared serviceéªŒè¯ä¸Šä¼ å‰ç½®æ¡ä»¶
        result = dify_service.validate_upload_preconditions(arxiv_id)
        
        return jsonify(result)
            
    except Exception as e:
        logger.error(f"éªŒè¯ä¸Šä¼ å‰ç½®æ¡ä»¶å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500