"""
ç»Ÿä¸€APIè·¯ç”± - æ•´åˆä¸¤ä¸ªåº”ç”¨çš„APIæ¥å£
æä¾›RESTful APIæ¥å£ç”¨äºå‰ç«¯è°ƒç”¨å’Œç¬¬ä¸‰æ–¹é›†æˆ
"""
from flask import Blueprint, request, jsonify, send_file
from services.task_service import paper_gather_service
from services.paper_gather_service import paper_data_service
from services.paper_explore_service import PaperService
from services.dify_service import DifyService
from HomeSystem.integrations.paper_analysis.analysis_service import PaperAnalysisService
from HomeSystem.integrations.database import ArxivPaperModel
import logging
import os
import sys
import json
import tempfile
import zipfile
import re
from typing import Dict, Any, Optional
import asyncio
import httpx
import yaml
from pathlib import Path

logger = logging.getLogger(__name__)

# å®šä¹‰é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.join(os.path.dirname(__file__), '..', '..', '..')
sys.path.append(PROJECT_ROOT)

api_bp = Blueprint('api', __name__, url_prefix='/api')

# åˆå§‹åŒ–æœåŠ¡
paper_explore_service = PaperService()
dify_service = DifyService()

# åˆå§‹åŒ–Redisè¿æ¥
try:
    import redis
    from config import REDIS_CONFIG
    redis_client = redis.Redis(
        host=REDIS_CONFIG['host'],
        port=REDIS_CONFIG['port'],
        db=REDIS_CONFIG['db'],
        decode_responses=True
    )
    redis_client.ping()
except Exception as e:
    logger.warning(f"APIæ¨¡å—Redisè¿æ¥å¤±è´¥: {e}")
    redis_client = None


def apply_remote_ocr_config():
    """
    ä»RedisåŠ è½½è¿œç¨‹OCRé…ç½®å¹¶è®¾ç½®ç¯å¢ƒå˜é‡
    
    Returns:
        dict: è¿œç¨‹OCRé…ç½®ä¿¡æ¯
    """
    config = {
        'enable_remote_ocr': False,
        'remote_ocr_endpoint': 'http://localhost:5001',
        'remote_ocr_timeout': 300,
        'remote_ocr_max_pages': 25
    }
    
    if redis_client:
        try:
            # ä»RedisåŠ è½½ç³»ç»Ÿè®¾ç½®
            system_settings_key = "system_settings:global"
            system_settings_data = redis_client.get(system_settings_key)
            
            if system_settings_data:
                system_settings = json.loads(system_settings_data)
                
                # æ›´æ–°é…ç½®
                config['enable_remote_ocr'] = system_settings.get('enable_remote_ocr', False)
                config['remote_ocr_endpoint'] = system_settings.get('remote_ocr_endpoint', 'http://localhost:5001')
                config['remote_ocr_timeout'] = system_settings.get('remote_ocr_timeout', 300)
                config['remote_ocr_max_pages'] = system_settings.get('remote_ocr_max_pages', 25)
                
                # å¦‚æœå¯ç”¨äº†è¿œç¨‹OCRï¼Œè®¾ç½®ç¯å¢ƒå˜é‡
                if config['enable_remote_ocr']:
                    import os
                    os.environ['REMOTE_OCR_ENDPOINT'] = config['remote_ocr_endpoint']
                    os.environ['REMOTE_OCR_TIMEOUT'] = str(config['remote_ocr_timeout'])
                    os.environ['REMOTE_OCR_MAX_PAGES'] = str(config['remote_ocr_max_pages'])
                    logger.info(f"ğŸŒ APIå·²è®¾ç½®è¿œç¨‹OCRç¯å¢ƒå˜é‡: {config['remote_ocr_endpoint']} (è¶…æ—¶: {config['remote_ocr_timeout']}ç§’, æœ€å¤§é¡µæ•°: {config['remote_ocr_max_pages']})")
                else:
                    logger.debug("ğŸ” APIä½¿ç”¨æœ¬åœ°OCR (è¿œç¨‹OCRæœªå¯ç”¨)")
            else:
                logger.debug("æœªæ‰¾åˆ°ç³»ç»Ÿè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤OCRé…ç½®")
                
        except Exception as e:
            logger.warning(f"åŠ è½½è¿œç¨‹OCRé…ç½®å¤±è´¥: {e}")
    else:
        logger.debug("Redisæœªè¿æ¥ï¼Œä½¿ç”¨é»˜è®¤OCRé…ç½®")
    
    return config


# åˆå§‹åŒ–åˆ†ææœåŠ¡
paper_analysis_service = PaperAnalysisService()

# åˆ†ææœåŠ¡é€‚é…å™¨ç±» - æ¡¥æ¥PaperAnalysisServiceå’ŒWeb APIæ¥å£
class AnalysisServiceAdapter:
    """Web APIåˆ†ææœåŠ¡é€‚é…å™¨"""
    
    def __init__(self, paper_service: PaperService, redis_client=None):
        self.paper_service = paper_service
        self.redis_client = redis_client
        self.analysis_threads = {}  # å­˜å‚¨æ­£åœ¨è¿›è¡Œçš„åˆ†æçº¿ç¨‹
        
        # é»˜è®¤é…ç½®
        self.default_config = {
            'analysis_model': 'deepseek.DeepSeek_V3',
            'vision_model': 'ollama.Qwen2_5_VL_7B', 
            'timeout': 1800  # å¢åŠ é»˜è®¤è¶…æ—¶ä¸º30åˆ†é’Ÿ
        }
        
        # Redisé”®å‰ç¼€
        self.REDIS_PREFIX = "deep_analysis"
        self.STATUS_KEY_PREFIX = f"{self.REDIS_PREFIX}:status"
        self.PROCESS_KEY_PREFIX = f"{self.REDIS_PREFIX}:process"
    
    def load_config(self) -> Dict[str, Any]:
        """ä»RedisåŠ è½½é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨æ–°çš„ç³»ç»Ÿè®¾ç½®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨æ—§é…ç½®å’Œé»˜è®¤é…ç½®"""
        config = self.default_config.copy()
        
        if self.redis_client:
            try:
                # ä¼˜å…ˆå°è¯•åŠ è½½æ–°çš„ç³»ç»Ÿè®¾ç½®
                system_config_key = "system_settings:global"
                saved_system_config = self.redis_client.get(system_config_key)
                if saved_system_config:
                    import json
                    system_data = json.loads(saved_system_config)
                    
                    # ä»ç³»ç»Ÿè®¾ç½®ä¸­æå–æ·±åº¦åˆ†æç›¸å…³é…ç½®
                    analysis_config = {}
                    
                    # æ¨¡å‹é…ç½®æ˜ å°„
                    if system_data.get('deep_analysis_model'):
                        analysis_config['analysis_model'] = system_data['deep_analysis_model']
                    elif system_data.get('llm_model_name'):
                        analysis_config['analysis_model'] = system_data['llm_model_name']
                    
                    if system_data.get('vision_model'):
                        analysis_config['vision_model'] = system_data['vision_model']
                    
                    if system_data.get('video_analysis_model'):
                        analysis_config['video_analysis_model'] = system_data['video_analysis_model']
                    
                    if system_data.get('analysis_timeout'):
                        analysis_config['timeout'] = system_data['analysis_timeout']
                    
                    # æ·±åº¦åˆ†æç›¸å…³é…ç½®
                    if 'enable_deep_analysis' in system_data:
                        analysis_config['enable_deep_analysis'] = system_data['enable_deep_analysis']
                    if 'enable_video_analysis' in system_data:
                        analysis_config['enable_video_analysis'] = system_data['enable_video_analysis']
                    if 'deep_analysis_threshold' in system_data:
                        analysis_config['deep_analysis_threshold'] = system_data['deep_analysis_threshold']
                    if 'ocr_char_limit_for_analysis' in system_data:
                        analysis_config['ocr_char_limit_for_analysis'] = system_data['ocr_char_limit_for_analysis']
                    if 'relevance_threshold' in system_data:
                        analysis_config['relevance_threshold'] = system_data['relevance_threshold']
                    
                    # è¿œç¨‹OCRé…ç½®
                    if 'enable_remote_ocr' in system_data:
                        analysis_config['enable_remote_ocr'] = system_data['enable_remote_ocr']
                    if 'remote_ocr_endpoint' in system_data:
                        analysis_config['remote_ocr_endpoint'] = system_data['remote_ocr_endpoint']
                    if 'remote_ocr_timeout' in system_data:
                        analysis_config['remote_ocr_timeout'] = system_data['remote_ocr_timeout']
                    
                    config.update(analysis_config)
                    logger.info(f"ä»ç³»ç»Ÿè®¾ç½®åŠ è½½æ·±åº¦åˆ†æé…ç½®: {analysis_config}")
                
                else:
                    # å¦‚æœç³»ç»Ÿè®¾ç½®ä¸å­˜åœ¨ï¼Œå°è¯•åŠ è½½æ—§çš„åˆ†æé…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
                    old_config_key = "analysis_config:global"
                    saved_old_config = self.redis_client.get(old_config_key)
                    if saved_old_config:
                        import json
                        old_data = json.loads(saved_old_config)
                        config.update(old_data)
                        logger.info(f"ä»æ—§é…ç½®åŠ è½½æ·±åº¦åˆ†æé…ç½®: {old_data}")
                
            except Exception as e:
                logger.warning(f"ä»RedisåŠ è½½é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return config
    
    def start_analysis(self, arxiv_id: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """å¯åŠ¨è®ºæ–‡æ·±åº¦åˆ†æ"""
        try:
            import threading
            from pathlib import Path
            
            logger.info(f"ğŸš€ å¼€å§‹å¯åŠ¨æ·±åº¦åˆ†æ - ArXiv ID: {arxiv_id}")
            
            # æ£€æŸ¥è®ºæ–‡æ˜¯å¦å­˜åœ¨
            paper = self.paper_service.get_paper_detail(arxiv_id)
            if not paper:
                logger.error(f"âŒ è®ºæ–‡ä¸å­˜åœ¨: {arxiv_id}")
                return {
                    'success': False,
                    'error': f'è®ºæ–‡ {arxiv_id} ä¸å­˜åœ¨'
                }
            
            # æ£€æŸ¥æ˜¯å¦å·²åœ¨åˆ†æä¸­ï¼ˆåŒ…æ‹¬Redisä¸­çš„æ´»è·ƒè¿›ç¨‹è®°å½•ï¼‰
            if self._is_analysis_running(arxiv_id):
                logger.warning(f"âš ï¸ è®ºæ–‡å·²åœ¨åˆ†æä¸­: {arxiv_id}")
                return {
                    'success': False,
                    'error': 'è¯¥è®ºæ–‡æ­£åœ¨åˆ†æä¸­ï¼Œè¯·ç¨å'
                }
            
            # æ›´æ–°åˆ†æçŠ¶æ€ä¸ºå¤„ç†ä¸­
            self.paper_service.update_analysis_status(arxiv_id, 'processing')
            
            # åŠ è½½å½“å‰é…ç½®
            current_config = self.load_config()
            analysis_config = {**current_config, **(config or {})}
            
            # åœ¨Redisä¸­è®°å½•åˆ†æè¿›ç¨‹ä¿¡æ¯
            self._record_analysis_start(arxiv_id, analysis_config)
            
            # åˆ›å»ºå¹¶å¯åŠ¨åˆ†æçº¿ç¨‹
            thread = threading.Thread(
                target=self._run_analysis,
                args=(arxiv_id, paper, analysis_config),
                daemon=True
            )
            thread.start()
            self.analysis_threads[arxiv_id] = thread
            
            logger.info(f"Started deep analysis for paper {arxiv_id}")
            
            return {
                'success': True,
                'message': 'æ·±åº¦åˆ†æå·²å¯åŠ¨',
                'status': 'processing'
            }
            
        except Exception as e:
            logger.error(f"Failed to start analysis for {arxiv_id}: {e}")
            try:
                self.paper_service.update_analysis_status(arxiv_id, 'failed')
                self._cleanup_analysis_record(arxiv_id)
            except:
                pass
            
            return {
                'success': False,
                'error': f'å¯åŠ¨åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _run_analysis(self, arxiv_id: str, paper: Dict[str, Any], config: Dict[str, Any]):
        """æ‰§è¡Œè®ºæ–‡åˆ†æï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        try:
            from pathlib import Path
            import re
            
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ·±åº¦åˆ†æ - ArXiv ID: {arxiv_id}")
            
            # åˆ›å»ºåˆ†ææœåŠ¡å®ä¾‹
            analysis_service = PaperAnalysisService(default_config=config)
            
            # è®¡ç®—è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
            # For Docker: /app/routes/api.py -> /app (2 parents) -> /app/data
            # For local: routes/api.py -> project root (4 parents) -> project_root/data
            project_root = Path(__file__).parent.parent
            if not (project_root / "data").exists():
                project_root = Path(__file__).parent.parent.parent.parent
            paper_folder_path = str(project_root / "data" / "paper_analyze" / arxiv_id)
            
            # å‡†å¤‡è®ºæ–‡æ•°æ®ï¼ˆç”¨äºPDFä¸‹è½½ï¼‰
            paper_data = {
                'title': paper.get('title', ''),
                'link': f"https://arxiv.org/abs/{arxiv_id}",
                'snippet': paper.get('abstract', ''),
                'categories': paper.get('categories', ''),
                'arxiv_id': arxiv_id
            }
            
            # æ‰§è¡Œå®Œæ•´çš„æ·±åº¦åˆ†ææµç¨‹
            result = analysis_service.perform_deep_analysis(
                arxiv_id=arxiv_id,
                paper_folder_path=paper_folder_path,
                config=config,
                paper_data=paper_data
            )
            
            if result['success']:
                # ä½¿ç”¨Webåº”ç”¨ç‰¹æœ‰çš„å›¾ç‰‡è·¯å¾„å¤„ç†
                processed_content = self._process_image_paths(
                    result['analysis_result'], 
                    arxiv_id
                )
                
                # ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“
                self.paper_service.save_analysis_result(arxiv_id, processed_content)
                
                # é‡æ–°ä¿å­˜å¤„ç†åçš„å†…å®¹åˆ°æ–‡ä»¶
                with open(result['analysis_file_path'], 'w', encoding='utf-8') as f:
                    f.write(processed_content)
                
                logger.info(f"Analysis completed for {arxiv_id}, saved {len(processed_content)} characters")
                self.paper_service.update_analysis_status(arxiv_id, 'completed')
            else:
                logger.error(f"âŒ æ·±åº¦åˆ†æå¤±è´¥: {arxiv_id}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                self.paper_service.update_analysis_status(arxiv_id, 'failed')
            
        except Exception as e:
            logger.error(f"ğŸ’¥ åˆ†æè¿‡ç¨‹å¤±è´¥ {arxiv_id}: {e}")
            try:
                self.paper_service.update_analysis_status(arxiv_id, 'failed')
            except:
                pass
        finally:
            # æ¸…ç†åˆ†æè®°å½•ï¼ˆåŒ…æ‹¬Redisè®°å½•å’Œå†…å­˜çº¿ç¨‹å¼•ç”¨ï¼‰
            self._cleanup_analysis_record(arxiv_id)
    
    def _process_image_paths(self, content: str, arxiv_id: str) -> str:
        """å¤„ç†Markdownå†…å®¹ä¸­çš„å›¾ç‰‡è·¯å¾„ï¼Œå°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºå¯è®¿é—®çš„URLè·¯å¾„"""
        try:
            logger.info(f"ğŸ–¼ï¸ Starting image path processing for {arxiv_id}")
            
            # åŒ¹é… ![alt](imgs/filename) æ ¼å¼
            img_pattern = r'!\[([^\]]*)\]\((imgs/[^)]+)\)'
            
            def replace_image_path(match):
                alt_text = match.group(1)
                relative_path = match.group(2)
                filename = relative_path.replace('imgs/', '')
                new_path = f"/paper/{arxiv_id}/imgs/{filename}"
                logger.debug(f"  ğŸ“¸ Converting: {relative_path} â†’ {new_path}")
                return f"![{alt_text}]({new_path})"
            
            # ç»Ÿè®¡å’Œæ›¿æ¢
            original_matches = re.findall(img_pattern, content)
            logger.info(f"  ğŸ“Š Found {len(original_matches)} image references for {arxiv_id}")
            
            processed_content = re.sub(img_pattern, replace_image_path, content)
            
            # éªŒè¯å¤„ç†ç»“æœ
            processed_matches = re.findall(r'!\[([^\]]*)\]\((/paper/[^)]+)\)', processed_content)
            logger.info(f"  âœ… Successfully processed {len(processed_matches)} image paths for {arxiv_id}")
            
            return processed_content
            
        except Exception as e:
            logger.error(f"âŒ Failed to process image paths for {arxiv_id}: {e}")
            return content
    
    def get_analysis_status(self, arxiv_id: str) -> Dict[str, Any]:
        """è·å–åˆ†æçŠ¶æ€"""
        try:
            status_info = self.paper_service.get_analysis_status(arxiv_id)
            
            if not status_info:
                return {
                    'success': True,
                    'status': 'not_started',
                    'message': 'å°šæœªå¼€å§‹åˆ†æ'
                }
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„çº¿ç¨‹
            if arxiv_id in self.analysis_threads:
                thread = self.analysis_threads[arxiv_id]
                if thread.is_alive():
                    status_info['is_running'] = True
                else:
                    del self.analysis_threads[arxiv_id]
                    status_info['is_running'] = False
            else:
                status_info['is_running'] = False
            
            return {
                'success': True,
                **status_info
            }
            
        except Exception as e:
            logger.error(f"Failed to get analysis status for {arxiv_id}: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_analysis_result(self, arxiv_id: str) -> Dict[str, Any]:
        """è·å–åˆ†æç»“æœ"""
        try:
            result = self.paper_service.get_analysis_result(arxiv_id)
            
            if not result:
                return {
                    'success': False,
                    'error': 'åˆ†æç»“æœä¸å­˜åœ¨'
                }
            
            return {
                'success': True,
                **result
            }
            
        except Exception as e:
            logger.error(f"Failed to get analysis result for {arxiv_id}: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _is_analysis_running(self, arxiv_id: str) -> bool:
        """æ£€æŸ¥åˆ†ææ˜¯å¦æ­£åœ¨è¿è¡Œï¼ˆåŒæ—¶æ£€æŸ¥å†…å­˜çº¿ç¨‹å’ŒRedisè®°å½•ï¼‰"""
        # æ£€æŸ¥å†…å­˜ä¸­çš„çº¿ç¨‹
        if arxiv_id in self.analysis_threads:
            thread = self.analysis_threads[arxiv_id]
            if thread.is_alive():
                return True
            else:
                del self.analysis_threads[arxiv_id]
        
        # æ£€æŸ¥Redisä¸­çš„è¿›ç¨‹è®°å½•
        if self.redis_client:
            try:
                process_key = f"{self.PROCESS_KEY_PREFIX}:{arxiv_id}"
                process_info = self.redis_client.get(process_key)
                if process_info:
                    import json
                    import time
                    process_data = json.loads(process_info)
                    start_time = process_data.get('start_time', 0)
                    timeout = process_data.get('timeout', self.default_config['timeout'])
                    
                    # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                    if time.time() - start_time > timeout:
                        logger.warning(f"åˆ†æè¿›ç¨‹è¶…æ—¶: {arxiv_id}, æ¸…ç†è¶…æ—¶è®°å½•")
                        self._cleanup_analysis_record(arxiv_id)
                        self.paper_service.update_analysis_status(arxiv_id, 'failed')
                        return False
                    
                    return True
            except Exception as e:
                logger.warning(f"æ£€æŸ¥Redisè¿›ç¨‹è®°å½•å¤±è´¥: {e}")
        
        return False
    
    def _record_analysis_start(self, arxiv_id: str, config: Dict[str, Any]):
        """åœ¨Redisä¸­è®°å½•åˆ†æå¼€å§‹ä¿¡æ¯"""
        if self.redis_client:
            try:
                import json
                import time
                import os
                
                process_key = f"{self.PROCESS_KEY_PREFIX}:{arxiv_id}"
                process_data = {
                    'start_time': time.time(),
                    'timeout': config.get('timeout', self.default_config['timeout']),
                    'config': config,
                    'pid': os.getpid(),
                    'status': 'processing'
                }
                
                # è®¾ç½®è¿‡æœŸæ—¶é—´ä¸ºè¶…æ—¶æ—¶é—´çš„2å€ï¼Œç¡®ä¿è®°å½•ä¸ä¼šæ°¸ä¹…å­˜åœ¨
                expire_time = config.get('timeout', self.default_config['timeout']) * 2
                self.redis_client.setex(
                    process_key, 
                    expire_time, 
                    json.dumps(process_data)
                )
                
                logger.info(f"å·²åœ¨Redisä¸­è®°å½•åˆ†æè¿›ç¨‹ä¿¡æ¯: {arxiv_id}")
            except Exception as e:
                logger.warning(f"è®°å½•åˆ†æå¼€å§‹ä¿¡æ¯å¤±è´¥: {e}")
    
    def _cleanup_analysis_record(self, arxiv_id: str):
        """æ¸…ç†Redisä¸­çš„åˆ†æè®°å½•"""
        if self.redis_client:
            try:
                process_key = f"{self.PROCESS_KEY_PREFIX}:{arxiv_id}"
                self.redis_client.delete(process_key)
                logger.info(f"å·²æ¸…ç†åˆ†æè¿›ç¨‹è®°å½•: {arxiv_id}")
            except Exception as e:
                logger.warning(f"æ¸…ç†åˆ†æè®°å½•å¤±è´¥: {e}")
        
        # åŒæ—¶æ¸…ç†å†…å­˜ä¸­çš„çº¿ç¨‹å¼•ç”¨
        if arxiv_id in self.analysis_threads:
            del self.analysis_threads[arxiv_id]
    
    def cancel_analysis(self, arxiv_id: str) -> Dict[str, Any]:
        """å–æ¶ˆæ­£åœ¨è¿›è¡Œçš„åˆ†æ"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„åˆ†æ
            if not self._is_analysis_running(arxiv_id):
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„åˆ†æä»»åŠ¡'
                }
            
            # æ›´æ–°çŠ¶æ€ä¸ºå·²å–æ¶ˆ
            self.paper_service.update_analysis_status(arxiv_id, 'cancelled')
            
            # æ¸…ç†Redisè®°å½•
            self._cleanup_analysis_record(arxiv_id)
            
            # æ³¨æ„ï¼šæˆ‘ä»¬ä¸èƒ½å¼ºåˆ¶ç»ˆæ­¢çº¿ç¨‹ï¼Œä½†å¯ä»¥é€šè¿‡çŠ¶æ€æ ‡è®°è®©çº¿ç¨‹è‡ªè¡Œé€€å‡º
            logger.info(f"å·²å–æ¶ˆåˆ†æä»»åŠ¡: {arxiv_id}")
            
            return {
                'success': True,
                'message': 'åˆ†æä»»åŠ¡å·²å–æ¶ˆ'
            }
            
        except Exception as e:
            logger.error(f"å–æ¶ˆåˆ†æä»»åŠ¡å¤±è´¥: {e}")
            return {
                'success': False,
                'error': f'å–æ¶ˆå¤±è´¥: {str(e)}'
            }
    
    def reset_analysis_status(self, arxiv_id: str) -> Dict[str, Any]:
        """é‡ç½®åˆ†æçŠ¶æ€ï¼ˆç®¡ç†åŠŸèƒ½ï¼‰"""
        try:
            # æ¸…ç†æ‰€æœ‰ç›¸å…³è®°å½•
            self._cleanup_analysis_record(arxiv_id)
            
            # é‡ç½®æ•°æ®åº“çŠ¶æ€
            self.paper_service.update_analysis_status(arxiv_id, 'pending')
            
            logger.info(f"å·²é‡ç½®åˆ†æçŠ¶æ€: {arxiv_id}")
            
            return {
                'success': True,
                'message': 'åˆ†æçŠ¶æ€å·²é‡ç½®'
            }
            
        except Exception as e:
            logger.error(f"é‡ç½®åˆ†æçŠ¶æ€å¤±è´¥: {e}")
            return {
                'success': False,
                'error': f'é‡ç½®å¤±è´¥: {str(e)}'
            }

# åˆ›å»ºé€‚é…å™¨å®ä¾‹
analysis_service = AnalysisServiceAdapter(paper_explore_service, redis_client)


# === è®ºæ–‡æ”¶é›†ç›¸å…³API (æ¥è‡ªPaperGather) ===

@api_bp.route('/collect/models')
def get_available_models():
    """è·å–å¯ç”¨çš„LLMæ¨¡å‹"""
    try:
        models = paper_gather_service.get_available_models()
        return jsonify({
            'success': True,
            'models': models
        })
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/collect/search_modes')
def get_search_modes():
    """è·å–å¯ç”¨çš„æœç´¢æ¨¡å¼"""
    try:
        modes = paper_gather_service.get_available_search_modes()
        return jsonify({
            'success': True,
            'search_modes': modes
        })
    except Exception as e:
        logger.error(f"è·å–æœç´¢æ¨¡å¼å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/collect/task/<task_id>/status')
def get_task_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        result = paper_gather_service.get_task_result(task_id)
        if not result:
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡ä¸å­˜åœ¨'
            }), 404
        
        return jsonify({
            'success': True,
            'task_result': result
        })
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/collect/task/<task_id>/stop', methods=['POST'])
def stop_task(task_id):
    """åœæ­¢ä»»åŠ¡"""
    try:
        success = paper_gather_service.stop_task(task_id)
        return jsonify({
            'success': success,
            'message': 'ä»»åŠ¡åœæ­¢æˆåŠŸ' if success else 'ä»»åŠ¡åœæ­¢å¤±è´¥'
        })
    except Exception as e:
        logger.error(f"åœæ­¢ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# === è®ºæ–‡æµè§ˆç›¸å…³API (æ¥è‡ªExplorePaperData) ===

@api_bp.route('/explore/search')
def api_search():
    """æœç´¢è®ºæ–‡"""
    try:
        query = request.args.get('q', '').strip()
        task_name = request.args.get('task_name', '').strip()
        task_id = request.args.get('task_id', '').strip()
        page = int(request.args.get('page', 1))
        per_page = min(int(request.args.get('per_page', 10)), 50)
        
        papers, total = paper_explore_service.search_papers(
            query=query,
            task_name=task_name,
            task_id=task_id,
            page=page, 
            per_page=per_page
        )
        
        return jsonify({
            'success': True,
            'data': papers,
            'total': total,
            'page': page,
            'per_page': per_page,
            'total_pages': (total + per_page - 1) // per_page
        })
    
    except Exception as e:
        logger.error(f"APIæœç´¢å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/explore/stats')
def api_stats():
    """è·å–ç»Ÿè®¡æ•°æ®"""
    try:
        stats = paper_explore_service.get_overview_stats()
        return jsonify({'success': True, 'data': stats})
    
    except Exception as e:
        logger.error(f"APIç»Ÿè®¡æ•°æ®è·å–å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/explore/tasks')
def api_tasks():
    """è·å–å¯ç”¨ä»»åŠ¡åˆ—è¡¨"""
    try:
        tasks_data = paper_explore_service.get_available_tasks()
        # å°†å¤æ‚çš„æ•°æ®ç»“æ„è½¬æ¢ä¸ºç®€å•çš„ä»»åŠ¡æ•°ç»„ï¼Œä¾›JavaScriptè¿­ä»£ä½¿ç”¨
        all_tasks = []
        
        # æ·»åŠ åŸºäºä»»åŠ¡åç§°çš„ä»»åŠ¡
        for task in tasks_data.get('task_names', []):
            all_tasks.append({
                'task_name': task['task_name'],
                'task_id': '',  # task_namesä¸­æ²¡æœ‰task_id
                'paper_count': task['paper_count']
            })
        
        # æ·»åŠ åŸºäºä»»åŠ¡IDçš„ä»»åŠ¡ï¼ˆé¿å…é‡å¤ï¼‰
        task_name_set = {task['task_name'] for task in all_tasks}
        for task in tasks_data.get('task_ids', []):
            if task['task_name'] not in task_name_set:
                all_tasks.append({
                    'task_name': task['task_name'],
                    'task_id': task['task_id'],
                    'paper_count': task['paper_count'],
                    'first_created': task.get('first_created'),
                    'last_created': task.get('last_created')
                })
        
        return jsonify({'success': True, 'data': {'tasks': all_tasks}})
    
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/explore/update_task_name', methods=['POST'])
def api_update_task_name():
    """æ›´æ–°ä»»åŠ¡åç§°"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_id = data.get('arxiv_id')
        new_task_name = (data.get('new_task_name') or '').strip()
        
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
            
        success = paper_explore_service.update_task_name(arxiv_id, new_task_name)
        
        if success:
            return jsonify({'success': True, 'message': 'ä»»åŠ¡åç§°æ›´æ–°æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'æ›´æ–°å¤±è´¥ï¼Œè®ºæ–‡ä¸å­˜åœ¨'}), 404
    
    except Exception as e:
        logger.error(f"æ›´æ–°ä»»åŠ¡åç§°å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/delete_paper', methods=['POST'])
def api_delete_paper_post():
    """åˆ é™¤å•ä¸ªè®ºæ–‡ (POSTè¯·æ±‚ï¼Œä»JSON bodyè·å–arxiv_id)"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'}), 400
        
        arxiv_id = data.get('arxiv_id')
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
        
        success = paper_explore_service.delete_paper(arxiv_id)
        
        if success:
            return jsonify({'success': True, 'message': 'è®ºæ–‡åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'åˆ é™¤å¤±è´¥ï¼Œè®ºæ–‡ä¸å­˜åœ¨'}), 404
    
    except Exception as e:
        logger.error(f"åˆ é™¤è®ºæ–‡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/explore/delete_paper/<arxiv_id>', methods=['DELETE'])
def api_delete_paper(arxiv_id):
    """åˆ é™¤å•ä¸ªè®ºæ–‡"""
    try:
        success = paper_explore_service.delete_paper(arxiv_id)
        
        if success:
            return jsonify({'success': True, 'message': 'è®ºæ–‡åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'åˆ é™¤å¤±è´¥ï¼Œè®ºæ–‡ä¸å­˜åœ¨'}), 404
    
    except Exception as e:
        logger.error(f"åˆ é™¤è®ºæ–‡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# === æ·±åº¦åˆ†æç›¸å…³API ===

@api_bp.route('/analysis/paper/<arxiv_id>/analyze', methods=['POST'])
def api_start_analysis(arxiv_id):
    """å¯åŠ¨æ·±åº¦è®ºæ–‡åˆ†æ"""
    try:
        logger.info(f"ğŸ¯ æ”¶åˆ°æ·±åº¦åˆ†æè¯·æ±‚ - ArXiv ID: {arxiv_id}")
        
        # è·å–é…ç½®å‚æ•°
        data = request.get_json() if request.is_json else {}
        config = data.get('config', {})
        
        # å¯åŠ¨åˆ†æ
        result = analysis_service.start_analysis(arxiv_id, config)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify(result), 400
            
    except Exception as e:
        logger.error(f"å¯åŠ¨æ·±åº¦åˆ†æå¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"å¯åŠ¨åˆ†æå¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/analysis/paper/<arxiv_id>/status')
def api_analysis_status(arxiv_id):
    """æŸ¥è¯¢åˆ†æçŠ¶æ€"""
    try:
        result = analysis_service.get_analysis_status(arxiv_id)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify(result), 404
            
    except Exception as e:
        logger.error(f"è·å–åˆ†æçŠ¶æ€å¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/analysis/paper/<arxiv_id>/result')
def api_analysis_result(arxiv_id):
    """è·å–åˆ†æç»“æœ"""
    try:
        result = analysis_service.get_analysis_result(arxiv_id)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify(result), 404
            
    except Exception as e:
        logger.error(f"è·å–åˆ†æç»“æœå¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"è·å–ç»“æœå¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/analysis/config', methods=['GET'])
def get_analysis_config():
    """è·å–æ·±åº¦åˆ†æé…ç½®å’Œå¯ç”¨æ¨¡å‹"""
    try:
        # å¯¼å…¥LLMFactory
        from HomeSystem.graph.llm_factory import LLMFactory
        factory = LLMFactory()
        
        # ä»Redisè·å–å½“å‰é…ç½®
        config_key = "analysis_config:global"
        current_config = {
            "analysis_model": "deepseek.DeepSeek_V3",
            "vision_model": "ollama.Qwen2_5_VL_7B",
            "timeout": 600
        }
        
        if redis_client:
            try:
                saved_config = redis_client.get(config_key)
                if saved_config:
                    current_config.update(json.loads(saved_config))
            except Exception as e:
                logger.warning(f"è¯»å–Redisé…ç½®å¤±è´¥: {e}")
        
        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        available_models = factory.get_available_llm_models()
        vision_models = factory.get_available_vision_models()
        
        return jsonify({
            'success': True,
            'data': {
                'current_config': current_config,
                'available_models': {
                    'analysis_models': available_models,
                    'vision_models': vision_models
                }
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–åˆ†æé…ç½®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"è·å–é…ç½®å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/analysis/config', methods=['POST'])
def save_analysis_config():
    """ä¿å­˜æ·±åº¦åˆ†æé…ç½®"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # éªŒè¯å¿…è¦å­—æ®µ
        analysis_model = data.get('analysis_model')
        vision_model = data.get('vision_model')
        timeout = data.get('timeout', 600)
        
        if not analysis_model or not vision_model:
            return jsonify({
                'success': False,
                'error': 'åˆ†ææ¨¡å‹å’Œè§†è§‰æ¨¡å‹ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # æ„å»ºé…ç½®
        config = {
            'analysis_model': analysis_model,
            'vision_model': vision_model,
            'timeout': timeout
        }
        
        # ä¿å­˜åˆ°Redis
        config_key = "analysis_config:global"
        if redis_client:
            try:
                redis_client.set(config_key, json.dumps(config))
                logger.info(f"é…ç½®å·²ä¿å­˜åˆ°Redis: {config}")
            except Exception as e:
                logger.error(f"ä¿å­˜é…ç½®åˆ°Rediså¤±è´¥: {e}")
                return jsonify({
                    'success': False,
                    'error': f'ä¿å­˜é…ç½®å¤±è´¥: {str(e)}'
                }), 500
        
        return jsonify({
            'success': True,
            'message': 'é…ç½®ä¿å­˜æˆåŠŸ',
            'config': config
        })
        
    except Exception as e:
        logger.error(f"ä¿å­˜åˆ†æé…ç½®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"ä¿å­˜é…ç½®å¤±è´¥: {str(e)}"
        }), 500


# === æ·±åº¦åˆ†æç®¡ç†API ===

@api_bp.route('/analysis/paper/<arxiv_id>/cancel', methods=['POST'])
def api_cancel_analysis(arxiv_id):
    """å–æ¶ˆæ­£åœ¨è¿›è¡Œçš„æ·±åº¦åˆ†æ"""
    try:
        result = analysis_service.cancel_analysis(arxiv_id)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify(result), 400
            
    except Exception as e:
        logger.error(f"å–æ¶ˆåˆ†æå¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"å–æ¶ˆåˆ†æå¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/analysis/paper/<arxiv_id>/reset', methods=['POST'])
def api_reset_analysis_status(arxiv_id):
    """é‡ç½®æ·±åº¦åˆ†æçŠ¶æ€"""
    try:
        result = analysis_service.reset_analysis_status(arxiv_id)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify(result), 400
            
    except Exception as e:
        logger.error(f"é‡ç½®åˆ†æçŠ¶æ€å¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"é‡ç½®çŠ¶æ€å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/analysis/recover', methods=['POST'])
def api_recover_interrupted_analysis():
    """æ¢å¤è¢«ä¸­æ–­çš„æ·±åº¦åˆ†æä»»åŠ¡"""
    try:
        result = paper_explore_service.recover_interrupted_analysis()
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"æ¢å¤ä¸­æ–­åˆ†æå¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"æ¢å¤å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/analysis/reset_stuck', methods=['POST'])
def api_reset_stuck_analysis():
    """é‡ç½®å¡ä½çš„æ·±åº¦åˆ†æä»»åŠ¡"""
    try:
        data = request.get_json() if request.is_json else {}
        max_hours = data.get('max_hours', 2)
        
        # éªŒè¯å‚æ•°
        if max_hours <= 0 or max_hours > 24:
            return jsonify({
                'success': False,
                'error': 'æœ€å¤§å°æ—¶æ•°å¿…é¡»åœ¨1-24ä¹‹é—´'
            }), 400
        
        result = paper_explore_service.reset_stuck_analysis(max_hours)
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"é‡ç½®å¡ä½åˆ†æå¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"é‡ç½®å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/analysis/batch_reset', methods=['POST'])
def api_batch_reset_analysis_status():
    """æ‰¹é‡é‡ç½®æ·±åº¦åˆ†æçŠ¶æ€"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
            }), 400
        
        arxiv_ids = data.get('arxiv_ids', [])
        status = data.get('status', 'pending')
        
        if not arxiv_ids:
            return jsonify({
                'success': False,
                'error': 'è®ºæ–‡IDåˆ—è¡¨ä¸èƒ½ä¸ºç©º'
            }), 400
        
        result = paper_explore_service.batch_reset_analysis_status(arxiv_ids, status)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify(result), 400
            
    except Exception as e:
        logger.error(f"æ‰¹é‡é‡ç½®åˆ†æçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"æ‰¹é‡é‡ç½®å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/analysis/statistics')
def api_analysis_statistics():
    """è·å–æ·±åº¦åˆ†æç»Ÿè®¡ä¿¡æ¯"""
    try:
        result = paper_explore_service.get_analysis_statistics()
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify(result), 500
            
    except Exception as e:
        logger.error(f"è·å–åˆ†æç»Ÿè®¡å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/settings/save', methods=['POST'])
def save_settings():
    """ä¿å­˜ç³»ç»Ÿè®¾ç½®ï¼ˆæ¨¡å‹è®¾ç½®å’Œæ·±åº¦åˆ†æè®¾ç½®ï¼‰"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # éªŒè¯å¿…è¦å­—æ®µ
        llm_model_name = data.get('llm_model_name')
        if not llm_model_name:
            return jsonify({
                'success': False,
                'error': 'LLMæ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # æ„å»ºé…ç½®
        config = {
            # LLMé…ç½®
            'llm_model_name': llm_model_name,
            'relevance_threshold': data.get('relevance_threshold', 0.7),
            'abstract_analysis_model': data.get('abstract_analysis_model'),
            'full_paper_analysis_model': data.get('full_paper_analysis_model'),
            'deep_analysis_model': data.get('deep_analysis_model'),
            'vision_model': data.get('vision_model'),
            'video_analysis_model': data.get('video_analysis_model'),
            
            # æ·±åº¦åˆ†æé…ç½®
            'enable_deep_analysis': data.get('enable_deep_analysis', True),
            'enable_video_analysis': data.get('enable_video_analysis', False),
            'deep_analysis_threshold': data.get('deep_analysis_threshold', 0.8),
            'ocr_char_limit_for_analysis': data.get('ocr_char_limit_for_analysis', 10000),
            'analysis_timeout': data.get('analysis_timeout', 600),
            
            # è¿œç¨‹OCRé…ç½®
            'enable_remote_ocr': data.get('enable_remote_ocr', False),
            'remote_ocr_endpoint': data.get('remote_ocr_endpoint', 'http://localhost:5001'),
            'remote_ocr_timeout': data.get('remote_ocr_timeout', 300),
            'remote_ocr_max_pages': data.get('remote_ocr_max_pages', 25)
        }
        
        # ä¿å­˜åˆ°Redis
        config_key = "system_settings:global"
        if redis_client:
            try:
                redis_client.set(config_key, json.dumps(config))
                logger.info(f"ç³»ç»Ÿè®¾ç½®å·²ä¿å­˜åˆ°Redis: {config}")
            except Exception as e:
                logger.error(f"ä¿å­˜ç³»ç»Ÿè®¾ç½®åˆ°Rediså¤±è´¥: {e}")
                return jsonify({
                    'success': False,
                    'error': f'ä¿å­˜è®¾ç½®å¤±è´¥: {str(e)}'
                }), 500
        
        return jsonify({
            'success': True,
            'message': 'è®¾ç½®ä¿å­˜æˆåŠŸ',
            'config': config
        })
        
    except Exception as e:
        logger.error(f"ä¿å­˜ç³»ç»Ÿè®¾ç½®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"ä¿å­˜è®¾ç½®å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/settings/load', methods=['GET'])
def load_settings():
    """åŠ è½½ç³»ç»Ÿè®¾ç½®"""
    try:
        config_key = "system_settings:global"
        config = {}
        
        if redis_client:
            try:
                config_data = redis_client.get(config_key)
                if config_data:
                    config = json.loads(config_data)
                    logger.info(f"ä»RedisåŠ è½½ç³»ç»Ÿè®¾ç½®: {config}")
            except Exception as e:
                logger.error(f"ä»RedisåŠ è½½ç³»ç»Ÿè®¾ç½®å¤±è´¥: {e}")
        
        return jsonify({
            'success': True,
            'config': config
        })
        
    except Exception as e:
        logger.error(f"åŠ è½½ç³»ç»Ÿè®¾ç½®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"åŠ è½½è®¾ç½®å¤±è´¥: {str(e)}"
        }), 500


# === ä»»åŠ¡å†å²ç®¡ç†ç›¸å…³API ===

@api_bp.route('/task/details/<task_id>')
def get_task_details(task_id):
    """è·å–ä»»åŠ¡è¯¦ç»†ä¿¡æ¯ - ç”¨äºå‰ç«¯æ¨¡æ€æ¡†æ˜¾ç¤º"""
    try:
        result = paper_gather_service.get_task_result(task_id)
        if not result:
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡ä¸å­˜åœ¨'
            }), 404
        
        return jsonify({
            'success': True,
            'data': result
        })
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500



@api_bp.route('/task/history')
def get_task_history():
    """è·å–ä»»åŠ¡å†å²åˆ—è¡¨"""
    try:
        page = int(request.args.get('page', 1))
        per_page = min(int(request.args.get('per_page', 50)), 100)
        
        # è·å–ä»»åŠ¡å†å²
        tasks = paper_gather_service.get_task_history(limit=per_page * 5)  # è·å–æ›´å¤šæ•°æ®ç”¨äºåˆ†é¡µ
        
        # è®¡ç®—åˆ†é¡µ
        total = len(tasks)
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        paginated_tasks = tasks[start_idx:end_idx]
        
        return jsonify({
            'success': True,
            'data': {
                'tasks': paginated_tasks,
                'pagination': {
                    'page': page,
                    'per_page': per_page,
                    'total': total,
                    'total_pages': (total + per_page - 1) // per_page
                }
            }
        })
    
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡å†å²å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/task/history/<task_id>', methods=['DELETE'])
def delete_task_history(task_id):
    """åˆ é™¤å†å²ä»»åŠ¡"""
    try:
        success, error_msg = paper_gather_service.delete_task_history(task_id)
        
        if success:
            return jsonify({
                'success': True,
                'message': 'ä»»åŠ¡åˆ é™¤æˆåŠŸ'
            })
        else:
            return jsonify({
                'success': False,
                'error': error_msg or 'åˆ é™¤å¤±è´¥'
            }), 400
    
    except Exception as e:
        logger.error(f"åˆ é™¤ä»»åŠ¡å†å²å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/task/config/<task_id>')
def get_task_config(task_id):
    """è·å–ç‰¹å®šä»»åŠ¡çš„é…ç½®"""
    try:
        # è·å–ä»»åŠ¡é…ç½®
        task_config = paper_gather_service.get_task_config_by_id(task_id)
        
        if not task_config:
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡ä¸å­˜åœ¨'
            }), 404
        
        return jsonify({
            'success': True,
            'data': {
                'task_id': task_id,
                'config': task_config
            }
        })
    
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡é…ç½®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/config/presets')
def get_config_presets():
    """è·å–é…ç½®é¢„è®¾åˆ—è¡¨"""
    try:
        # è¿”å›ç©ºçš„é¢„è®¾åˆ—è¡¨ï¼ˆå¯ä»¥åç»­æ‰©å±•ï¼‰
        return jsonify({
            'success': True,
            'data': {
                'presets': []
            }
        })
    
    except Exception as e:
        logger.error(f"è·å–é…ç½®é¢„è®¾å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/config/presets', methods=['POST'])
def create_config_preset():
    """åˆ›å»ºé…ç½®é¢„è®¾"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # ç›®å‰è¿”å›æˆåŠŸä½†ä¸å®é™…ä¿å­˜ï¼ˆå¯ä»¥åç»­æ‰©å±•ï¼‰
        return jsonify({
            'success': True,
            'message': 'é¢„è®¾åˆ›å»ºæˆåŠŸï¼ˆåŠŸèƒ½æš‚æœªå®Œå…¨å®ç°ï¼‰',
            'preset_id': 'temp_id'
        })
    
    except Exception as e:
        logger.error(f"åˆ›å»ºé…ç½®é¢„è®¾å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/config/presets/<preset_id>', methods=['DELETE'])
def delete_config_preset(preset_id):
    """åˆ é™¤é…ç½®é¢„è®¾"""
    try:
        # ç›®å‰è¿”å›æˆåŠŸä½†ä¸å®é™…åˆ é™¤ï¼ˆå¯ä»¥åç»­æ‰©å±•ï¼‰
        return jsonify({
            'success': True,
            'message': 'é¢„è®¾åˆ é™¤æˆåŠŸï¼ˆåŠŸèƒ½æš‚æœªå®Œå…¨å®ç°ï¼‰'
        })
    
    except Exception as e:
        logger.error(f"åˆ é™¤é…ç½®é¢„è®¾å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/models')
def get_models():
    """è·å–å¯ç”¨çš„LLMæ¨¡å‹åˆ—è¡¨"""
    try:
        models = paper_gather_service.get_available_models()
        return jsonify({
            'success': True,
            'data': models
        })
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# === ç³»ç»ŸçŠ¶æ€ç›¸å…³API ===

@api_bp.route('/running_tasks')
def get_running_tasks():
    """è·å–è¿è¡Œä¸­ä»»åŠ¡çŠ¶æ€ - ç”¨äºé¦–é¡µå®æ—¶æ›´æ–°"""
    try:
        # è·å–è¿è¡Œä¸­ä»»åŠ¡æ•°é‡å’Œè¯¦æƒ…
        running_count = paper_gather_service.get_running_tasks_count()
        running_details = paper_gather_service.get_running_tasks_detail()
        
        return jsonify({
            'success': True,
            'data': {
                'count': running_count,
                'details': running_details
            }
        })
    except Exception as e:
        logger.error(f"è·å–è¿è¡Œä¸­ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/system_stats')
def get_system_stats():
    """è·å–ç³»ç»Ÿæ ¸å¿ƒç»Ÿè®¡æ•°æ® - ç”¨äºé¦–é¡µå±•ç¤ºå¡ç‰‡"""
    try:
        # è·å–è®ºæ–‡æ”¶é›†ç»Ÿè®¡
        collect_stats = paper_data_service.get_paper_statistics()
        
        # è·å–è¿è¡Œä¸­ä»»åŠ¡æ•°é‡
        running_tasks_count = paper_gather_service.get_running_tasks_count()
        
        # è·å–å®šæ—¶ä»»åŠ¡æ•°é‡
        scheduled_tasks = paper_gather_service.get_scheduled_tasks()
        scheduled_count = len(scheduled_tasks) if scheduled_tasks else 0
        
        # æ„å»ºæ ¸å¿ƒç»Ÿè®¡æ•°æ®
        core_stats = {
            'total_papers': collect_stats.get('total_papers', 0),
            'analyzed_papers': collect_stats.get('analyzed_papers', 0),
            'running_tasks': running_tasks_count,
            'scheduled_tasks': scheduled_count
        }
        
        return jsonify({
            'success': True,
            'data': core_stats
        })
    except Exception as e:
        logger.error(f"è·å–ç³»ç»Ÿç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/tasks/status')
def get_tasks_status():
    """è·å–æ‰€æœ‰æ´»åŠ¨ä»»åŠ¡çš„çŠ¶æ€ - ç”¨äºå‰ç«¯å®šæ—¶åˆ·æ–°"""
    try:
        # è·å–è¿è¡Œä¸­ä»»åŠ¡è¯¦æƒ…
        running_tasks = paper_gather_service.get_running_tasks_detail()
        
        # æ ¼å¼åŒ–ä»»åŠ¡çŠ¶æ€æ•°æ®ä¾›å‰ç«¯ä½¿ç”¨
        status_data = []
        for task in running_tasks:
            status_data.append({
                'task_id': task.get('task_id', ''),
                'status': task.get('status', 'unknown'),
                'progress': task.get('progress', 0.0),
                'start_time': task.get('start_time'),
                'duration': task.get('duration')
            })
        
        return jsonify({
            'success': True,
            'data': status_data
        })
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ========== å…³äºé¡µé¢ç³»ç»ŸçŠ¶æ€ç›¸å…³API ==========

@api_bp.route('/about/system_status')
def get_about_system_status():
    """è·å–å…³äºé¡µé¢çš„ç³»ç»ŸçŠ¶æ€ä¿¡æ¯"""
    try:
        import os
        import sys
        import redis
        import requests
        from datetime import datetime
        
        status_info = {
            'timestamp': datetime.now().isoformat(),
            'database': {'postgresql': False, 'redis': False},
            'llm_services': {'available_count': 0, 'total_providers': 0},
            'external_services': {
                'siyuan': False,
                'dify': False, 
                'ollama': False
            }
        }
        
        # æ£€æµ‹æ•°æ®åº“è¿æ¥çŠ¶æ€
        try:
            import psycopg2
            postgres_config = {
                'host': os.getenv('DB_HOST', 'localhost'),
                'port': int(os.getenv('DB_PORT', 5432)),
                'database': os.getenv('DB_NAME', 'homesystem'),
                'user': os.getenv('DB_USER', 'homesystem'),
                'password': os.getenv('DB_PASSWORD', 'homesystem123'),
            }
            conn = psycopg2.connect(**postgres_config)
            conn.close()
            status_info['database']['postgresql'] = True
        except Exception as e:
            logger.debug(f"PostgreSQLè¿æ¥æ£€æµ‹å¤±è´¥: {e}")
        
        # æ£€æµ‹Redisè¿æ¥çŠ¶æ€
        try:
            redis_host = os.getenv('REDIS_HOST', 'localhost')
            redis_port = int(os.getenv('REDIS_PORT', 6379))
            r = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
            r.ping()
            status_info['database']['redis'] = True
        except Exception as e:
            logger.debug(f"Redisè¿æ¥æ£€æµ‹å¤±è´¥: {e}")
        
        # æ£€æµ‹LLMæœåŠ¡çŠ¶æ€
        try:
            # åŠ¨æ€æ£€æµ‹å¯ç”¨çš„API Keyå’Œå‡†ç¡®çš„æ¨¡å‹æ•°é‡
            llm_providers = {
                'deepseek': (os.getenv('DEEPSEEK_API_KEY'), 2),  # DeepSeek V3 + R1
                'siliconflow': (os.getenv('SILICONFLOW_API_KEY'), 6),  # 6ä¸ªèŠå¤©æ¨¡å‹
                'volcano': (os.getenv('VOLCANO_API_KEY'), 3),  # è±†åŒ…3ä¸ªç‰ˆæœ¬
                'moonshot': (os.getenv('MOONSHOT_API_KEY'), 2),  # Kimi K2 + V1
                'dashscope': (os.getenv('DASHSCOPE_API_KEY'), 5),  # é˜¿é‡Œäº‘5ä¸ªæ¨¡å‹
                'zhipuai': (os.getenv('ZHIPUAI_API_KEY'), 2),  # GLM-4.5 + Air
                'ollama': (os.getenv('OLLAMA_BASE_URL'), 4),  # 4ä¸ªæœ¬åœ°æ¨¡å‹
                'openai': (os.getenv('OPENAI_API_KEY'), 0)  # ä»…embedding
            }
            
            available_providers = []
            model_count = 0
            embedding_count = 0
            
            for provider, (key, models) in llm_providers.items():
                if key and not key.startswith('your_'):
                    available_providers.append(provider)
                    model_count += models
                    
                    # è®¡ç®—embeddingæ¨¡å‹
                    if provider == 'siliconflow':
                        embedding_count += 1
                    elif provider == 'ollama':
                        embedding_count += 3
                    elif provider == 'openai':
                        embedding_count += 2
            
            status_info['llm_services']['available_count'] = model_count
            status_info['llm_services']['embedding_count'] = embedding_count
            status_info['llm_services']['total_providers'] = len(available_providers)
            status_info['llm_services']['providers'] = available_providers
        except Exception as e:
            logger.debug(f"LLMæœåŠ¡æ£€æµ‹å¤±è´¥: {e}")
        
        # æ£€æµ‹å¤–éƒ¨æœåŠ¡çŠ¶æ€
        # SiYuan
        try:
            siyuan_url = os.getenv('SIYUAN_API_URL', 'http://192.168.5.54:6806')
            response = requests.get(f"{siyuan_url}/api/system/getConf", timeout=3)
            if response.status_code == 200:
                status_info['external_services']['siyuan'] = True
        except Exception as e:
            logger.debug(f"SiYuanè¿æ¥æ£€æµ‹å¤±è´¥: {e}")
        
        # Dify - ä¿®å¤æ£€æµ‹é€»è¾‘
        try:
            dify_url = os.getenv('DIFY_BASE_URL', 'http://192.168.5.54:5001')
            # Difyå¯èƒ½æ²¡æœ‰æ ‡å‡†çš„health-checkç«¯ç‚¹ï¼Œå°è¯•è®¿é—®æ ¹è·¯å¾„æˆ–APIç«¯ç‚¹
            try:
                # å…ˆå°è¯•æ ¹è·¯å¾„
                response = requests.get(dify_url, timeout=3)
                if response.status_code in [200, 301, 302, 404]:  # è¿™äº›çŠ¶æ€ç è¡¨ç¤ºæœåŠ¡åœ¨è¿è¡Œ
                    status_info['external_services']['dify'] = True
                else:
                    raise Exception(f"æ ¹è·¯å¾„è¿”å›çŠ¶æ€ç : {response.status_code}")
            except Exception:
                # å¦‚æœæ ¹è·¯å¾„å¤±è´¥ï¼Œå°è¯•APIç«¯ç‚¹
                response = requests.get(f"{dify_url}/v1/info", timeout=3)
                if response.status_code in [200, 401, 403]:  # åŒ…æ‹¬è®¤è¯é”™è¯¯ï¼Œè¯´æ˜æœåŠ¡åœ¨è¿è¡Œ
                    status_info['external_services']['dify'] = True
        except Exception as e:
            logger.debug(f"Difyè¿æ¥æ£€æµ‹å¤±è´¥: {e}")
        
        # Ollama
        try:
            ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://192.168.5.217:11434')
            response = requests.get(f"{ollama_url}/api/tags", timeout=3)
            if response.status_code == 200:
                status_info['external_services']['ollama'] = True
        except Exception as e:
            logger.debug(f"Ollamaè¿æ¥æ£€æµ‹å¤±è´¥: {e}")
        
        return jsonify({
            'success': True,
            'data': status_info
        })
        
    except Exception as e:
        logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

async def validate_api_token(provider_config: dict, timeout: int = 5) -> bool:
    """éªŒè¯API tokenæ˜¯å¦å®é™…å¯ç”¨"""
    try:
        api_key_env = provider_config.get('api_key_env')
        if not api_key_env:
            # Ollamaç­‰æœ¬åœ°æœåŠ¡ä¸éœ€è¦API keyï¼Œæ£€æŸ¥URLè¿é€šæ€§
            base_url = os.getenv(provider_config.get('base_url_env', ''), provider_config.get('base_url', ''))
            if base_url:
                async with httpx.AsyncClient(timeout=timeout) as client:
                    response = await client.get(f"{base_url}/api/tags" if 'ollama' in base_url else base_url)
                    return response.status_code < 500
            return False
        
        api_key = os.getenv(api_key_env)
        if not api_key or api_key.startswith('your_'):
            return False
        
        base_url = os.getenv(provider_config.get('base_url_env', ''), provider_config.get('base_url'))
        
        # æ ¹æ®ä¸åŒä¾›åº”å•†è¿›è¡Œå®é™…APIè°ƒç”¨æµ‹è¯•
        async with httpx.AsyncClient(timeout=timeout) as client:
            headers = {'Authorization': f'Bearer {api_key}'}
            
            if 'deepseek' in base_url.lower():
                response = await client.get(f"{base_url}/models", headers=headers)
            elif 'siliconflow' in base_url.lower():
                response = await client.get(f"{base_url}/models", headers=headers)
            elif 'moonshot' in base_url.lower():
                response = await client.get(f"{base_url}/models", headers=headers)
            elif 'bigmodel' in base_url.lower():
                # æ™ºè°±AI
                response = await client.post(f"{base_url}/chat/completions", 
                    headers=headers, 
                    json={"model": "glm-4.5", "messages": [{"role": "user", "content": "test"}], "max_tokens": 1})
            elif 'dashscope' in base_url.lower():
                # é˜¿é‡Œäº‘
                response = await client.get(f"{base_url}/models", headers=headers)
            elif 'volces' in base_url.lower():
                # ç«å±±å¼•æ“
                response = await client.post(f"{base_url}/chat/completions",
                    headers=headers,
                    json={"model": "doubao-seed-1.6", "messages": [{"role": "user", "content": "test"}], "max_tokens": 1})
            else:
                # é€šç”¨OpenAIå…¼å®¹æµ‹è¯•
                response = await client.get(f"{base_url}/models", headers=headers)
            
            return response.status_code < 400
            
    except Exception as e:
        logger.debug(f"API token validation failed for {provider_config.get('name', 'unknown')}: {e}")
        return False

def load_llm_config() -> dict:
    """åŠ è½½LLMé…ç½®æ–‡ä»¶"""
    try:
        # Try Docker path first (2 parents: /app/routes/api.py -> /app/HomeSystem/...)
        config_path = Path(__file__).parent.parent / 'HomeSystem' / 'graph' / 'config' / 'llm_providers.yaml'
        
        # If Docker path doesn't exist, try local development path (4 parents)
        if not config_path.exists():
            config_path = Path(__file__).parent.parent.parent.parent / 'HomeSystem' / 'graph' / 'config' / 'llm_providers.yaml'
        
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        logger.error(f"åŠ è½½LLMé…ç½®å¤±è´¥: {e}")
        return {}


@api_bp.route('/about/llm_models')  
def get_about_llm_models():
    """è·å–æ‰€æœ‰å¯ç”¨çš„LLMæ¨¡å‹è¯¦ç»†ä¿¡æ¯ï¼Œé€šè¿‡å®é™…APIè°ƒç”¨éªŒè¯å¯ç”¨æ€§"""
    try:
        # åŠ è½½å®Œæ•´çš„LLMé…ç½®
        config = load_llm_config()
        if not config:
            return jsonify({
                'success': False,
                'error': 'æ— æ³•åŠ è½½LLMé…ç½®æ–‡ä»¶'
            }), 500
        
        providers_data = {}
        total_chat_models = 0
        total_embedding_models = 0
        
        # éªŒè¯API tokenå¯ç”¨æ€§çš„å¼‚æ­¥å‡½æ•°åŒ…è£…å™¨
        async def validate_all_providers():
            validation_results = {}
            
            # éªŒè¯æ‰€æœ‰LLMæä¾›å•†
            for provider_key, provider_config in config.get('providers', {}).items():
                is_available = await validate_api_token(provider_config)
                validation_results[provider_key] = is_available
            
            # éªŒè¯æ‰€æœ‰Embeddingæä¾›å•†
            for provider_key, provider_config in config.get('embedding_providers', {}).items():
                is_available = await validate_api_token(provider_config)
                validation_results[f"{provider_key}_embedding"] = is_available
            
            return validation_results
        
        # è¿è¡ŒéªŒè¯
        validation_results = asyncio.run(validate_all_providers())
        
        # DeepSeek
        if os.getenv('DEEPSEEK_API_KEY') and not os.getenv('DEEPSEEK_API_KEY').startswith('your_'):
            providers_data['deepseek'] = {
                'name': 'DeepSeek',
                'chat_models': [
                    {
                        'key': 'deepseek.DeepSeek_V3',
                        'name': 'deepseek-chat',
                        'display_name': 'DeepSeek V3',
                        'description': 'MoEæ¶æ„ï¼Œ14.8ä¸‡äº¿tokenè®­ç»ƒï¼Œ671Bæ€»å‚æ•°/37Bæ¿€æ´»',
                        'max_tokens': 131072,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': False
                    },
                    {
                        'key': 'deepseek.DeepSeek_R1',
                        'name': 'deepseek-reasoner',
                        'display_name': 'DeepSeek R1',
                        'description': 'æœ€æ–°æ¨ç†æ¨¡å‹ï¼ŒAIME 2025è¾¾87.5%å‡†ç¡®ç‡',
                        'max_tokens': 131072,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': True
                    }
                ],
                'embedding_models': []
            }
            total_chat_models += 2

        # SiliconFlow
        if os.getenv('SILICONFLOW_API_KEY') and not os.getenv('SILICONFLOW_API_KEY').startswith('your_'):
            providers_data['siliconflow'] = {
                'name': 'SiliconFlow (ç¡…åŸºæµåŠ¨)',
                'chat_models': [
                    {
                        'key': 'siliconflow.DeepSeek_R1',
                        'name': 'deepseek-ai/DeepSeek-R1',
                        'display_name': 'DeepSeek R1',
                        'description': 'é€šè¿‡ç¡…åŸºæµåŠ¨æä¾›çš„DeepSeek R1æ¨ç†ä¼˜åŒ–ç‰ˆæœ¬',
                        'max_tokens': 131072,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': True
                    },
                    {
                        'key': 'siliconflow.DeepSeek_V3',
                        'name': 'deepseek-ai/DeepSeek-V3',
                        'display_name': 'DeepSeek V3',
                        'description': 'é€šè¿‡ç¡…åŸºæµåŠ¨æä¾›çš„DeepSeek V3ï¼Œ671Bæ€»å‚æ•°/37Bæ¿€æ´»',
                        'max_tokens': 131072,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': False
                    },
                    {
                        'key': 'siliconflow.QwQ_32B',
                        'name': 'Qwen/QwQ-32B-Preview',
                        'display_name': 'é€šä¹‰åƒé—® QwQ-32B',
                        'description': 'é˜¿é‡Œé€šä¹‰åƒé—®æ¨ç†å¢å¼ºç‰ˆæœ¬ï¼Œ32Bå‚æ•°',
                        'max_tokens': 32768,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': True
                    },
                    {
                        'key': 'siliconflow.Qwen2_5_72B',
                        'name': 'Qwen/Qwen2.5-72B-Instruct',
                        'display_name': 'é€šä¹‰åƒé—® 2.5-72B',
                        'description': 'é€šä¹‰åƒé—®2.5ç³»åˆ—æœ€å¼ºç‰ˆæœ¬ï¼Œ72Bå‚æ•°',
                        'max_tokens': 131072,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': False
                    },
                    {
                        'key': 'siliconflow.Qwen3_235B_A22B',
                        'name': 'Qwen/Qwen3-235B-A22B-Instruct-2507',
                        'display_name': 'é€šä¹‰åƒé—® 3-235B-A22B',
                        'description': 'é€šä¹‰åƒé—®3ç³»åˆ—æœ€å¼ºç‰ˆæœ¬ï¼Œ235Bå‚æ•°ï¼Œæ”¯æŒ256Kä¸Šä¸‹æ–‡',
                        'max_tokens': 10240,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': False
                    },
                    {
                        'key': 'siliconflow.Qwen3_235B_A22B_Thinking',
                        'name': 'Qwen/Qwen3-235B-A22B-Thinking-2507',
                        'display_name': 'é€šä¹‰åƒé—® 3-235B-A22B-æ€è€ƒç‰ˆ',
                        'description': 'é€šä¹‰åƒé—®3ç³»åˆ—æœ€å¼ºæ€è€ƒç‰ˆæœ¬ï¼Œ235Bå‚æ•°',
                        'max_tokens': 10240,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': True
                    }
                ],
                'embedding_models': [
                    {
                        'key': 'siliconflow.BGE_Large_ZH_V1_5',
                        'name': 'BAAI/bge-large-zh-v1.5',
                        'display_name': 'BGE Large ä¸­æ–‡ v1.5',
                        'description': 'ä¸­æ–‡ä¼˜åŒ–çš„embeddingæ¨¡å‹ï¼Œ326Må‚æ•°',
                        'dimensions': 1024,
                        'max_input': 512
                    }
                ]
            }
            total_chat_models += 6
            total_embedding_models += 1

        # Volcano Engine (è±†åŒ…)
        if os.getenv('VOLCANO_API_KEY') and not os.getenv('VOLCANO_API_KEY').startswith('your_'):
            providers_data['volcano'] = {
                'name': 'Volcano Engine (è±†åŒ…)',
                'chat_models': [
                    {
                        'key': 'volcano.Doubao_1_6',
                        'name': 'doubao-seed-1.6',
                        'display_name': 'è±†åŒ…1.6 å…¨èƒ½ç‰ˆ',
                        'description': 'All-in-Oneç»¼åˆæ¨¡å‹ï¼Œæ”¯æŒæ·±åº¦æ€è€ƒå’Œå¤šæ¨¡æ€ï¼Œ256Kä¸Šä¸‹æ–‡',
                        'max_tokens': 16384,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': True
                    },
                    {
                        'key': 'volcano.Doubao_1_6_Thinking',
                        'name': 'doubao-seed-1.6-thinking',
                        'display_name': 'è±†åŒ…1.6 æ·±åº¦æ€è€ƒç‰ˆ',
                        'description': 'æ·±åº¦æ€è€ƒå¼ºåŒ–ç‰ˆï¼Œæ•°å­¦æ¨ç†èƒ½åŠ›çªå‡ºï¼Œ256Kä¸Šä¸‹æ–‡',
                        'max_tokens': 16384,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': True
                    },
                    {
                        'key': 'volcano.Doubao_1_6_Flash',
                        'name': 'doubao-seed-1.6-flash',
                        'display_name': 'è±†åŒ…1.6 æé€Ÿç‰ˆ',
                        'description': 'æä½å»¶è¿Ÿç‰ˆæœ¬ï¼ŒTOPTä»…éœ€10msï¼Œæ”¯æŒ256Kä¸Šä¸‹æ–‡',
                        'max_tokens': 16384,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': False
                    }
                ],
                'embedding_models': []
            }
            total_chat_models += 3

        # Moonshot (Kimi)
        if os.getenv('MOONSHOT_API_KEY') and not os.getenv('MOONSHOT_API_KEY').startswith('your_'):
            providers_data['moonshot'] = {
                'name': 'MoonShot (æœˆä¹‹æš—é¢)',
                'chat_models': [
                    {
                        'key': 'moonshot.Kimi_K2',
                        'name': 'kimi-k2-0711-preview',
                        'display_name': 'Kimi K2',
                        'description': 'ä¸‡äº¿å‚æ•°MoEæ™ºèƒ½ä½“æ¨¡å‹ï¼Œä¸“æ³¨ä»£ç å’Œæ¨ç†ï¼Œ1Tæ€»å‚æ•°/32Bæ¿€æ´»',
                        'max_tokens': 16384,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': False
                    },
                    {
                        'key': 'moonshot.Kimi_V1_128K',
                        'name': 'moonshot-v1-128k',
                        'display_name': 'Kimi v1 128K',
                        'description': 'é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸“ç”¨ç‰ˆæœ¬ï¼Œæ”¯æŒ128Kä¸Šä¸‹æ–‡',
                        'max_tokens': 16384,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': False
                    }
                ],
                'embedding_models': []
            }
            total_chat_models += 2

        # ZhipuAI (æ™ºè°±AI)
        if os.getenv('ZHIPUAI_API_KEY') and not os.getenv('ZHIPUAI_API_KEY').startswith('your_'):
            providers_data['zhipuai'] = {
                'name': 'ZhipuAI (æ™ºè°±AI)',
                'chat_models': [
                    {
                        'key': 'zhipuai.GLM_4_5',
                        'name': 'glm-4.5',
                        'display_name': 'GLM-4.5',
                        'description': 'æ™ºèƒ½ä½“åŸç”Ÿæ——èˆ°æ¨¡å‹ï¼ŒMoEæ¶æ„ï¼Œå…¨çƒæ’åç¬¬3ï¼Œ355Bæ€»å‚æ•°/32Bæ¿€æ´»',
                        'max_tokens': 32768,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': False
                    },
                    {
                        'key': 'zhipuai.GLM_4_5_Air',
                        'name': 'glm-4.5-air',
                        'display_name': 'GLM-4.5-Air',
                        'description': 'è½»é‡åŒ–ç‰ˆæœ¬ï¼Œé«˜æ•ˆæ™ºèƒ½ä½“æ¨¡å‹ï¼Œ106Bæ€»å‚æ•°/12Bæ¿€æ´»ï¼Œæ€§èƒ½è¯„åˆ†59.8',
                        'max_tokens': 32768,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': False
                    }
                ],
                'embedding_models': []
            }
            total_chat_models += 2

        # Alibaba (é˜¿é‡Œäº‘)
        if os.getenv('DASHSCOPE_API_KEY') and not os.getenv('DASHSCOPE_API_KEY').startswith('your_'):
            providers_data['alibaba'] = {
                'name': 'Alibaba Cloud (é˜¿é‡Œäº‘)',
                'chat_models': [
                    {
                        'key': 'alibaba.Qwen_Turbo_Latest',
                        'name': 'qwen-turbo-latest',
                        'display_name': 'é€šä¹‰åƒé—® Turbo æœ€æ–°ç‰ˆ',
                        'description': 'æœ€æ–°ç‰ˆTurboæ¨¡å‹ï¼Œæ”¯æŒæ€è€ƒæ¨¡å¼ï¼Œé€Ÿåº¦æœ€å¿«æˆæœ¬æœ€ä½ï¼Œé€‚åˆç®€å•ä»»åŠ¡',
                        'max_tokens': 8192,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': True
                    },
                    {
                        'key': 'alibaba.Qwen_Turbo',
                        'name': 'qwen-turbo',
                        'display_name': 'é€šä¹‰åƒé—® Turbo',
                        'description': 'é«˜æ•ˆè½»é‡çº§æ¨¡å‹ï¼Œé€Ÿåº¦å¿«æˆæœ¬ä½ï¼Œé€‚åˆæ—¥å¸¸å¯¹è¯å’Œç®€å•ä»»åŠ¡',
                        'max_tokens': 8192,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': False
                    },
                    {
                        'key': 'alibaba.Qwen_Plus',
                        'name': 'qwen-plus',
                        'display_name': 'é€šä¹‰åƒé—® Plus',
                        'description': 'å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬çš„æ¨¡å‹ï¼Œæ”¯æŒæ€è€ƒæ¨¡å¼ï¼Œé€‚åˆå¤æ‚æ¨ç†ä»»åŠ¡',
                        'max_tokens': 32768,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': True
                    },
                    {
                        'key': 'alibaba.Qwen3_235B_A22B',
                        'name': 'qwen3-235b-a22b-instruct-2507',
                        'display_name': 'é€šä¹‰åƒé—® 3-235B-A22B',
                        'description': 'é€šä¹‰åƒé—®3ç³»åˆ—æœ€å¼ºç‰ˆæœ¬ï¼ŒMoEæ¶æ„ï¼Œæ”¯æŒ256Kä¸Šä¸‹æ–‡ï¼Œ235Bå‚æ•°',
                        'max_tokens': 32768,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': False
                    },
                    {
                        'key': 'alibaba.Qwen3_235B_A22B_Thinking',
                        'name': 'qwen3-235b-a22b-thinking-2507',
                        'display_name': 'é€šä¹‰åƒé—® 3-235B-A22B-æ€è€ƒç‰ˆ',
                        'description': 'ä¸“é—¨çš„æ€è€ƒæ¨¡å¼æ¨¡å‹ï¼Œæ”¯æŒ80Kæ¨ç†è¿‡ç¨‹é•¿åº¦ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Š',
                        'max_tokens': 32768,
                        'supports_functions': True,
                        'supports_vision': False,
                        'supports_thinking': True
                    }
                ],
                'embedding_models': []
            }
            total_chat_models += 5

        # Ollama (æœ¬åœ°)
        if os.getenv('OLLAMA_BASE_URL'):
            providers_data['ollama'] = {
                'name': 'Ollama (æœ¬åœ°éƒ¨ç½²)',
                'chat_models': [
                    {
                        'key': 'ollama.DeepSeek_R1_14B',
                        'name': 'deepseek-r1:14b',
                        'display_name': 'DeepSeek R1 14B',
                        'description': 'DeepSeekæ¨ç†æ¨¡å‹14Bç‰ˆæœ¬ï¼Œæ”¯æŒ128Kä¸Šä¸‹æ–‡',
                        'max_tokens': 32768,
                        'supports_functions': False,
                        'supports_vision': False,
                        'supports_thinking': True
                    },
                    {
                        'key': 'ollama.Qwen2_5_VL_7B',
                        'name': 'qwen2.5vl:7b',
                        'display_name': 'é€šä¹‰åƒé—® 2.5-VL-7B (è§†è§‰)',
                        'description': 'é€šä¹‰åƒé—®2.5ç³»åˆ—7Bç‰ˆæœ¬ï¼Œæ”¯æŒè§†è§‰å’Œå›¾ç‰‡åˆ†æ',
                        'max_tokens': 32768,
                        'supports_functions': False,
                        'supports_vision': True,
                        'supports_thinking': False
                    },
                    {
                        'key': 'ollama.Qwen3_30B',
                        'name': 'qwen3:30b',
                        'display_name': 'é€šä¹‰åƒé—®3 30B',
                        'description': 'MoEæ¶æ„ä»£ç ä¸“ç”¨æ¨¡å‹ï¼Œå¤šè¯­è¨€æ”¯æŒï¼Œæ”¯æŒ128Kä¸Šä¸‹æ–‡',
                        'max_tokens': 32768,
                        'supports_functions': False,
                        'supports_vision': False,
                        'supports_thinking': False
                    },
                    {
                        'key': 'ollama.gpt-oss',
                        'name': 'gpt-oss',
                        'display_name': 'Open AI GPT OSS',
                        'description': 'MoEæ¶æ„ä»£ç ä¸“ç”¨æ¨¡å‹ï¼Œå¤šè¯­è¨€æ”¯æŒï¼Œæ”¯æŒ128Kä¸Šä¸‹æ–‡ï¼Œ20Bå‚æ•°',
                        'max_tokens': 32768,
                        'supports_functions': False,
                        'supports_vision': False,
                        'supports_thinking': False
                    }
                ],
                'embedding_models': [
                    {
                        'key': 'ollama.BGE_M3',
                        'name': 'bge-m3:latest',
                        'display_name': 'BGE-M3',
                        'description': 'BAAIå¼€æºçš„å¤šè¯­è¨€embeddingæ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±æ–‡ï¼Œ560Må‚æ•°',
                        'dimensions': 1024,
                        'max_input': 8192
                    },
                    {
                        'key': 'ollama.Nomic_Embed_Text',
                        'name': 'nomic-embed-text:latest',
                        'display_name': 'Nomic Embed Text',
                        'description': 'é«˜æ•ˆçš„è‹±æ–‡æ–‡æœ¬embeddingæ¨¡å‹ï¼Œ137Må‚æ•°',
                        'dimensions': 768,
                        'max_input': 2048
                    },
                    {
                        'key': 'ollama.MxBai_Embed_Large',
                        'name': 'mxbai-embed-large:latest',
                        'display_name': 'MxBai Embed Large',
                        'description': 'é«˜è´¨é‡çš„é€šç”¨embeddingæ¨¡å‹ï¼Œ335Må‚æ•°',
                        'dimensions': 1024,
                        'max_input': 512
                    }
                ]
            }
            total_chat_models += 4
            total_embedding_models += 3

        # OpenAI (å¦‚æœé…ç½®äº†çš„è¯)
        if os.getenv('OPENAI_API_KEY') and not os.getenv('OPENAI_API_KEY').startswith('your_'):
            providers_data['openai'] = {
                'name': 'OpenAI',
                'chat_models': [],
                'embedding_models': [
                    {
                        'key': 'openai.Text_Embedding_3_Large',
                        'name': 'text-embedding-3-large',
                        'display_name': 'Text Embedding 3 Large',
                        'description': 'OpenAIæœ€æ–°å¤§å‹embeddingæ¨¡å‹',
                        'dimensions': 3072,
                        'max_input': 8191
                    },
                    {
                        'key': 'openai.Text_Embedding_3_Small',
                        'name': 'text-embedding-3-small',
                        'display_name': 'Text Embedding 3 Small',
                        'description': 'OpenAIç´§å‡‘å‹embeddingæ¨¡å‹',
                        'dimensions': 1536,
                        'max_input': 8191
                    }
                ]
            }
            total_embedding_models += 2
        
        return jsonify({
            'success': True,
            'data': {
                'providers': providers_data,
                'total_chat_models': total_chat_models,
                'total_embedding_models': total_embedding_models,
                'total_providers': len(providers_data)
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–LLMæ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ========== è¿ç§»å’Œä»»åŠ¡ç®¡ç†ç›¸å…³API ==========

@api_bp.route('/tasks/available_for_migration')
def api_get_available_tasks_for_migration():
    """è·å–å¯ç”¨äºè¿ç§»çš„ä»»åŠ¡åˆ—è¡¨"""
    try:
        tasks = paper_explore_service.get_available_tasks()
        return jsonify({'success': True, 'data': {'tasks': tasks}})
    
    except Exception as e:
        logger.error(f"è·å–è¿ç§»ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/get_tasks')
def api_get_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨ï¼ˆç”¨äºæ‰¹é‡æ“ä½œï¼‰"""
    try:
        tasks_data = paper_explore_service.get_available_tasks()
        # å°†å¤æ‚çš„æ•°æ®ç»“æ„è½¬æ¢ä¸ºç®€å•çš„ä»»åŠ¡æ•°ç»„ï¼Œä¾›æ‰¹é‡æ“ä½œä½¿ç”¨
        all_tasks = []
        
        # æ·»åŠ åŸºäºä»»åŠ¡åç§°çš„ä»»åŠ¡
        for task in tasks_data.get('task_names', []):
            all_tasks.append({
                'task_name': task['task_name'],
                'task_id': '',  # task_namesä¸­æ²¡æœ‰task_id
                'paper_count': task['paper_count'],
                'created_at': None  # task_namesä¸­æ²¡æœ‰æ—¶é—´ä¿¡æ¯
            })
        
        # æ·»åŠ åŸºäºä»»åŠ¡IDçš„ä»»åŠ¡ï¼ˆé¿å…é‡å¤ï¼‰
        task_name_set = {task['task_name'] for task in all_tasks}
        for task in tasks_data.get('task_ids', []):
            if task['task_name'] not in task_name_set:
                all_tasks.append({
                    'task_name': task['task_name'],
                    'task_id': task['task_id'],
                    'paper_count': task['paper_count'],
                    'created_at': task.get('last_created')  # ä½¿ç”¨last_createdä½œä¸ºå±•ç¤ºæ—¶é—´
                })
        
        return jsonify({'success': True, 'tasks': all_tasks})
    
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/migrate_paper_to_task', methods=['POST'])
def api_migrate_paper_to_task():
    """å•ä¸ªè®ºæ–‡è¿ç§»åˆ°æ–°ä»»åŠ¡"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_id = data.get('arxiv_id')
        target_task_name = data.get('target_task_name', '').strip()
        target_task_id = data.get('target_task_id', '').strip()
        
        if not arxiv_id or not target_task_name:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        success = paper_explore_service.migrate_paper_to_task(
            arxiv_id, target_task_name, target_task_id or None
        )
        
        if success:
            return jsonify({'success': True, 'message': 'è®ºæ–‡è¿ç§»æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'è¿ç§»å¤±è´¥ï¼Œè®ºæ–‡ä¸å­˜åœ¨'}), 404
    
    except Exception as e:
        logger.error(f"è®ºæ–‡è¿ç§»å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/batch_migrate_to_task', methods=['POST'])
def api_batch_migrate_to_task():
    """æ‰¹é‡è®ºæ–‡è¿ç§»åˆ°æ–°ä»»åŠ¡"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_ids = data.get('arxiv_ids', [])
        target_task_name = data.get('target_task_name', '').strip()
        target_task_id = data.get('target_task_id', '').strip()
        
        if not arxiv_ids or not target_task_name:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        affected_rows = paper_explore_service.batch_migrate_papers_to_task(
            arxiv_ids, target_task_name, target_task_id or None
        )
        
        return jsonify({
            'success': True, 
            'affected_rows': affected_rows,
            'message': f'æˆåŠŸè¿ç§» {affected_rows} ç¯‡è®ºæ–‡'
        })
    
    except Exception as e:
        logger.error(f"æ‰¹é‡è¿ç§»å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/assign_task_to_paper', methods=['POST'])
def api_assign_task_to_paper():
    """ä¸ºè®ºæ–‡åˆ†é…ä»»åŠ¡"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_id = data.get('arxiv_id')
        task_name = data.get('task_name', '').strip()
        task_id = data.get('task_id', '').strip()
        
        if not arxiv_id or not task_name:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        success = paper_explore_service.assign_task_to_paper(
            arxiv_id, task_name, task_id or None
        )
        
        if success:
            return jsonify({'success': True, 'message': 'ä»»åŠ¡åˆ†é…æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'åˆ†é…å¤±è´¥ï¼Œè®ºæ–‡ä¸å­˜åœ¨'}), 404
    
    except Exception as e:
        logger.error(f"ä»»åŠ¡åˆ†é…å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/assign_task', methods=['POST'])
def api_assign_task():
    """ä¸ºå•ä¸ªè®ºæ–‡åˆ†é…ä»»åŠ¡"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_id = data.get('arxiv_id', '').strip()
        task_name = data.get('task_name', '').strip()
        task_id = data.get('task_id', '').strip()
        
        if not arxiv_id or not task_name:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: arxiv_id å’Œ task_name'}), 400
        
        # ä½¿ç”¨æ‰¹é‡åˆ†é…æ–¹æ³•å¤„ç†å•ä¸ªè®ºæ–‡
        affected_rows = paper_explore_service.batch_assign_task_to_papers(
            [arxiv_id], task_name, task_id or None
        )
        
        if affected_rows > 0:
            return jsonify({
                'success': True, 
                'message': f'æˆåŠŸä¸ºè®ºæ–‡ {arxiv_id} åˆ†é…ä»»åŠ¡ {task_name}'
            })
        else:
            return jsonify({
                'success': False, 
                'error': f'è®ºæ–‡ {arxiv_id} ä¸å­˜åœ¨æˆ–ä»»åŠ¡åˆ†é…å¤±è´¥'
            }), 404
    
    except Exception as e:
        logger.error(f"åˆ†é…ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/batch_assign_tasks', methods=['POST'])
def api_batch_assign_tasks():
    """æ‰¹é‡åˆ†é…ä»»åŠ¡ - ä¸å‰ç«¯æœŸæœ›çš„è·¯å¾„åŒ¹é…"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_ids = data.get('arxiv_ids', [])
        task_name = data.get('task_name', '').strip()
        task_id = data.get('task_id', '').strip()
        
        if not arxiv_ids or not task_name:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        affected_rows = paper_explore_service.batch_assign_task_to_papers(
            arxiv_ids, task_name, task_id or None
        )
        
        return jsonify({
            'success': True, 
            'affected_rows': affected_rows,
            'message': f'æˆåŠŸä¸º {affected_rows} ç¯‡è®ºæ–‡åˆ†é…ä»»åŠ¡'
        })
    
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ†é…ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/batch_assign_task', methods=['POST'])
def api_batch_assign_task():
    """æ‰¹é‡åˆ†é…ä»»åŠ¡"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_ids = data.get('arxiv_ids', [])
        task_name = data.get('task_name', '').strip()
        task_id = data.get('task_id', '').strip()
        
        if not arxiv_ids or not task_name:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        affected_rows = paper_explore_service.batch_assign_task_to_papers(
            arxiv_ids, task_name, task_id or None
        )
        
        return jsonify({
            'success': True, 
            'affected_rows': affected_rows,
            'message': f'æˆåŠŸä¸º {affected_rows} ç¯‡è®ºæ–‡åˆ†é…ä»»åŠ¡'
        })
    
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ†é…ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# === ç›¸å…³æ€§è¯„åˆ†API ===

@api_bp.route('/update_relevance_score', methods=['POST'])
def api_update_relevance_score():
    """æ›´æ–°å•ä¸ªè®ºæ–‡çš„ç›¸å…³æ€§è¯„åˆ†"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_id = data.get('arxiv_id', '').strip()
        relevance_score = data.get('relevance_score')
        
        if not arxiv_id or relevance_score is None:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: arxiv_id å’Œ relevance_score'}), 400
        
        # éªŒè¯è¯„åˆ†èŒƒå›´
        try:
            score = float(relevance_score)
            # å¦‚æœè¯„åˆ†åœ¨0-10èŒƒå›´å†…ï¼Œè½¬æ¢ä¸º0-1èŒƒå›´
            if 0 <= score <= 10:
                score_normalized = score / 10.0
            elif 0 <= score <= 1:
                score_normalized = score
            else:
                return jsonify({'success': False, 'error': 'ç›¸å…³æ€§è¯„åˆ†å¿…é¡»åœ¨0-1æˆ–0-10ä¹‹é—´'}), 400
        except (ValueError, TypeError):
            return jsonify({'success': False, 'error': 'ç›¸å…³æ€§è¯„åˆ†å¿…é¡»æ˜¯æœ‰æ•ˆæ•°å­—'}), 400
        
        # æ›´æ–°ç›¸å…³æ€§è¯„åˆ†
        success = paper_explore_service.update_paper_relevance(arxiv_id, score_normalized)
        
        if success:
            return jsonify({
                'success': True, 
                'message': f'æˆåŠŸæ›´æ–°è®ºæ–‡ {arxiv_id} çš„ç›¸å…³æ€§è¯„åˆ†ä¸º {score}'
            })
        else:
            return jsonify({
                'success': False, 
                'error': f'è®ºæ–‡ {arxiv_id} ä¸å­˜åœ¨æˆ–æ›´æ–°å¤±è´¥'
            }), 404
    
    except Exception as e:
        logger.error(f"æ›´æ–°ç›¸å…³æ€§è¯„åˆ†å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/update_relevance', methods=['POST'])
def api_update_relevance():
    """æ›´æ–°ç›¸å…³æ€§ï¼ˆå…¼å®¹æ—§ç‰ˆAPIï¼‰"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_id = data.get('arxiv_id', '').strip()
        relevance_score = data.get('relevance_score') or data.get('score')
        
        if not arxiv_id or relevance_score is None:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        # éªŒè¯è¯„åˆ†èŒƒå›´
        try:
            score = float(relevance_score)
            # å¦‚æœè¯„åˆ†åœ¨0-10èŒƒå›´å†…ï¼Œè½¬æ¢ä¸º0-1èŒƒå›´
            if 0 <= score <= 10:
                score_normalized = score / 10.0
            elif 0 <= score <= 1:
                score_normalized = score
            else:
                return jsonify({'success': False, 'error': 'ç›¸å…³æ€§è¯„åˆ†å¿…é¡»åœ¨0-1æˆ–0-10ä¹‹é—´'}), 400
        except (ValueError, TypeError):
            return jsonify({'success': False, 'error': 'ç›¸å…³æ€§è¯„åˆ†å¿…é¡»æ˜¯æœ‰æ•ˆæ•°å­—'}), 400
        
        # æ›´æ–°ç›¸å…³æ€§è¯„åˆ†
        success = paper_explore_service.update_paper_relevance(arxiv_id, score_normalized)
        
        if success:
            return jsonify({
                'success': True, 
                'message': f'æˆåŠŸæ›´æ–°è®ºæ–‡ {arxiv_id} çš„ç›¸å…³æ€§è¯„åˆ†ä¸º {score}'
            })
        else:
            return jsonify({
                'success': False, 
                'error': f'è®ºæ–‡ {arxiv_id} ä¸å­˜åœ¨æˆ–æ›´æ–°å¤±è´¥'
            }), 404
    
    except Exception as e:
        logger.error(f"æ›´æ–°ç›¸å…³æ€§å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# === æ‰¹é‡æ“ä½œAPI ===

@api_bp.route('/batch_delete_papers', methods=['POST'])
def api_batch_delete_papers():
    """æ‰¹é‡åˆ é™¤è®ºæ–‡"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_ids = data.get('arxiv_ids', [])
        
        if not arxiv_ids:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: arxiv_ids'}), 400
        
        # æ‰§è¡Œæ‰¹é‡åˆ é™¤
        deleted_count = 0
        errors = []
        
        for arxiv_id in arxiv_ids:
            try:
                success = paper_explore_service.delete_paper(arxiv_id)
                if success:
                    deleted_count += 1
                else:
                    errors.append(f"è®ºæ–‡ {arxiv_id} åˆ é™¤å¤±è´¥")
            except Exception as e:
                errors.append(f"è®ºæ–‡ {arxiv_id} åˆ é™¤å¤±è´¥: {str(e)}")
        
        return jsonify({
            'success': True,
            'deleted_count': deleted_count,
            'total_requested': len(arxiv_ids),
            'errors': errors,
            'message': f'æˆåŠŸåˆ é™¤ {deleted_count}/{len(arxiv_ids)} ç¯‡è®ºæ–‡'
        })
    
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ é™¤è®ºæ–‡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/batch_remove_from_dify', methods=['POST'])
def api_batch_remove_from_dify():
    """æ‰¹é‡ä»DifyçŸ¥è¯†åº“ç§»é™¤è®ºæ–‡"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_ids = data.get('arxiv_ids', [])
        
        if not arxiv_ids:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: arxiv_ids'}), 400
        
        # æ‰§è¡Œæ‰¹é‡ç§»é™¤
        removed_count = 0
        errors = []
        
        for arxiv_id in arxiv_ids:
            try:
                result = dify_service.remove_paper_from_dify(arxiv_id)
                if result.get('success'):
                    removed_count += 1
                else:
                    errors.append(f"è®ºæ–‡ {arxiv_id} ç§»é™¤å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            except Exception as e:
                errors.append(f"è®ºæ–‡ {arxiv_id} ç§»é™¤å¤±è´¥: {str(e)}")
        
        return jsonify({
            'success': True,
            'removed_count': removed_count,
            'total_requested': len(arxiv_ids),
            'errors': errors,
            'message': f'æˆåŠŸä»Difyç§»é™¤ {removed_count}/{len(arxiv_ids)} ç¯‡è®ºæ–‡'
        })
    
    except Exception as e:
        logger.error(f"æ‰¹é‡ä»Difyç§»é™¤è®ºæ–‡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/refresh', methods=['POST'])
def api_refresh():
    """åˆ·æ–°æ•°æ®"""
    try:
        # è¿™é‡Œå¯ä»¥å®ç°æ•°æ®åˆ·æ–°é€»è¾‘ï¼Œæ¯”å¦‚æ¸…é™¤ç¼“å­˜ç­‰
        # ç›®å‰ç®€å•è¿”å›æˆåŠŸå“åº”
        return jsonify({
            'success': True,
            'message': 'æ•°æ®åˆ·æ–°æˆåŠŸ'
        })
    
    except Exception as e:
        logger.error(f"æ•°æ®åˆ·æ–°å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/migration_preview', methods=['POST'])
def api_migration_preview():
    """è¿ç§»é¢„è§ˆ"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        source_task = data.get('source_task', '').strip()
        target_task = data.get('target_task', '').strip()
        
        if not source_task or not target_task:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: source_task å’Œ target_task'}), 400
        
        # è·å–æºä»»åŠ¡çš„è®ºæ–‡åˆ—è¡¨ä½œä¸ºé¢„è§ˆ
        papers = paper_explore_service.get_papers_by_task_name(source_task, page=1, per_page=50)
        
        preview_info = {
            'source_task': source_task,
            'target_task': target_task,
            'papers_count': papers[1] if isinstance(papers, tuple) else 0,
            'preview_papers': []
        }
        
        if isinstance(papers, tuple) and papers[0]:
            preview_info['preview_papers'] = [
                {
                    'arxiv_id': paper.get('arxiv_id'),
                    'title': paper.get('title', '')[:100] + '...' if len(paper.get('title', '')) > 100 else paper.get('title', ''),
                    'authors': paper.get('authors', '')
                }
                for paper in papers[0][:10]  # åªæ˜¾ç¤ºå‰10ç¯‡ä½œä¸ºé¢„è§ˆ
            ]
        
        return jsonify({
            'success': True,
            'data': preview_info
        })
    
    except Exception as e:
        logger.error(f"è¿ç§»é¢„è§ˆå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/one_click_upload', methods=['POST'])
def api_one_click_upload():
    """ä¸€é”®ä¸Šä¼ ï¼ˆæ‰¹é‡ä¸Šä¼ åˆ°Difyï¼‰"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        # è·å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡
        arxiv_ids = data.get('arxiv_ids', [])
        
        if not arxiv_ids:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šè®ºæ–‡IDï¼Œåˆ™ä¸Šä¼ æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡
            eligible_papers = paper_explore_service.get_eligible_papers_for_upload()
            arxiv_ids = [paper.get('arxiv_id') for paper in eligible_papers if paper.get('arxiv_id')]
        
        if not arxiv_ids:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡'}), 400
        
        # æ‰§è¡Œæ‰¹é‡ä¸Šä¼ 
        uploaded_count = 0
        errors = []
        
        for arxiv_id in arxiv_ids:
            try:
                result = dify_service.upload_paper_to_dify(arxiv_id)
                if result.get('success'):
                    uploaded_count += 1
                else:
                    errors.append(f"è®ºæ–‡ {arxiv_id} ä¸Šä¼ å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            except Exception as e:
                errors.append(f"è®ºæ–‡ {arxiv_id} ä¸Šä¼ å¤±è´¥: {str(e)}")
        
        return jsonify({
            'success': True,
            'uploaded_count': uploaded_count,
            'total_requested': len(arxiv_ids),
            'errors': errors,
            'message': f'æˆåŠŸä¸Šä¼  {uploaded_count}/{len(arxiv_ids)} ç¯‡è®ºæ–‡åˆ°Dify'
        })
    
    except Exception as e:
        logger.error(f"ä¸€é”®ä¸Šä¼ å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# === ä¸‹è½½åˆ†æç›¸å…³API ===

@api_bp.route('/paper/<arxiv_id>/download_analysis')
def api_download_analysis(arxiv_id):
    """APIæ¥å£ - ä¸‹è½½åˆ†æç»“æœï¼ˆMarkdown + å›¾ç‰‡æ‰“åŒ…ä¸ºZIPï¼‰"""
    try:
        # è·å–åˆ†æç»“æœ
        result = analysis_service.get_analysis_result(arxiv_id)
        
        if not result['success']:
            return jsonify({
                'success': False,
                'error': 'åˆ†æç»“æœä¸å­˜åœ¨'
            }), 404
        
        # åˆ›å»ºä¸´æ—¶ZIPæ–‡ä»¶
        temp_zip = tempfile.NamedTemporaryFile(delete=False, suffix='.zip')
        
        try:
            with zipfile.ZipFile(temp_zip.name, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                # æ·»åŠ Markdownæ–‡ä»¶
                markdown_content = result['content']
                
                # å¤„ç†å›¾ç‰‡è·¯å¾„ï¼Œè½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
                processed_markdown = _process_markdown_for_download(markdown_content, arxiv_id)
                
                zip_file.writestr(f"{arxiv_id}_analysis.md", processed_markdown)
                
                # æ·»åŠ å›¾ç‰‡æ–‡ä»¶
                images_dir = os.path.join(PROJECT_ROOT, "data/paper_analyze", arxiv_id, "imgs")
                if os.path.exists(images_dir):
                    for filename in os.listdir(images_dir):
                        if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                            image_path = os.path.join(images_dir, filename)
                            if os.path.isfile(image_path):
                                zip_file.write(image_path, f"imgs/{filename}")
            
            # è¿”å›ZIPæ–‡ä»¶
            return send_file(
                temp_zip.name,
                as_attachment=True,
                download_name=f"{arxiv_id}_deep_analysis.zip",
                mimetype='application/zip'
            )
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆåœ¨å‘é€åä¼šè¢«è‡ªåŠ¨åˆ é™¤ï¼‰
            pass
            
    except Exception as e:
        logger.error(f"ä¸‹è½½åˆ†æç»“æœå¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"ä¸‹è½½å¤±è´¥: {str(e)}"
        }), 500


def _process_markdown_for_download(content: str, arxiv_id: str) -> str:
    """
    å¤„ç†Markdownå†…å®¹ï¼Œå°†ç½‘é¡µURLè·¯å¾„è½¬æ¢ä¸ºæœ¬åœ°ç›¸å¯¹è·¯å¾„
    
    Args:
        content: åŸå§‹Markdownå†…å®¹
        arxiv_id: ArXivè®ºæ–‡ID
        
    Returns:
        str: å¤„ç†åçš„Markdownå†…å®¹
    """
    try:
        # å°†ç½‘é¡µURLè·¯å¾„è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
        pattern = rf'/paper/{re.escape(arxiv_id)}/imgs/([^)]+)'
        replacement = r'imgs/\1'
        
        processed_content = re.sub(pattern, replacement, content)
        
        return processed_content
        
    except Exception as e:
        logger.error(f"å¤„ç†Markdownä¸‹è½½å†…å®¹å¤±è´¥: {e}")
        return content


# === ä¸­æ–‡æœç´¢åŠ©æ‰‹API ===

@api_bp.route('/search/translate', methods=['POST'])
def translate_chinese_search():
    """ä¸­æ–‡æœç´¢éœ€æ±‚è½¬æ¢ä¸ºè‹±æ–‡æœç´¢å…³é”®è¯å’Œéœ€æ±‚æè¿°"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æä¾›JSONæ•°æ®'
            }), 400
        
        chinese_input = data.get('chinese_input', '').strip()
        model_name = data.get('model_name', 'ollama.Qwen3_30B')
        
        if not chinese_input:
            return jsonify({
                'success': False,
                'error': 'è¯·è¾“å…¥ä¸­æ–‡æœç´¢éœ€æ±‚'
            }), 400
        
        # å¯¼å…¥å¹¶åˆ›å»ºä¸­æ–‡æœç´¢åŠ©æ‰‹
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
        from HomeSystem.workflow.paper_gather_task.chinese_search_assistant import ChineseSearchAssistantLLM
        
        assistant = ChineseSearchAssistantLLM(model_name=model_name)
        result = assistant.convert_chinese_to_english_search(chinese_input)
        
        return jsonify({
            'success': True,
            'data': {
                'search_keywords': result.search_keywords,
                'user_requirements': result.user_requirements,
                'suggested_task_name': result.suggested_task_name,
                'confidence': result.confidence,
                'notes': result.notes,
                'model_used': model_name
            }
        })
    
    except Exception as e:
        logger.error(f"ä¸­æ–‡æœç´¢è½¬æ¢å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"è½¬æ¢å¤±è´¥: {str(e)}"
        }), 500


# === DifyçŸ¥è¯†åº“ç›¸å…³API ===

@api_bp.route('/dify_upload_all_eligible', methods=['POST'])
def api_dify_upload_all_eligible():
    """ä¸€é”®ä¸Šä¼ å…¨éƒ¨ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡åˆ°DifyçŸ¥è¯†åº“"""
    try:
        data = request.get_json() if request.is_json else {}
        filters = data.get('filters', {})
        
        # è°ƒç”¨æ‰¹é‡ä¸Šä¼ æœåŠ¡
        result = dify_service.upload_all_eligible_papers_with_summary(filters)
        
        if result.get('success'):
            return jsonify(result)
        else:
            return jsonify(result), 400
            
    except Exception as e:
        logger.error(f"ä¸€é”®ä¸Šä¼ å…¨éƒ¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"ä¸Šä¼ å¤±è´¥: {str(e)}",
            'total_eligible': 0,
            'success_count': 0,
            'failed_count': 0,
            'progress': 0
        }), 500


@api_bp.route('/dify_batch_verify', methods=['POST'])
def api_dify_batch_verify():
    """ä¸€é”®éªŒè¯çŸ¥è¯†åº“ä¸­æ‰€æœ‰æ–‡æ¡£çš„çŠ¶æ€"""
    try:
        # è°ƒç”¨æ‰¹é‡éªŒè¯æœåŠ¡
        result = dify_service.batch_verify_all_documents()
        
        if result.get('success'):
            return jsonify(result)
        else:
            return jsonify(result), 400
            
    except Exception as e:
        logger.error(f"ä¸€é”®éªŒè¯çŸ¥è¯†åº“å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"éªŒè¯å¤±è´¥: {str(e)}",
            'total': 0,
            'verified': 0,
            'failed': 0,
            'missing': 0,
            'progress': 0
        }), 500


@api_bp.route('/dify_upload/<arxiv_id>', methods=['POST'])
def api_dify_upload_single(arxiv_id):
    """ä¸Šä¼ å•ä¸ªè®ºæ–‡åˆ°DifyçŸ¥è¯†åº“"""
    try:
        result = dify_service.upload_paper_to_dify(arxiv_id)
        
        if result.get('success'):
            return jsonify(result)
        else:
            return jsonify(result), 400
            
    except Exception as e:
        logger.error(f"ä¸Šä¼ è®ºæ–‡åˆ°Difyå¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"ä¸Šä¼ å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/dify_remove/<arxiv_id>', methods=['POST', 'DELETE'])
def api_dify_remove_single(arxiv_id):
    """ç§»é™¤å•ä¸ªè®ºæ–‡ä»DifyçŸ¥è¯†åº“ï¼ˆæ— /api/å‰ç¼€ç‰ˆæœ¬ï¼‰"""
    return api_dify_remove_single_paper(arxiv_id)


@api_bp.route('/dify_status/<arxiv_id>')
def api_dify_status(arxiv_id):
    """æ£€æŸ¥å•ä¸ªè®ºæ–‡åœ¨Difyä¸­çš„çŠ¶æ€"""
    try:
        result = dify_service.verify_dify_document(arxiv_id)
        
        if result.get('success'):
            return jsonify(result)
        else:
            return jsonify(result), 404
    except Exception as e:
        logger.error(f"æ£€æŸ¥DifyçŠ¶æ€å¤±è´¥ {arxiv_id}: {e}")
        return jsonify({
            'success': False,
            'error': f"çŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}"
        }), 500

@api_bp.route('/api/upload_to_dify', methods=['POST'])
def api_upload_to_dify():
    """æ‰¹é‡ä¸Šä¼ è®ºæ–‡åˆ°DifyçŸ¥è¯†åº“ - å…¼å®¹å‰ç«¯è°ƒç”¨"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_ids = data.get('arxiv_ids', [])
        if not arxiv_ids:
            return jsonify({'success': False, 'error': 'æœªæä¾›è®ºæ–‡IDåˆ—è¡¨'}), 400
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªIDï¼Œç›´æ¥è°ƒç”¨å•ç¯‡ä¸Šä¼ 
        if len(arxiv_ids) == 1:
            result = dify_service.upload_paper_to_dify(arxiv_ids[0])
            return jsonify(result)
        
        # å¤šä¸ªIDçš„æƒ…å†µï¼Œè°ƒç”¨æ‰¹é‡ä¸Šä¼ 
        results = {
            'success_count': 0,
            'failed_count': 0,
            'results': []
        }
        
        for arxiv_id in arxiv_ids:
            try:
                result = dify_service.upload_paper_to_dify(arxiv_id)
                if result.get('success'):
                    results['success_count'] += 1
                    results['results'].append({
                        'arxiv_id': arxiv_id,
                        'status': 'success',
                        'message': 'ä¸Šä¼ æˆåŠŸ'
                    })
                else:
                    results['failed_count'] += 1
                    results['results'].append({
                        'arxiv_id': arxiv_id,
                        'status': 'failed',
                        'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
                    })
            except Exception as e:
                results['failed_count'] += 1
                results['results'].append({
                    'arxiv_id': arxiv_id,
                    'status': 'failed',
                    'error': str(e)
                })
        
        # å¦‚æœå…¨éƒ¨æˆåŠŸ
        if results['failed_count'] == 0:
            return jsonify({
                'success': True,
                'message': f'æˆåŠŸä¸Šä¼ {results["success_count"]}ç¯‡è®ºæ–‡',
                'data': results
            })
        else:
            return jsonify({
                'success': False,
                'error': f'ä¸Šä¼ å¤±è´¥{results["failed_count"]}ç¯‡ï¼ŒæˆåŠŸ{results["success_count"]}ç¯‡',
                'data': results
            }), 400
            
    except Exception as e:
        logger.error(f"æ‰¹é‡ä¸Šä¼ è®ºæ–‡åˆ°Difyå¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@api_bp.route('/api/remove_from_dify', methods=['POST'])
def api_remove_from_dify():
    """ä»DifyçŸ¥è¯†åº“ç§»é™¤è®ºæ–‡ - å…¼å®¹å‰ç«¯è°ƒç”¨"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_ids = data.get('arxiv_ids', [])
        if not arxiv_ids:
            return jsonify({'success': False, 'error': 'æœªæä¾›è®ºæ–‡IDåˆ—è¡¨'}), 400
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç§»é™¤åŠŸèƒ½
        if not hasattr(dify_service, 'remove_paper_from_dify'):
            return jsonify({
                'success': False,
                'error': 'æš‚ä¸æ”¯æŒä»Difyç§»é™¤è®ºæ–‡åŠŸèƒ½'
            }), 501
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªID
        if len(arxiv_ids) == 1:
            result = dify_service.remove_paper_from_dify(arxiv_ids[0])
            return jsonify(result)
        
        # å¤šä¸ªIDçš„æƒ…å†µ
        results = {
            'success_count': 0,
            'failed_count': 0,
            'results': []
        }
        
        for arxiv_id in arxiv_ids:
            try:
                result = dify_service.remove_paper_from_dify(arxiv_id)
                if result.get('success'):
                    results['success_count'] += 1
                    results['results'].append({
                        'arxiv_id': arxiv_id,
                        'status': 'success',
                        'message': 'ç§»é™¤æˆåŠŸ'
                    })
                else:
                    results['failed_count'] += 1
                    results['results'].append({
                        'arxiv_id': arxiv_id,
                        'status': 'failed',
                        'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
                    })
            except Exception as e:
                results['failed_count'] += 1
                results['results'].append({
                    'arxiv_id': arxiv_id,
                    'status': 'failed',
                    'error': str(e)
                })
        
        # è¿”å›ç»“æœ
        if results['failed_count'] == 0:
            return jsonify({
                'success': True,
                'message': f'æˆåŠŸç§»é™¤{results["success_count"]}ç¯‡è®ºæ–‡',
                'data': results
            })
        else:
            return jsonify({
                'success': False,
                'error': f'ç§»é™¤å¤±è´¥{results["failed_count"]}ç¯‡ï¼ŒæˆåŠŸ{results["success_count"]}ç¯‡',
                'data': results
            }), 400
            
    except Exception as e:
        logger.error(f"ä»Difyç§»é™¤è®ºæ–‡å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/dify_statistics')
def api_dify_statistics():
    """è·å–DifyçŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # è·å–åŸºæœ¬ç»Ÿè®¡
        eligible_papers = dify_service.get_eligible_papers_for_upload()
        
        # è®¡ç®—å·²ä¸Šä¼ çš„è®ºæ–‡æ•°é‡
        all_papers = paper_explore_service.get_paper_statistics()
        uploaded_count = all_papers.get('dify_uploaded', 0) if all_papers else 0
        
        statistics = {
            'total_papers': all_papers.get('total_papers', 0) if all_papers else 0,
            'eligible_for_upload': len(eligible_papers),
            'already_uploaded': uploaded_count,
            'dify_service_available': dify_service.is_available()
        }
        
        return jsonify({
            'success': True,
            'data': statistics
        })
        
    except Exception as e:
        logger.error(f"è·å–Difyç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}"
        }), 500


@api_bp.route('/dify_upload/<arxiv_id>', methods=['POST'])
def api_dify_upload_single_paper(arxiv_id):
    """å•ä¸ªè®ºæ–‡ä¸Šä¼ åˆ°DifyçŸ¥è¯†åº“ - å…¼å®¹ExplorePaperDataè°ƒç”¨æ¨¡å¼"""
    try:
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
        
        # è°ƒç”¨shared serviceä¸Šä¼ 
        result = dify_service.upload_paper_to_dify(arxiv_id)
        
        if result.get('success'):
            return jsonify({
                'success': True,
                'message': 'è®ºæ–‡ä¸Šä¼ æˆåŠŸ',
                'data': {
                    'arxiv_id': arxiv_id,
                    'dataset_id': result.get('dataset_id'),
                    'document_id': result.get('document_id'),
                    'document_name': result.get('document_name')
                }
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'ä¸Šä¼ å¤±è´¥')
            }), 400
            
    except Exception as e:
        logger.error(f"ä¸Šä¼ å•ä¸ªè®ºæ–‡åˆ°Difyå¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/dify_remove/<arxiv_id>', methods=['POST', 'DELETE'])
def api_dify_remove_single_paper(arxiv_id):
    """ä»DifyçŸ¥è¯†åº“ç§»é™¤å•ä¸ªè®ºæ–‡ - å…¼å®¹ExplorePaperDataè°ƒç”¨æ¨¡å¼"""
    try:
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
        
        # è°ƒç”¨shared serviceç§»é™¤
        result = dify_service.remove_paper_from_dify(arxiv_id)
        
        if result.get('success'):
            return jsonify({
                'success': True,
                'message': 'è®ºæ–‡ç§»é™¤æˆåŠŸ',
                'data': {
                    'arxiv_id': arxiv_id
                }
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'ç§»é™¤å¤±è´¥')
            }), 400
            
    except Exception as e:
        logger.error(f"ä»Difyç§»é™¤å•ä¸ªè®ºæ–‡å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/dify_verify/<arxiv_id>', methods=['POST'])
def api_dify_verify_single_paper(arxiv_id):
    """éªŒè¯è®ºæ–‡æ˜¯å¦å­˜åœ¨äºDifyæœåŠ¡å™¨ - å…¼å®¹ExplorePaperDataè°ƒç”¨æ¨¡å¼"""
    try:
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
        
        # è°ƒç”¨shared serviceéªŒè¯
        result = dify_service.verify_dify_document(arxiv_id)
        
        if result.get('success'):
            return jsonify({
                'success': True,
                'data': result
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'éªŒè¯å¤±è´¥')
            }), 404
            
    except Exception as e:
        logger.error(f"éªŒè¯Difyæ–‡æ¡£å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/dify_clean/<arxiv_id>', methods=['POST'])
def api_dify_clean_single_paper(arxiv_id):
    """æ¸…ç†æ— æ•ˆçš„Difyæ–‡æ¡£è®°å½• - å…¼å®¹ExplorePaperDataè°ƒç”¨æ¨¡å¼"""
    try:
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
        
        # è·å–è®ºæ–‡æ•°æ®å¹¶æ¸…ç†æ— æ•ˆè®°å½•
        paper_dict = dify_service._get_paper_data(arxiv_id)
        if not paper_dict:
            return jsonify({'success': False, 'error': 'è®ºæ–‡ä¸å­˜åœ¨'}), 400
        
        # æ¸…ç†æ•°æ®åº“ä¸­çš„Difyä¿¡æ¯
        if dify_service.db_ops:
            paper = dify_service.db_ops.get_by_field(ArxivPaperModel, 'arxiv_id', arxiv_id)
            if paper:
                arxiv_paper = paper if isinstance(paper, ArxivPaperModel) else ArxivPaperModel.from_dict(paper.to_dict())
                arxiv_paper.clear_dify_info()
                
                clear_data = {
                    'dify_dataset_id': arxiv_paper.dify_dataset_id,
                    'dify_document_id': arxiv_paper.dify_document_id,
                    'dify_document_name': arxiv_paper.dify_document_name,
                    'dify_character_count': arxiv_paper.dify_character_count,
                    'dify_segment_count': arxiv_paper.dify_segment_count,
                    'dify_upload_time': arxiv_paper.dify_upload_time,
                    'dify_metadata': json.dumps(arxiv_paper.dify_metadata) if arxiv_paper.dify_metadata else '{}'
                }
                
                success = dify_service.db_ops.update(arxiv_paper, clear_data)
                if success:
                    return jsonify({
                        'success': True,
                        'message': 'æ— æ•ˆè®°å½•å·²æ¸…ç†',
                        'data': {'arxiv_id': arxiv_id}
                    })
                else:
                    return jsonify({'success': False, 'error': 'æ¸…ç†å¤±è´¥'}), 500
            else:
                return jsonify({'success': False, 'error': 'è®ºæ–‡ä¸å­˜åœ¨'}), 400
        else:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“æœåŠ¡ä¸å¯ç”¨'}), 500
            
    except Exception as e:
        logger.error(f"æ¸…ç†Difyè®°å½•å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_bp.route('/dify_validate_upload/<arxiv_id>')
def api_dify_validate_upload_single_paper(arxiv_id):
    """éªŒè¯è®ºæ–‡ä¸Šä¼ å‰ç½®æ¡ä»¶ - å…¼å®¹ExplorePaperDataè°ƒç”¨æ¨¡å¼"""
    try:
        if not arxiv_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è®ºæ–‡ID'}), 400
        
        # è°ƒç”¨shared serviceéªŒè¯ä¸Šä¼ å‰ç½®æ¡ä»¶
        result = dify_service.validate_upload_preconditions(arxiv_id)
        
        return jsonify(result)
            
    except Exception as e:
        logger.error(f"éªŒè¯ä¸Šä¼ å‰ç½®æ¡ä»¶å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ========== è®ºæ–‡åˆ›å»ºç›¸å…³API ==========

@api_bp.route('/create_paper_from_pdf', methods=['POST'])
def api_create_paper_from_pdf():
    """ä»PDFæ–‡ä»¶åˆ›å»ºè®ºæ–‡"""
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸Šä¼ çš„æ–‡ä»¶
        if 'pdf_file' not in request.files:
            return jsonify({'success': False, 'error': 'æœªä¸Šä¼ PDFæ–‡ä»¶'}), 400
            
        pdf_file = request.files['pdf_file']
        if pdf_file.filename == '':
            return jsonify({'success': False, 'error': 'PDFæ–‡ä»¶åä¸ºç©º'}), 400
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not pdf_file.filename.lower().endswith('.pdf'):
            return jsonify({'success': False, 'error': 'ä»…æ”¯æŒPDFæ–‡ä»¶'}), 400
        
        # è·å–å¯é€‰å‚æ•°
        task_name = request.form.get('task_name', '').strip() or None
        task_id = request.form.get('task_id', '').strip() or None
        
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from HomeSystem.utility.arxiv.arxiv import ArxivData
        from HomeSystem.integrations.database import DatabaseOperations, ArxivPaperModel
        import tempfile
        import os
        import uuid
        from datetime import datetime
        
        # ä¿å­˜ä¸Šä¼ çš„PDFåˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_pdf:
            pdf_file.save(temp_pdf.name)
            temp_pdf_path = temp_pdf.name
        
        try:
            # åº”ç”¨è¿œç¨‹OCRé…ç½®ï¼ˆåœ¨OCRå¤„ç†ä¹‹å‰ï¼‰
            ocr_config = apply_remote_ocr_config()
            
            # å…ˆç”Ÿæˆå”¯ä¸€çš„arxiv_idï¼ˆä½¿ç”¨ç®€åŒ–æ ¼å¼åŠ ä¸Štimestampï¼‰
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            generated_id = f"manual_{timestamp}_{str(uuid.uuid4())[:8]}"
            
            # åˆ›å»ºArxivDataå¯¹è±¡å¹¶è®¾ç½®arxiv_id
            arxiv_data = ArxivData()
            arxiv_data.arxiv_id = generated_id
            
            # è·å–æ ‡å‡†ç›®å½•è·¯å¾„å¹¶åˆ›å»ºç›®å½•
            pdf_dir = arxiv_data.get_paper_directory()
            pdf_dir.mkdir(parents=True, exist_ok=True)
            
            # è®¾ç½®PDFè·¯å¾„ï¼Œå¯ç”¨å…ƒæ•°æ®æå–
            arxiv_data.set_pdf_path(temp_pdf_path, extract_metadata=True)
            
            # æ‰‹åŠ¨æ‰§è¡ŒOCRå¹¶ä¿å­˜åˆ°æ­£ç¡®ä½ç½®
            logger.info(f"æ‰§è¡ŒOCRå¤„ç†ï¼Œä¿å­˜åˆ°: {pdf_dir}")
            
            # æ ¹æ®OCRç±»å‹å†³å®šå¤„ç†é¡µæ•°
            if ocr_config.get('enable_remote_ocr', False):
                ocr_max_pages = ocr_config.get('remote_ocr_max_pages', 25)
                logger.info(f"ä½¿ç”¨è¿œç¨‹OCRå¤„ç† {ocr_max_pages} é¡µ")
            else:
                ocr_max_pages = 25  # æœ¬åœ°OCRä¹Ÿä½¿ç”¨25é¡µï¼Œå› ä¸ºè¿™æ˜¯å®Œæ•´å¤„ç†
                logger.info(f"ä½¿ç”¨æœ¬åœ°OCRå¤„ç† {ocr_max_pages} é¡µ")
            
            ocr_result, ocr_status = arxiv_data.performOCR(
                max_pages=ocr_max_pages,
                use_paddleocr=True,
                use_remote_ocr=ocr_config.get('enable_remote_ocr', False),
                auto_save=True,
                save_path=str(pdf_dir)
            )
            
            if ocr_result:
                logger.info(f"OCRå¤„ç†æˆåŠŸï¼Œæå– {len(ocr_result)} å­—ç¬¦")
            else:
                logger.warning("OCRå¤„ç†å¤±è´¥æˆ–æœªæå–åˆ°å†…å®¹")
            
            # è®¾ç½®åŸºæœ¬å±æ€§
            if not arxiv_data.title:
                arxiv_data.title = f"Manual Upload - {pdf_file.filename}"
            
            # åˆ›å»ºæ•°æ®åº“æ¨¡å‹
            paper_model = ArxivPaperModel()
            paper_model.arxiv_id = generated_id
            paper_model.title = arxiv_data.title or f"Manual Upload - {pdf_file.filename}"
            paper_model.authors = arxiv_data.authors or ""
            paper_model.abstract = arxiv_data.snippet or ""
            paper_model.categories = "manual_upload"
            paper_model.published_date = datetime.now().strftime('%Y-%m-%d')
            paper_model.pdf_url = ""  # æ²¡æœ‰ArXiv URL
            paper_model.processing_status = "completed"
            paper_model.task_name = task_name
            paper_model.task_id = task_id
            
            # æ·»åŠ å…ƒæ•°æ®æ ‡è®°è¿™æ˜¯æ‰‹åŠ¨ä¸Šä¼ 
            paper_model.add_metadata('source', 'manual_upload')
            paper_model.add_metadata('original_filename', pdf_file.filename)
            paper_model.add_metadata('upload_timestamp', datetime.now().isoformat())
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            from HomeSystem.integrations.database import DatabaseOperations
            db_ops = DatabaseOperations()
            success = db_ops.create(paper_model)
            
            if success:
                # å¤åˆ¶ä¸´æ—¶PDFåˆ°æ ‡å‡†ç›®å½•ï¼ˆPDFç›®å½•å·²åœ¨å‰é¢åˆ›å»ºï¼‰
                try:
                    pdf_file_path = pdf_dir / f"{generated_id}.pdf"
                    import shutil
                    shutil.copy2(temp_pdf_path, pdf_file_path)
                    logger.info(f"PDFå·²ä¿å­˜åˆ°æ ‡å‡†ç›®å½•: {pdf_file_path}")
                    
                except Exception as e:
                    logger.warning(f"ä¿å­˜PDFåˆ°æ ‡å‡†ç›®å½•å¤±è´¥: {e}")
                    # ä¸å½±å“ä¸»æµç¨‹ï¼Œåªè®°å½•è­¦å‘Š
                
                return jsonify({
                    'success': True,
                    'arxiv_id': generated_id,
                    'title': paper_model.title,
                    'authors': paper_model.authors,
                    'abstract': paper_model.abstract,
                    'message': 'è®ºæ–‡åˆ›å»ºæˆåŠŸ',
                    'redirect_url': f'/explore/paper/{generated_id}'
                })
            else:
                return jsonify({'success': False, 'error': 'æ•°æ®åº“ä¿å­˜å¤±è´¥'}), 500
                
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_pdf_path):
                os.unlink(temp_pdf_path)
            
    except Exception as e:
        logger.error(f"ä»PDFåˆ›å»ºè®ºæ–‡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_bp.route('/create_paper_from_arxiv', methods=['POST'])
def api_create_paper_from_arxiv():
    """ä»ArXiv URLåˆ›å»ºè®ºæ–‡"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400
        
        arxiv_input = data.get('arxiv_input', '').strip()
        if not arxiv_input:
            return jsonify({'success': False, 'error': 'è¯·è¾“å…¥ArXivé“¾æ¥æˆ–ID'}), 400
        
        task_name = data.get('task_name', '').strip() or None
        task_id = data.get('task_id', '').strip() or None
        
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from HomeSystem.utility.arxiv.arxiv import ArxivData
        from HomeSystem.integrations.database import DatabaseOperations, ArxivPaperModel
        from datetime import datetime
        
        # è§„èŒƒåŒ–ArXivè¾“å…¥ä¸ºå®Œæ•´URL
        if not arxiv_input.startswith('http'):
            # å¦‚æœåªæ˜¯IDï¼Œè½¬æ¢ä¸ºå®Œæ•´URL
            arxiv_input = f"https://arxiv.org/abs/{arxiv_input}"
        
        # åˆ›å»ºArxivDataå¯¹è±¡
        arxiv_data = ArxivData()
        
        # ä»ArXivé“¾æ¥è·å–å…ƒæ•°æ®
        success = arxiv_data.populate_from_arxiv_link(arxiv_input)
        if not success:
            return jsonify({'success': False, 'error': 'æ— æ³•ä»ArXivè·å–è®ºæ–‡ä¿¡æ¯ï¼Œè¯·æ£€æŸ¥é“¾æ¥æˆ–IDæ˜¯å¦æ­£ç¡®'}), 400
        
        # æ£€æŸ¥è®ºæ–‡æ˜¯å¦å·²å­˜åœ¨
        db_ops = DatabaseOperations()
        if arxiv_data.arxiv_id:
            existing_paper = db_ops.get_by_field(ArxivPaperModel, 'arxiv_id', arxiv_data.arxiv_id)
        else:
            existing_paper = None
        if existing_paper:
            return jsonify({
                'success': False, 
                'error': f'è®ºæ–‡å·²å­˜åœ¨: {arxiv_data.arxiv_id}',
                'existing_paper': {
                    'arxiv_id': existing_paper.arxiv_id,
                    'title': existing_paper.title,
                    'redirect_url': f'/explore/paper/{existing_paper.arxiv_id}'
                }
            }), 409
        
        # åº”ç”¨è¿œç¨‹OCRé…ç½®ï¼ˆåœ¨PDFä¸‹è½½å’Œæ½œåœ¨OCRå¤„ç†ä¹‹å‰ï¼‰
        ocr_config = apply_remote_ocr_config()
        
        # ä¸‹è½½PDFåˆ°æ ‡å‡†ç›®å½•
        try:
            if arxiv_data.pdf_link:
                arxiv_data.downloadPdf(use_standard_path=True, check_existing=True)
                logger.info(f"PDFä¸‹è½½æˆåŠŸ: {arxiv_data.arxiv_id}")
        except Exception as e:
            logger.warning(f"PDFä¸‹è½½å¤±è´¥: {e}")
            # ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­åˆ›å»ºè®ºæ–‡è®°å½•
        
        # åˆ›å»ºæ•°æ®åº“æ¨¡å‹
        paper_model = ArxivPaperModel()
        paper_model.arxiv_id = arxiv_data.arxiv_id or ""
        paper_model.title = arxiv_data.title or "Untitled"
        paper_model.authors = arxiv_data.authors or ""
        paper_model.abstract = arxiv_data.snippet or ""
        paper_model.categories = arxiv_data.categories or "Unknown"
        paper_model.published_date = arxiv_data.published_date or ""
        paper_model.pdf_url = arxiv_data.pdf_link or ""
        paper_model.processing_status = "completed"
        paper_model.task_name = task_name
        paper_model.task_id = task_id
        
        # æ·»åŠ å…ƒæ•°æ®æ ‡è®°è¿™æ˜¯ä»ArXivåˆ›å»º
        paper_model.add_metadata('source', 'arxiv_manual_add')
        paper_model.add_metadata('creation_timestamp', datetime.now().isoformat())
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        success = db_ops.create(paper_model)
        
        if success:
            return jsonify({
                'success': True,
                'arxiv_id': paper_model.arxiv_id,
                'title': paper_model.title,
                'authors': paper_model.authors,
                'abstract': paper_model.abstract,
                'message': 'è®ºæ–‡åˆ›å»ºæˆåŠŸ',
                'redirect_url': f'/explore/paper/{paper_model.arxiv_id}'
            })
        else:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“ä¿å­˜å¤±è´¥'}), 500
            
    except Exception as e:
        logger.error(f"ä»ArXivåˆ›å»ºè®ºæ–‡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500