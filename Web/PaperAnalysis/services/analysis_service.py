"""
æ·±åº¦è®ºæ–‡åˆ†ææœåŠ¡
å¤„ç†è®ºæ–‡æ·±åº¦åˆ†æçš„ä¸šåŠ¡é€»è¾‘
"""
import os
import sys
import threading
import time
import logging
import re
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
from datetime import datetime

# æ·»åŠ  HomeSystem æ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from .paper_explore_service import PaperService

# å¯¼å…¥ArXivç›¸å…³æ¨¡å—ç”¨äºè®ºæ–‡ä¸‹è½½å’ŒOCRå¤„ç†
from HomeSystem.utility.arxiv.arxiv import ArxivData

logger = logging.getLogger(__name__)


class DeepAnalysisService:
    """æ·±åº¦è®ºæ–‡åˆ†ææœåŠ¡ç±»"""
    
    def __init__(self, paper_service: PaperService, redis_client=None):
        self.paper_service = paper_service
        self.analysis_threads = {}  # å­˜å‚¨æ­£åœ¨è¿›è¡Œçš„åˆ†æçº¿ç¨‹
        self.correction_threads = {}  # å­˜å‚¨æ­£åœ¨è¿›è¡Œçš„å…¬å¼çº é”™çº¿ç¨‹
        self.redis_client = redis_client  # Rediså®¢æˆ·ç«¯ç”¨äºè¯»å–é…ç½®
        
        # é»˜è®¤é…ç½®
        self.default_config = {
            'analysis_model': 'deepseek.DeepSeek_V3',
            'vision_model': 'ollama.Qwen2_5_VL_7B',
            'timeout': 600  # 10åˆ†é’Ÿè¶…æ—¶
        }
    
    def load_config(self) -> Dict[str, Any]:
        """
        ä»RedisåŠ è½½é…ç½®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        
        Returns:
            Dict: é…ç½®å­—å…¸
        """
        config = self.default_config.copy()
        
        if self.redis_client:
            try:
                config_key = "analysis_config:global"
                saved_config = self.redis_client.get(config_key)
                if saved_config:
                    import json
                    saved_data = json.loads(saved_config)
                    config.update(saved_data)
                    logger.info(f"ä»RedisåŠ è½½é…ç½®: {config}")
                else:
                    logger.info("Redisä¸­æœªæ‰¾åˆ°é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            except Exception as e:
                logger.warning(f"ä»RedisåŠ è½½é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        else:
            logger.info("Redisä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        return config
    
    def start_analysis(self, arxiv_id: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        å¯åŠ¨è®ºæ–‡æ·±åº¦åˆ†æ
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            config: åˆ†æé…ç½®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict: æ“ä½œç»“æœ
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹å¯åŠ¨æ·±åº¦åˆ†æ - ArXiv ID: {arxiv_id}")
            
            # æ£€æŸ¥è®ºæ–‡æ˜¯å¦å­˜åœ¨
            paper = self.paper_service.get_paper_detail(arxiv_id)
            if not paper:
                logger.error(f"âŒ è®ºæ–‡ä¸å­˜åœ¨: {arxiv_id}")
                return {
                    'success': False,
                    'error': f'è®ºæ–‡ {arxiv_id} ä¸å­˜åœ¨'
                }
            
            logger.info(f"âœ… è®ºæ–‡å­˜åœ¨æ£€æŸ¥é€šè¿‡: {arxiv_id}")
            
            # æ£€æŸ¥æ˜¯å¦å·²åœ¨åˆ†æä¸­
            if arxiv_id in self.analysis_threads:
                thread = self.analysis_threads[arxiv_id]
                if thread.is_alive():
                    logger.warning(f"âš ï¸ è®ºæ–‡å·²åœ¨åˆ†æä¸­: {arxiv_id}")
                    return {
                        'success': False,
                        'error': 'è¯¥è®ºæ–‡æ­£åœ¨åˆ†æä¸­ï¼Œè¯·ç¨å'
                    }
                else:
                    # æ¸…ç†å·²å®Œæˆçš„çº¿ç¨‹
                    del self.analysis_threads[arxiv_id]
                    logger.info(f"ğŸ§¹ æ¸…ç†äº†å·²å®Œæˆçš„çº¿ç¨‹: {arxiv_id}")
            
            # æ›´æ–°åˆ†æçŠ¶æ€ä¸ºå¤„ç†ä¸­
            self.paper_service.update_analysis_status(arxiv_id, 'processing')
            
            # åŠ è½½å½“å‰é…ç½®
            current_config = self.load_config()
            
            # åˆå¹¶é…ç½®ï¼ˆç”¨æˆ·ä¼ å…¥çš„é…ç½®ä¼˜å…ˆï¼‰
            analysis_config = {**current_config, **(config or {})}
            
            # åˆ›å»ºå¹¶å¯åŠ¨åˆ†æçº¿ç¨‹
            thread = threading.Thread(
                target=self._run_analysis,
                args=(arxiv_id, paper, analysis_config),
                daemon=True
            )
            thread.start()
            
            # ä¿å­˜çº¿ç¨‹å¼•ç”¨
            self.analysis_threads[arxiv_id] = thread
            
            logger.info(f"Started deep analysis for paper {arxiv_id}")
            
            return {
                'success': True,
                'message': 'æ·±åº¦åˆ†æå·²å¯åŠ¨',
                'status': 'processing'
            }
            
        except Exception as e:
            logger.error(f"Failed to start analysis for {arxiv_id}: {e}")
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            try:
                self.paper_service.update_analysis_status(arxiv_id, 'failed')
            except:
                pass
            
            return {
                'success': False,
                'error': f'å¯åŠ¨åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _run_analysis(self, arxiv_id: str, paper: Dict[str, Any], config: Dict[str, Any]):
        """
        æ‰§è¡Œè®ºæ–‡åˆ†æï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼‰
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            config: åˆ†æé…ç½®
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ·±åº¦åˆ†æ - ArXiv ID: {arxiv_id}")
            
            # ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡è®ºæ–‡æ•°æ®å’Œæ–‡ä»¶å¤¹
            paper_folder = self._prepare_paper_folder(arxiv_id, paper)
            if not paper_folder:
                logger.error(f"âŒ è®ºæ–‡æ–‡ä»¶å¤¹å‡†å¤‡å¤±è´¥: {arxiv_id}")
                self.paper_service.update_analysis_status(arxiv_id, 'failed')
                return
                
            logger.info(f"âœ… è®ºæ–‡æ–‡ä»¶å¤¹å‡†å¤‡å®Œæˆ: {paper_folder}")
            
            # ç¬¬äºŒæ­¥ï¼šä¸‹è½½è®ºæ–‡PDFï¼ˆå¦‚æœå°šæœªä¸‹è½½ï¼‰
            success = self._download_paper_pdf(arxiv_id, paper, paper_folder)
            if not success:
                logger.error(f"âŒ è®ºæ–‡PDFä¸‹è½½å¤±è´¥: {arxiv_id}")
                self.paper_service.update_analysis_status(arxiv_id, 'failed')
                return
                
            logger.info(f"âœ… è®ºæ–‡PDFä¸‹è½½å®Œæˆ: {arxiv_id}")
            
            # ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡ŒOCRå¤„ç†ï¼ˆå¦‚æœå°šæœªå¤„ç†ï¼‰
            success = self._perform_paper_ocr(arxiv_id, paper_folder)
            if not success:
                logger.error(f"âŒ è®ºæ–‡OCRå¤„ç†å¤±è´¥: {arxiv_id}")
                self.paper_service.update_analysis_status(arxiv_id, 'failed')
                return
                
            logger.info(f"âœ… è®ºæ–‡OCRå¤„ç†å®Œæˆ: {arxiv_id}")
            
            # ç¬¬å››æ­¥ï¼šæ‰§è¡Œæ·±åº¦åˆ†æ
            success = self._execute_deep_analysis(arxiv_id, paper_folder, config)
            if not success:
                logger.error(f"âŒ æ·±åº¦åˆ†ææ‰§è¡Œå¤±è´¥: {arxiv_id}")
                self.paper_service.update_analysis_status(arxiv_id, 'failed')
                return
                
            logger.info(f"âœ… æ·±åº¦åˆ†ææ‰§è¡Œå®Œæˆ: {arxiv_id}")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ åˆ†æè¿‡ç¨‹å¤±è´¥ {arxiv_id}: {e}")
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            try:
                self.paper_service.update_analysis_status(arxiv_id, 'failed')
            except:
                pass
        finally:
            # æ¸…ç†çº¿ç¨‹å¼•ç”¨
            if arxiv_id in self.analysis_threads:
                del self.analysis_threads[arxiv_id]
    
    def _prepare_paper_folder(self, arxiv_id: str, paper: Dict[str, Any]) -> Optional[str]:
        """
        å‡†å¤‡è®ºæ–‡åˆ†ææ–‡ä»¶å¤¹
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            
        Returns:
            str: è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # ä½¿ç”¨ç›¸å¯¹è·¯å¾„è·å–é¡¹ç›®æ ¹ç›®å½•
            project_root = Path(__file__).parent.parent.parent.parent
            paper_folder = project_root / "data" / "paper_analyze" / arxiv_id
            
            # åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            paper_folder.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"ğŸ“ è®ºæ–‡æ–‡ä»¶å¤¹å·²å‡†å¤‡: {paper_folder}")
            return str(paper_folder)
            
        except Exception as e:
            logger.error(f"âŒ å‡†å¤‡è®ºæ–‡æ–‡ä»¶å¤¹å¤±è´¥ {arxiv_id}: {e}")
            return None
    
    def _download_paper_pdf(self, arxiv_id: str, paper: Dict[str, Any], paper_folder: str) -> bool:
        """
        ä¸‹è½½è®ºæ–‡PDFæ–‡ä»¶
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            paper_folder: è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ£€æŸ¥PDFæ˜¯å¦å·²å­˜åœ¨
            pdf_path = os.path.join(paper_folder, f"{arxiv_id}.pdf")
            if os.path.exists(pdf_path):
                logger.info(f"ğŸ“„ PDFæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {pdf_path}")
                return True
            
            # æ„é€ PDFä¸‹è½½URL
            pdf_url = paper.get('pdf_url')
            if not pdf_url:
                # æ„é€ æ ‡å‡†çš„ArXiv PDF URL
                pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                
            logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½PDF: {pdf_url}")
            
            # åˆ›å»ºArxivDataå®ä¾‹å¹¶ä¸‹è½½PDF
            arxiv_data = ArxivData({
                'title': paper.get('title', ''),
                'link': f"https://arxiv.org/abs/{arxiv_id}",
                'snippet': paper.get('abstract', ''),
                'categories': paper.get('categories', ''),
                'arxiv_id': arxiv_id
            })
            
            # ä¸‹è½½PDFåˆ°æŒ‡å®šè·¯å¾„ï¼ˆä¼ é€’ç›®å½•è·¯å¾„ï¼Œè®©downloadPdfè‡ªè¡Œå¤„ç†æ–‡ä»¶åï¼‰
            arxiv_data.downloadPdf(save_path=paper_folder)
            
            # æ£€æŸ¥æ˜¯å¦ä¸‹è½½æˆåŠŸï¼ˆdownloadPdfä¼šæ ¹æ®æ ‡é¢˜åˆ›å»ºæ–‡ä»¶åï¼‰
            # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°å®é™…åˆ›å»ºçš„PDFæ–‡ä»¶
            pdf_files = [f for f in os.listdir(paper_folder) if f.endswith('.pdf')]
            if pdf_files:
                # é‡å‘½åä¸ºæ ‡å‡†æ ¼å¼
                actual_pdf_path = os.path.join(paper_folder, pdf_files[0])
                if actual_pdf_path != pdf_path and os.path.exists(actual_pdf_path):
                    os.rename(actual_pdf_path, pdf_path)
                    logger.info(f"ğŸ“ PDFé‡å‘½åä¸ºæ ‡å‡†æ ¼å¼: {pdf_path}")
            else:
                logger.error(f"âŒ æœªæ‰¾åˆ°ä¸‹è½½çš„PDFæ–‡ä»¶")
                return False
            
            if os.path.exists(pdf_path) and os.path.getsize(pdf_path) > 0:
                logger.info(f"âœ… PDFä¸‹è½½æˆåŠŸ: {pdf_path}")
                return True
            else:
                logger.error(f"âŒ PDFä¸‹è½½å¤±è´¥ï¼Œæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º: {pdf_path}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½PDFå¤±è´¥ {arxiv_id}: {e}")
            return False
    
    def _perform_paper_ocr(self, arxiv_id: str, paper_folder: str) -> bool:
        """
        æ‰§è¡Œè®ºæ–‡OCRå¤„ç†
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            paper_folder: è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            bool: OCRå¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ£€æŸ¥OCRç»“æœæ˜¯å¦å·²å­˜åœ¨
            ocr_file = os.path.join(paper_folder, f"{arxiv_id}_paddleocr.md")
            if os.path.exists(ocr_file):
                logger.info(f"ğŸ“ OCRæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†: {ocr_file}")
                return True
            
            # æ£€æŸ¥PDFæ–‡ä»¶
            pdf_path = os.path.join(paper_folder, f"{arxiv_id}.pdf")
            if not os.path.exists(pdf_path):
                logger.error(f"âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
                return False
            
            logger.info(f"ğŸ” å¼€å§‹OCRå¤„ç†: {pdf_path}")
            
            # åˆ›å»ºArxivDataå®ä¾‹å¹¶æ‰§è¡ŒOCR
            arxiv_data = ArxivData({
                'title': '',
                'link': f"https://arxiv.org/abs/{arxiv_id}",
                'snippet': '',
                'categories': '',
                'arxiv_id': arxiv_id
            })
            
            # ä»æ–‡ä»¶åŠ è½½PDF
            with open(pdf_path, 'rb') as f:
                arxiv_data.pdf = f.read()
            
            # æ‰§è¡ŒPaddleOCRå¤„ç†
            ocr_result, status_info = arxiv_data.performOCR(
                use_paddleocr=True, 
                auto_save=True,
                save_path=paper_folder
            )
            
            if ocr_result and len(ocr_result.strip()) > 0:
                logger.info(f"âœ… OCRå¤„ç†æˆåŠŸï¼Œç”Ÿæˆ {len(ocr_result)} å­—ç¬¦: {arxiv_id}")
                return True
            else:
                logger.error(f"âŒ OCRå¤„ç†å¤±è´¥ï¼Œæœªç”Ÿæˆæœ‰æ•ˆå†…å®¹: {arxiv_id}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ OCRå¤„ç†å¤±è´¥ {arxiv_id}: {e}")
            return False
    
    def _execute_deep_analysis(self, arxiv_id: str, paper_folder: str, config: Dict[str, Any]) -> bool:
        """
        æ‰§è¡Œæ·±åº¦åˆ†æ
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            paper_folder: è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
            config: åˆ†æé…ç½®
            
        Returns:
            bool: åˆ†ææ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ¤– å¼€å§‹æ·±åº¦åˆ†æ: {arxiv_id}")
            
            # åŠ¨æ€å¯¼å…¥æ·±åº¦åˆ†ææ™ºèƒ½ä½“ï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¯åŠ¨æ—¶çš„å…¼å®¹æ€§é—®é¢˜ï¼‰
            try:
                from HomeSystem.graph.deep_paper_analysis_agent import create_deep_paper_analysis_agent
                logger.info("âœ… Successfully imported deep paper analysis agent")
            except Exception as import_error:
                logger.error(f"âŒ Failed to import deep paper analysis agent: {import_error}")
                return False
            
            # åˆ›å»ºæ·±åº¦åˆ†ææ™ºèƒ½ä½“
            logger.info("ğŸ¤– Creating deep paper analysis agent...")
            agent = create_deep_paper_analysis_agent(
                analysis_model=config['analysis_model'],
                vision_model=config['vision_model']
            )
            logger.info("âœ… Deep paper analysis agent created successfully")
            
            # æ‰§è¡Œåˆ†æ
            analysis_result, report_content = agent.analyze_and_generate_report(
                folder_path=paper_folder,
                thread_id=f"web_analysis_{arxiv_id}_{int(time.time())}"
            )
            
            # æ£€æŸ¥åˆ†ææ˜¯å¦æˆåŠŸ
            if 'error' in analysis_result:
                logger.error(f"Analysis failed for {arxiv_id}: {analysis_result['error']}")
                return False
            
            # å¤„ç†åˆ†æç»“æœ
            if analysis_result.get('analysis_result') or report_content:
                # ä½¿ç”¨åˆ†æç»“æœæˆ–æŠ¥å‘Šå†…å®¹
                final_content = analysis_result.get('analysis_result') or report_content
                
                # å¤„ç†å›¾ç‰‡è·¯å¾„
                processed_content = self._process_image_paths(final_content, arxiv_id)
                
                # ä¿å­˜åˆ†æç»“æœ
                self.paper_service.save_analysis_result(arxiv_id, processed_content)
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                analysis_file = os.path.join(paper_folder, f"{arxiv_id}_analysis.md")
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    f.write(processed_content)
                
                logger.info(f"Analysis completed for {arxiv_id}, saved {len(processed_content)} characters")
                
                # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
                self.paper_service.update_analysis_status(arxiv_id, 'completed')
                return True
            else:
                logger.warning(f"No analysis result generated for {arxiv_id}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æ‰§è¡Œæ·±åº¦åˆ†æå¤±è´¥ {arxiv_id}: {e}")
            return False
    
    def _process_image_paths(self, content: str, arxiv_id: str) -> str:
        """
        å¤„ç†Markdownå†…å®¹ä¸­çš„å›¾ç‰‡è·¯å¾„ï¼Œå°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºå¯è®¿é—®çš„URLè·¯å¾„
        
        Args:
            content: åŸå§‹Markdownå†…å®¹
            arxiv_id: ArXivè®ºæ–‡ID
            
        Returns:
            str: å¤„ç†åçš„Markdownå†…å®¹
        """
        try:
            logger.info(f"ğŸ–¼ï¸ Starting image path processing for {arxiv_id}")
            
            # ä½¿ç”¨æ›´å®½æ³›çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å„ç§Markdownå›¾ç‰‡è¯­æ³•æ ¼å¼
            # åŒ¹é… ![alt](imgs/filename) æ ¼å¼
            img_pattern = r'!\[([^\]]*)\]\((imgs/[^)]+)\)'
            
            def replace_image_path(match):
                alt_text = match.group(1)
                relative_path = match.group(2)
                # è½¬æ¢ä¸ºFlaskå¯è®¿é—®çš„URLè·¯å¾„
                filename = relative_path.replace('imgs/', '')
                new_path = f"/paper/{arxiv_id}/analysis_images/{filename}"
                logger.debug(f"  ğŸ“¸ Converting: {relative_path} â†’ {new_path}")
                return f"![{alt_text}]({new_path})"
            
            # å…ˆè®°å½•åŸå§‹å›¾ç‰‡æ•°é‡ç”¨äºè°ƒè¯•
            original_matches = re.findall(img_pattern, content)
            logger.info(f"  ğŸ“Š Found {len(original_matches)} image references for {arxiv_id}")
            
            if original_matches:
                # è®°å½•å‰5ä¸ªåŒ¹é…é¡¹ç”¨äºè°ƒè¯•
                sample_matches = original_matches[:5]
                logger.debug(f"  ğŸ“‹ Sample matches: {sample_matches}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„å›¾ç‰‡
                unique_images = set([match[1] for match in original_matches])
                if len(unique_images) != len(original_matches):
                    logger.warning(f"  âš ï¸ Found {len(original_matches) - len(unique_images)} duplicate image references")
            
            # æ›¿æ¢æ‰€æœ‰å›¾ç‰‡è·¯å¾„
            processed_content = re.sub(img_pattern, replace_image_path, content)
            
            # éªŒè¯å¤„ç†ç»“æœ
            processed_matches = re.findall(r'!\[([^\]]*)\]\((/paper/[^)]+)\)', processed_content)
            logger.info(f"  âœ… Successfully processed {len(processed_matches)} image paths for {arxiv_id}")
            
            # é¢å¤–éªŒè¯ï¼šç¡®ä¿æ²¡æœ‰é—ç•™çš„ imgs/ è·¯å¾„
            remaining_old_paths = re.findall(r'!\[([^\]]*)\]\((imgs/[^)]+)\)', processed_content)
            if remaining_old_paths:
                logger.error(f"  âŒ Found {len(remaining_old_paths)} unprocessed imgs/ paths: {remaining_old_paths[:3]}")
                # å°è¯•å†æ¬¡å¤„ç†
                processed_content = re.sub(img_pattern, replace_image_path, processed_content)
                remaining_after_retry = re.findall(r'!\[([^\]]*)\]\((imgs/[^)]+)\)', processed_content)
                if remaining_after_retry:
                    logger.error(f"  âŒ Still have {len(remaining_after_retry)} unprocessed paths after retry")
                else:
                    logger.info(f"  âœ… Successfully processed remaining paths after retry")
            
            # éªŒè¯å¤„ç†æ˜¯å¦æˆåŠŸ
            if len(original_matches) != len(processed_matches):
                logger.warning(f"  âš ï¸ Mismatch in image count: original={len(original_matches)}, processed={len(processed_matches)}")
            
            # æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿä¸­å›¾ç‰‡æ˜¯å¦å­˜åœ¨ï¼ˆå¯é€‰éªŒè¯ï¼‰
            if processed_matches:
                self._validate_image_files_exist(arxiv_id, processed_matches[:3])  # åªéªŒè¯å‰3ä¸ª
            
            logger.info(f"ğŸ–¼ï¸ Image path processing completed for {arxiv_id}")
            return processed_content
            
        except Exception as e:
            logger.error(f"âŒ Failed to process image paths for {arxiv_id}: {e}")
            logger.error(f"   Content length: {len(content) if content else 0} characters")
            return content
    
    def _validate_image_files_exist(self, arxiv_id: str, sample_matches: list) -> None:
        """
        éªŒè¯å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨äºæ–‡ä»¶ç³»ç»Ÿä¸­
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            sample_matches: æ ·æœ¬åŒ¹é…ç»“æœåˆ—è¡¨
        """
        try:
            import os
            base_path = os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'paper_analyze')
            img_dir = os.path.join(base_path, arxiv_id, 'imgs')
            
            if not os.path.exists(img_dir):
                logger.warning(f"  âš ï¸ Image directory does not exist: {img_dir}")
                return
            
            for alt_text, url_path in sample_matches:
                # ä»URLè·¯å¾„æå–æ–‡ä»¶å
                filename = url_path.split('/')[-1]
                img_path = os.path.join(img_dir, filename)
                
                if os.path.exists(img_path):
                    file_size = os.path.getsize(img_path)
                    logger.debug(f"  âœ… Image exists: {filename} ({file_size} bytes)")
                else:
                    logger.warning(f"  âš ï¸ Image file not found: {filename}")
                    
        except Exception as e:
            logger.debug(f"  ğŸ’­ Image validation skipped due to error: {e}")
    
    def get_analysis_status(self, arxiv_id: str) -> Dict[str, Any]:
        """
        è·å–åˆ†æçŠ¶æ€
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            
        Returns:
            Dict: çŠ¶æ€ä¿¡æ¯
        """
        try:
            # ä»æ•°æ®åº“è·å–çŠ¶æ€
            status_info = self.paper_service.get_analysis_status(arxiv_id)
            
            if not status_info:
                return {
                    'success': True,
                    'status': 'not_started',
                    'message': 'å°šæœªå¼€å§‹åˆ†æ'
                }
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„çº¿ç¨‹
            if arxiv_id in self.analysis_threads:
                thread = self.analysis_threads[arxiv_id]
                if thread.is_alive():
                    status_info['is_running'] = True
                else:
                    # æ¸…ç†å·²å®Œæˆçš„çº¿ç¨‹
                    del self.analysis_threads[arxiv_id]
                    status_info['is_running'] = False
            else:
                status_info['is_running'] = False
            
            return {
                'success': True,
                **status_info
            }
            
        except Exception as e:
            logger.error(f"Failed to get analysis status for {arxiv_id}: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_analysis_result(self, arxiv_id: str) -> Dict[str, Any]:
        """
        è·å–åˆ†æç»“æœ
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            
        Returns:
            Dict: åˆ†æç»“æœ
        """
        try:
            result = self.paper_service.get_analysis_result(arxiv_id)
            
            if not result:
                return {
                    'success': False,
                    'error': 'åˆ†æç»“æœä¸å­˜åœ¨'
                }
            
            return {
                'success': True,
                **result
            }
            
        except Exception as e:
            logger.error(f"Failed to get analysis result for {arxiv_id}: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def cancel_analysis(self, arxiv_id: str) -> Dict[str, Any]:
        """
        å–æ¶ˆæ­£åœ¨è¿›è¡Œçš„åˆ†æ
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            
        Returns:
            Dict: æ“ä½œç»“æœ
        """
        try:
            if arxiv_id not in self.analysis_threads:
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„åˆ†æ'
                }
            
            thread = self.analysis_threads[arxiv_id]
            if not thread.is_alive():
                del self.analysis_threads[arxiv_id]
                return {
                    'success': False,
                    'error': 'åˆ†æå·²å®Œæˆæˆ–å·²åœæ­¢'
                }
            
            # æ³¨æ„ï¼šPythonçº¿ç¨‹æ— æ³•å¼ºåˆ¶åœæ­¢ï¼Œè¿™é‡Œåªèƒ½æ ‡è®°çŠ¶æ€
            # å®é™…çš„çº¿ç¨‹ä»ä¼šç»§ç»­è¿è¡Œç›´åˆ°è‡ªç„¶ç»“æŸ
            del self.analysis_threads[arxiv_id]
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            self.paper_service.update_analysis_status(arxiv_id, 'cancelled')
            
            logger.info(f"Analysis cancelled for {arxiv_id}")
            
            return {
                'success': True,
                'message': 'åˆ†æå·²å–æ¶ˆ'
            }
            
        except Exception as e:
            logger.error(f"Failed to cancel analysis for {arxiv_id}: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def cleanup_completed_threads(self):
        """æ¸…ç†å·²å®Œæˆçš„åˆ†æçº¿ç¨‹"""
        completed_threads = []
        
        for arxiv_id, thread in self.analysis_threads.items():
            if not thread.is_alive():
                completed_threads.append(arxiv_id)
        
        for arxiv_id in completed_threads:
            del self.analysis_threads[arxiv_id]
        
        if completed_threads:
            logger.info(f"Cleaned up {len(completed_threads)} completed analysis threads")
    
    def get_active_analyses(self) -> Dict[str, Any]:
        """
        è·å–æ‰€æœ‰æ´»è·ƒçš„åˆ†æä»»åŠ¡
        
        Returns:
            Dict: æ´»è·ƒä»»åŠ¡ä¿¡æ¯
        """
        try:
            self.cleanup_completed_threads()
            
            active_analyses = []
            for arxiv_id, thread in self.analysis_threads.items():
                if thread.is_alive():
                    status_info = self.paper_service.get_analysis_status(arxiv_id)
                    if status_info:
                        active_analyses.append({
                            'arxiv_id': arxiv_id,
                            **status_info
                        })
            
            return {
                'success': True,
                'active_count': len(active_analyses),
                'analyses': active_analyses
            }
            
        except Exception as e:
            logger.error(f"Failed to get active analyses: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    # === å…¬å¼çº é”™ç›¸å…³æ–¹æ³• ===
    
    def start_formula_correction(self, arxiv_id: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        å¯åŠ¨å…¬å¼çº é”™
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            config: çº é”™é…ç½®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict: æ“ä½œç»“æœ
        """
        try:
            logger.info(f"ğŸ”§ å¼€å§‹å¯åŠ¨å…¬å¼çº é”™ - ArXiv ID: {arxiv_id}")
            
            # æ£€æŸ¥è®ºæ–‡æ˜¯å¦å­˜åœ¨
            paper = self.paper_service.get_paper_detail(arxiv_id)
            if not paper:
                logger.error(f"âŒ è®ºæ–‡ä¸å­˜åœ¨: {arxiv_id}")
                return {
                    'success': False,
                    'error': f'è®ºæ–‡ {arxiv_id} ä¸å­˜åœ¨'
                }
            
            # æ£€æŸ¥åˆ†ææ–‡ä»¶æ˜¯å¦å­˜åœ¨
            project_root = Path(__file__).parent.parent.parent.parent
            paper_folder = project_root / "data" / "paper_analyze" / arxiv_id
            analysis_file = paper_folder / f"{arxiv_id}_analysis.md"
            ocr_file = paper_folder / f"{arxiv_id}_paddleocr.md"
            
            if not analysis_file.exists():
                logger.error(f"âŒ åˆ†ææ–‡ä»¶ä¸å­˜åœ¨: {analysis_file}")
                return {
                    'success': False,
                    'error': 'åˆ†ææ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿›è¡Œæ·±åº¦åˆ†æ'
                }
            
            if not ocr_file.exists():
                logger.error(f"âŒ OCRæ–‡ä»¶ä¸å­˜åœ¨: {ocr_file}")
                return {
                    'success': False,
                    'error': 'OCRæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œå…¬å¼çº é”™'
                }
            
            logger.info(f"âœ… æ–‡ä»¶æ£€æŸ¥é€šè¿‡: {arxiv_id}")
            
            # æ£€æŸ¥æ˜¯å¦å·²åœ¨çº é”™ä¸­
            if arxiv_id in self.correction_threads:
                thread = self.correction_threads[arxiv_id]
                if thread.is_alive():
                    logger.warning(f"âš ï¸ è®ºæ–‡å·²åœ¨çº é”™ä¸­: {arxiv_id}")
                    return {
                        'success': False,
                        'error': 'è¯¥è®ºæ–‡æ­£åœ¨è¿›è¡Œå…¬å¼çº é”™ï¼Œè¯·ç¨å'
                    }
                else:
                    # æ¸…ç†å·²å®Œæˆçš„çº¿ç¨‹
                    del self.correction_threads[arxiv_id]
                    logger.info(f"ğŸ§¹ æ¸…ç†äº†å·²å®Œæˆçš„çº é”™çº¿ç¨‹: {arxiv_id}")
            
            # åˆ›å»ºå¹¶å¯åŠ¨çº é”™çº¿ç¨‹
            thread = threading.Thread(
                target=self._run_formula_correction,
                args=(arxiv_id, str(analysis_file), str(ocr_file), config or {}),
                daemon=True
            )
            thread.start()
            
            # ä¿å­˜çº¿ç¨‹å¼•ç”¨
            self.correction_threads[arxiv_id] = thread
            
            logger.info(f"Started formula correction for paper {arxiv_id}")
            
            return {
                'success': True,
                'message': 'å…¬å¼çº é”™å·²å¯åŠ¨',
                'status': 'processing'
            }
            
        except Exception as e:
            logger.error(f"Failed to start formula correction for {arxiv_id}: {e}")
            return {
                'success': False,
                'error': f'å¯åŠ¨å…¬å¼çº é”™å¤±è´¥: {str(e)}'
            }
    
    def _run_formula_correction(self, arxiv_id: str, analysis_file_path: str, 
                               ocr_file_path: str, config: Dict[str, Any]):
        """
        æ‰§è¡Œå…¬å¼çº é”™ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼‰
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            analysis_file_path: åˆ†ææ–‡ä»¶è·¯å¾„
            ocr_file_path: OCRæ–‡ä»¶è·¯å¾„
            config: çº é”™é…ç½®
        """
        try:
            logger.info(f"ğŸ”§ å¼€å§‹æ‰§è¡Œå…¬å¼çº é”™ - ArXiv ID: {arxiv_id}")
            
            # åˆ›å»ºå¤‡ä»½æ–‡ä»¶
            backup_success = self._create_analysis_backup(arxiv_id, analysis_file_path)
            if not backup_success:
                logger.error(f"âŒ åˆ›å»ºå¤‡ä»½æ–‡ä»¶å¤±è´¥: {arxiv_id}")
                return
            
            logger.info(f"âœ… å¤‡ä»½æ–‡ä»¶åˆ›å»ºæˆåŠŸ: {arxiv_id}")
            
            # åŠ¨æ€å¯¼å…¥å…¬å¼çº é”™æ™ºèƒ½ä½“
            try:
                from HomeSystem.graph.formula_correction_agent import create_formula_correction_agent
                logger.info("âœ… Successfully imported formula correction agent")
            except Exception as import_error:
                logger.error(f"âŒ Failed to import formula correction agent: {import_error}")
                return
            
            # åˆ›å»ºå…¬å¼çº é”™æ™ºèƒ½ä½“
            correction_model = config.get('correction_model', 'ollama.Qwen3_30B')
            logger.info(f"ğŸ¤– Creating formula correction agent with model: {correction_model}")
            
            agent = create_formula_correction_agent(
                correction_model=correction_model
            )
            logger.info("âœ… Formula correction agent created successfully")
            
            # æ‰§è¡Œå…¬å¼çº é”™
            correction_result = agent.correct_formulas(
                analysis_file_path=analysis_file_path,
                ocr_file_path=ocr_file_path,
                thread_id=f"web_correction_{arxiv_id}_{int(time.time())}"
            )
            
            # æ£€æŸ¥çº é”™æ˜¯å¦æˆåŠŸ
            if 'error' in correction_result:
                logger.error(f"Formula correction failed for {arxiv_id}: {correction_result['error']}")
                return
            
            # å¤„ç†çº é”™ç»“æœ
            corrected_content = correction_result.get('corrected_content')
            if corrected_content:
                # å¤„ç†å›¾ç‰‡è·¯å¾„
                processed_content = self._process_image_paths(corrected_content, arxiv_id)
                
                # ä¿å­˜çº é”™åçš„å†…å®¹åˆ°åŸåˆ†ææ–‡ä»¶
                with open(analysis_file_path, 'w', encoding='utf-8') as f:
                    f.write(processed_content)
                
                # æ›´æ–°æ•°æ®åº“ä¸­çš„åˆ†æç»“æœ
                self.paper_service.save_analysis_result(arxiv_id, processed_content)
                
                corrections_applied = correction_result.get('corrections_applied', [])
                logger.info(f"Formula correction completed for {arxiv_id}, applied {len(corrections_applied)} corrections")
                
                # è®°å½•çº é”™å†å²ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
                self._record_correction_history(arxiv_id, corrections_applied)
                
            else:
                logger.info(f"No corrections needed for {arxiv_id}")
                
        except Exception as e:
            logger.error(f"ğŸ’¥ å…¬å¼çº é”™è¿‡ç¨‹å¤±è´¥ {arxiv_id}: {e}")
        finally:
            # æ¸…ç†çº¿ç¨‹å¼•ç”¨
            if arxiv_id in self.correction_threads:
                del self.correction_threads[arxiv_id]
    
    def _create_analysis_backup(self, arxiv_id: str, analysis_file_path: str) -> bool:
        """
        åˆ›å»ºåˆ†ææ–‡ä»¶çš„å¤‡ä»½
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            analysis_file_path: åˆ†ææ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: å¤‡ä»½æ˜¯å¦æˆåŠŸ
        """
        try:
            import shutil
            from datetime import datetime
            
            # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_filename = f"{arxiv_id}_analysis_backup_{timestamp}.md"
            backup_file_path = os.path.join(os.path.dirname(analysis_file_path), backup_filename)
            
            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(analysis_file_path, backup_file_path)
            
            logger.info(f"ğŸ“‹ Created backup file: {backup_file_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to create backup for {arxiv_id}: {e}")
            return False
    
    def _record_correction_history(self, arxiv_id: str, corrections_applied: list):
        """
        è®°å½•çº é”™å†å²ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            corrections_applied: åº”ç”¨çš„çº é”™åˆ—è¡¨
        """
        try:
            # è¿™é‡Œå¯ä»¥è®°å½•åˆ°æ•°æ®åº“æˆ–æ—¥å¿—æ–‡ä»¶
            # ç›®å‰åªè®°å½•åˆ°æ—¥å¿—
            logger.info(f"ğŸ“ Correction history for {arxiv_id}: {len(corrections_applied)} corrections applied")
            for i, correction in enumerate(corrections_applied):
                logger.info(f"  [{i+1}] {correction.get('operation', 'unknown')}: {correction.get('message', 'N/A')}")
                
        except Exception as e:
            logger.debug(f"Failed to record correction history: {e}")
    
    def get_formula_correction_status(self, arxiv_id: str) -> Dict[str, Any]:
        """
        è·å–å…¬å¼çº é”™çŠ¶æ€
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            
        Returns:
            Dict: çŠ¶æ€ä¿¡æ¯
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„çº é”™çº¿ç¨‹
            if arxiv_id in self.correction_threads:
                thread = self.correction_threads[arxiv_id]
                if thread.is_alive():
                    return {
                        'success': True,
                        'status': 'processing',
                        'message': 'æ­£åœ¨è¿›è¡Œå…¬å¼çº é”™',
                        'is_running': True
                    }
                else:
                    # æ¸…ç†å·²å®Œæˆçš„çº¿ç¨‹
                    del self.correction_threads[arxiv_id]
                    return {
                        'success': True,
                        'status': 'completed',
                        'message': 'å…¬å¼çº é”™å·²å®Œæˆ',
                        'is_running': False
                    }
            else:
                return {
                    'success': True,
                    'status': 'not_started',
                    'message': 'å°šæœªå¼€å§‹å…¬å¼çº é”™',
                    'is_running': False
                }
            
        except Exception as e:
            logger.error(f"Failed to get formula correction status for {arxiv_id}: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def cancel_formula_correction(self, arxiv_id: str) -> Dict[str, Any]:
        """
        å–æ¶ˆæ­£åœ¨è¿›è¡Œçš„å…¬å¼çº é”™
        
        Args:
            arxiv_id: ArXivè®ºæ–‡ID
            
        Returns:
            Dict: æ“ä½œç»“æœ
        """
        try:
            if arxiv_id not in self.correction_threads:
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„å…¬å¼çº é”™'
                }
            
            thread = self.correction_threads[arxiv_id]
            if not thread.is_alive():
                del self.correction_threads[arxiv_id]
                return {
                    'success': False,
                    'error': 'å…¬å¼çº é”™å·²å®Œæˆæˆ–å·²åœæ­¢'
                }
            
            # æ³¨æ„ï¼šPythonçº¿ç¨‹æ— æ³•å¼ºåˆ¶åœæ­¢ï¼Œè¿™é‡Œåªèƒ½æ ‡è®°çŠ¶æ€
            del self.correction_threads[arxiv_id]
            
            logger.info(f"Formula correction cancelled for {arxiv_id}")
            
            return {
                'success': True,
                'message': 'å…¬å¼çº é”™å·²å–æ¶ˆ'
            }
            
        except Exception as e:
            logger.error(f"Failed to cancel formula correction for {arxiv_id}: {e}")
            return {
                'success': False,
                'error': str(e)
            }