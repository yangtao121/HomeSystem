#!/usr/bin/env python3
"""
VLASè®ºæ–‡å¿«é€Ÿåˆ†æè„šæœ¬

å¿«é€Ÿç”ŸæˆVLASè®ºæ–‡çš„åŸºæœ¬åˆ†ææŠ¥å‘Šï¼Œè·³è¿‡å¤æ‚çš„å¤šè½®è¿­ä»£ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰çš„è®ºæ–‡åˆ†æå·¥å…·ã€‚
"""

import sys
import os
from pathlib import Path
from loguru import logger

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/mnt/nfs_share/code/homesystem')

from HomeSystem.graph.tool.paper_analysis_tools import create_paper_analysis_tools
from HomeSystem.graph.llm_factory import get_llm
from HomeSystem.graph.parser.paper_folder_parser import create_paper_folder_parser


def quick_analyze_vlas():
    """å¿«é€Ÿåˆ†æVLASè®ºæ–‡"""
    
    paper_folder = "/mnt/nfs_share/code/homesystem/data/paper_analyze/2502.13508"
    
    if not os.path.exists(paper_folder):
        logger.error(f"è®ºæ–‡æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {paper_folder}")
        return
    
    logger.info("ğŸš€ å¼€å§‹VLASè®ºæ–‡å¿«é€Ÿåˆ†æ")
    logger.info(f"ğŸ“ åˆ†ææ–‡ä»¶å¤¹: {paper_folder}")
    
    try:
        # 1. è§£æè®ºæ–‡æ–‡ä»¶å¤¹
        logger.info("ğŸ“– è§£æè®ºæ–‡æ–‡ä»¶å¤¹...")
        parser = create_paper_folder_parser(paper_folder)
        folder_data = parser.parse_folder()
        
        paper_text = folder_data["paper_text"]
        logger.info(f"ğŸ“„ è®ºæ–‡æ–‡æœ¬é•¿åº¦: {len(paper_text)} å­—ç¬¦")
        logger.info(f"ğŸ–¼ï¸ å¯ç”¨å›¾ç‰‡: {len(folder_data['available_images'])} å¼ ")
        
        # 2. åˆ›å»ºLLMåˆ†æå·¥å…·
        logger.info("ğŸ¤– åˆå§‹åŒ–åˆ†æå·¥å…·...")
        llm = get_llm("deepseek.DeepSeek_V3")
        analysis_tools = create_paper_analysis_tools(llm)
        
        # 3. æ‰§è¡Œå„é¡¹åˆ†æ
        results = {}
        
        # èƒŒæ™¯å’Œç›®æ ‡åˆ†æ
        logger.info("ğŸ” åˆ†æç ”ç©¶èƒŒæ™¯å’Œç›®æ ‡...")
        background_tool = analysis_tools[0]  # BackgroundObjectivesTool
        background_result = background_tool.invoke({"paper_text": paper_text})
        results["background_objectives"] = background_result
        logger.info("âœ… èƒŒæ™¯å’Œç›®æ ‡åˆ†æå®Œæˆ")
        
        # æ–¹æ³•å’Œå‘ç°åˆ†æ
        logger.info("âš™ï¸ åˆ†æç ”ç©¶æ–¹æ³•å’Œä¸»è¦å‘ç°...")
        methods_tool = analysis_tools[1]  # MethodsFindingsTool
        methods_result = methods_tool.invoke({"paper_text": paper_text})
        results["methods_findings"] = methods_result
        logger.info("âœ… æ–¹æ³•å’Œå‘ç°åˆ†æå®Œæˆ")
        
        # ç»“è®ºå’Œæœªæ¥å·¥ä½œåˆ†æ
        logger.info("ğŸ“Š åˆ†æç»“è®ºå’Œæœªæ¥å·¥ä½œ...")
        conclusions_tool = analysis_tools[2]  # ConclusionsFutureTool
        conclusions_result = conclusions_tool.invoke({"paper_text": paper_text})
        results["conclusions_future"] = conclusions_result
        logger.info("âœ… ç»“è®ºå’Œæœªæ¥å·¥ä½œåˆ†æå®Œæˆ")
        
        # æå–å…³é”®è¯
        logger.info("ğŸ·ï¸ æå–å…³é”®è¯...")
        import json
        
        # è§£æä¹‹å‰çš„ç»“æœ
        bg_data = json.loads(background_result) if isinstance(background_result, str) else background_result
        methods_data = json.loads(methods_result) if isinstance(methods_result, str) else methods_result
        conclusions_data = json.loads(conclusions_result) if isinstance(conclusions_result, str) else conclusions_result
        
        keywords_tool = analysis_tools[3]  # KeywordsSynthesisTool
        keywords_result = keywords_tool.invoke({
            "research_background": bg_data.get("research_background", ""),
            "research_objectives": bg_data.get("research_objectives", ""),
            "methods": methods_data.get("methods", ""),
            "key_findings": methods_data.get("key_findings", ""),
            "conclusions": conclusions_data.get("conclusions", "")
        })
        results["keywords"] = keywords_result
        logger.info("âœ… å…³é”®è¯æå–å®Œæˆ")
        
        # 4. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        logger.info("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        report_content = generate_analysis_report(results)
        
        # 5. ä¿å­˜æŠ¥å‘Š
        output_path = os.path.join(paper_folder, "VLAS_å¿«é€Ÿåˆ†ææŠ¥å‘Š.md")
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info("="*60)
        logger.info("âœ… å¿«é€Ÿåˆ†æå®Œæˆï¼")
        logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        logger.info(f"ğŸ“Š æŠ¥å‘Šé•¿åº¦: {len(report_content)} å­—ç¬¦")
        logger.info("="*60)
        
        # æ˜¾ç¤ºæŠ¥å‘Šé¢„è§ˆ
        print("\n" + "="*60)
        print("ğŸ“ VLASè®ºæ–‡åˆ†ææŠ¥å‘Šé¢„è§ˆ:")
        print("="*60)
        
        # æ˜¾ç¤ºæŠ¥å‘Šçš„å‰500å­—ç¬¦
        preview = report_content[:500] + "..." if len(report_content) > 500 else report_content
        print(preview)
        
        return output_path
        
    except Exception as e:
        logger.error(f"âŒ å¿«é€Ÿåˆ†æå¤±è´¥: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return None


def generate_analysis_report(results):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    import json
    from datetime import datetime
    
    # è§£æåˆ†æç»“æœ
    bg_data = json.loads(results["background_objectives"]) if isinstance(results["background_objectives"], str) else results["background_objectives"]
    methods_data = json.loads(results["methods_findings"]) if isinstance(results["methods_findings"], str) else results["methods_findings"]
    conclusions_data = json.loads(results["conclusions_future"]) if isinstance(results["conclusions_future"], str) else results["conclusions_future"]
    keywords_data = json.loads(results["keywords"]) if isinstance(results["keywords"], str) else results["keywords"]
    
    report = f"""# VLAS: Vision-Language-Action Model with Speech Instructions - æ·±åº¦åˆ†ææŠ¥å‘Š

**è®ºæ–‡æ ‡é¢˜**: VLAS: VISION-LANGUAGE-ACTION MODEL WITH SPEECH INSTRUCTIONS FOR CUSTOMIZED ROBOT MANIPULATION

**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

**åˆ†ææ¦‚è¦**: æœ¬æŠ¥å‘Šå¯¹VLASè®ºæ–‡è¿›è¡Œäº†æ·±åº¦åˆ†æï¼Œæ¶µç›–ç ”ç©¶èƒŒæ™¯ã€æŠ€æœ¯æ–¹æ³•ã€ä¸»è¦è´¡çŒ®å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

VLASæ˜¯é¦–ä¸ªç›´æ¥æ”¯æŒè¯­éŸ³æŒ‡ä»¤çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œä¸“ä¸ºå®šåˆ¶åŒ–æœºå™¨äººæ“ä½œè€Œè®¾è®¡ã€‚è¯¥ç ”ç©¶é€šè¿‡ç«¯åˆ°ç«¯çš„è¯­éŸ³è¯†åˆ«é›†æˆï¼Œè§£å†³äº†ä¼ ç»ŸVLAæ¨¡å‹ä»…æ”¯æŒæ–‡æœ¬æŒ‡ä»¤çš„å±€é™æ€§ï¼Œä¸ºæ›´è‡ªç„¶çš„äººæœºäº¤äº’å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

## ğŸ¯ ç ”ç©¶èƒŒæ™¯ä¸ç›®æ ‡

### ç ”ç©¶èƒŒæ™¯
{bg_data.get('research_background', 'æš‚æ— æ•°æ®')}

### ç ”ç©¶ç›®æ ‡  
{bg_data.get('research_objectives', 'æš‚æ— æ•°æ®')}

---

## âš™ï¸ æŠ€æœ¯æ–¹æ³•ä¸åˆ›æ–°

### ç ”ç©¶æ–¹æ³•
{methods_data.get('methods', 'æš‚æ— æ•°æ®')}

### ä¸»è¦å‘ç°
{methods_data.get('key_findings', 'æš‚æ— æ•°æ®')}

---

## ğŸ“Š ç»“è®ºä¸å±•æœ›

### ä¸»è¦ç»“è®º
{conclusions_data.get('conclusions', 'æš‚æ— æ•°æ®')}

### ç ”ç©¶å±€é™æ€§
{conclusions_data.get('limitations', 'æš‚æ— æ•°æ®')}

### æœªæ¥å·¥ä½œæ–¹å‘
{conclusions_data.get('future_work', 'æš‚æ— æ•°æ®')}

---

## ğŸ·ï¸ å…³é”®è¯

{', '.join(keywords_data.get('keywords', [])) if keywords_data.get('keywords') else 'æš‚æ— å…³é”®è¯'}

---

## ğŸ” æŠ€æœ¯è¯„ä¼°

### åˆ›æ–°ç‚¹è¯„ä»·
1. **ç«¯åˆ°ç«¯è¯­éŸ³é›†æˆ**: é¦–æ¬¡åœ¨VLAæ¨¡å‹ä¸­å®ç°ç›´æ¥è¯­éŸ³æŒ‡ä»¤å¤„ç†ï¼Œé¿å…äº†ä¼ ç»ŸASRç³»ç»Ÿçš„å¤æ‚æ€§
2. **ä¸ªæ€§åŒ–å®šåˆ¶**: é€šè¿‡Voice RAGæŠ€æœ¯æ”¯æŒåŸºäºä¸ªäººç‰¹å¾çš„å®šåˆ¶åŒ–æ“ä½œ
3. **å¤šæ¨¡æ€èåˆ**: æœ‰æ•ˆç»“åˆè§†è§‰ã€è¯­è¨€å’Œè¯­éŸ³æ¨¡æ€ï¼Œæå‡äº†äººæœºäº¤äº’çš„è‡ªç„¶æ€§

### æŠ€æœ¯å½±å“
- **å­¦æœ¯ä»·å€¼**: ä¸ºVLAæ¨¡å‹å‘å±•å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘
- **åº”ç”¨æ½œåŠ›**: åœ¨å®¶åº­æŠ¤ç†ã€ä¸ªäººåŠ©ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿é˜”åº”ç”¨å‰æ™¯
- **æŠ€æœ¯çªç ´**: è§£å†³äº†è¯­éŸ³ä¿¡æ¯ä¸¢å¤±å’Œç³»ç»Ÿå¤æ‚æ€§é—®é¢˜

### å±€é™æ€§åˆ†æ
- ä¾èµ–äºè¯­éŸ³æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§
- åœ¨å˜ˆæ‚ç¯å¢ƒä¸‹çš„é²æ£’æ€§æœ‰å¾…éªŒè¯
- è®¡ç®—å¤æ‚åº¦å¯èƒ½é™åˆ¶å®æ—¶åº”ç”¨

---

## ğŸ“ˆ ç ”ç©¶æ„ä¹‰

æœ¬ç ”ç©¶åœ¨è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹é¢†åŸŸå–å¾—äº†é‡è¦çªç ´ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­éŸ³æŒ‡ä»¤é›†æˆæ–¹é¢ã€‚VLASæ¨¡å‹ä¸ä»…æå‡äº†äººæœºäº¤äº’çš„è‡ªç„¶æ€§ï¼Œè¿˜ä¸ºä¸ªæ€§åŒ–æœºå™¨äººæœåŠ¡å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚è¯¥å·¥ä½œå¯¹æœªæ¥æ™ºèƒ½æœºå™¨äººçš„å‘å±•å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚

---

**æŠ¥å‘Šç”Ÿæˆå·¥å…·**: HomeSystemæ·±åº¦è®ºæ–‡åˆ†æç³»ç»Ÿ  
**åˆ†ææ¨¡å‹**: DeepSeek V3  
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    return report


if __name__ == "__main__":
    quick_analyze_vlas()