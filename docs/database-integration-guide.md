# Home System æ•°æ®åº“é›†æˆå®Œæ•´æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»äº† Home System æ•°æ®åº“é›†æˆçš„è®¾è®¡ç†å¿µã€éƒ¨ç½²æ–¹å¼å’Œä½¿ç”¨æ–¹æ³•ï¼ŒåŒ…æ‹¬ PostgreSQL å’Œ Redis çš„é…ç½®ã€ArXiv è®ºæ–‡ç®¡ç†åŠŸèƒ½çš„ä½¿ç”¨ç­‰ã€‚

## ğŸ¯ æ¦‚è¿°

Home System æä¾›äº†ç»Ÿä¸€çš„æ•°æ®åº“åŸºç¡€è®¾æ–½ï¼Œæ”¯æŒ PostgreSQL å’Œ Redisï¼Œä¸ºç³»ç»Ÿå„ä¸ªæ¨¡å—æä¾›æ•°æ®æŒä¹…åŒ–å’Œç¼“å­˜æœåŠ¡ã€‚è¯¥ç³»ç»Ÿä¸“é—¨é’ˆå¯¹ ArXiv è®ºæ–‡ç®¡ç†è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæ”¯æŒè®ºæ–‡çš„è‡ªåŠ¨é‡‡é›†ã€å­˜å‚¨ã€æŸ¥è¯¢å’Œå¤„ç†çŠ¶æ€è·Ÿè¸ªã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸ—„ï¸ **åŒæ•°æ®åº“æ¶æ„**: PostgreSQL (ä¸»å­˜å‚¨) + Redis (ç¼“å­˜)
- ğŸ“š **ArXiv ä¸“ç”¨ä¼˜åŒ–**: è®ºæ–‡å»é‡ã€çŠ¶æ€è·Ÿè¸ªã€æ‰¹é‡å¤„ç†
- ğŸš€ **å®¹å™¨åŒ–éƒ¨ç½²**: Docker Compose ä¸€é”®å¯åŠ¨
- âš¡ **é«˜æ€§èƒ½**: è¿æ¥æ± ã€ç´¢å¼•ä¼˜åŒ–ã€æ™ºèƒ½ç¼“å­˜
- ğŸ”§ **æ˜“äºæ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒè‡ªå®šä¹‰æ•°æ®æ¨¡å‹

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ç³»ç»Ÿæ¶æ„å›¾

```
HomeSystem/
â”œâ”€â”€ integrations/
â”‚   â””â”€â”€ database/
â”‚       â”œâ”€â”€ __init__.py           # åŒ…å¯¼å‡º
â”‚       â”œâ”€â”€ connection.py         # æ•°æ®åº“è¿æ¥ç®¡ç†
â”‚       â”œâ”€â”€ models.py            # æ•°æ®æ¨¡å‹å®šä¹‰
â”‚       â””â”€â”€ operations.py        # æ•°æ®åº“æ“ä½œæ¥å£
â”œâ”€â”€ utility/
â”‚   â””â”€â”€ arxiv/
â”‚       â”œâ”€â”€ arxiv.py             # ArXiv API å·¥å…·
â”‚       â””â”€â”€ database_integration.py  # ArXiv æ•°æ®åº“é›†æˆ
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ simple_arxiv_demo.py     # ç®€åŒ–ä½¿ç”¨ç¤ºä¾‹
â”‚   â””â”€â”€ database_usage_example.py # å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
â””â”€â”€ docs/
    â””â”€â”€ database-integration-guide.md  # æœ¬æ–‡æ¡£
```

### æŠ€æœ¯æ ˆ

- **PostgreSQL 15**: ä¸»æ•°æ®åº“ï¼Œå­˜å‚¨è®ºæ–‡æ•°æ®
- **Redis 7**: ç¼“å­˜æ•°æ®åº“ï¼Œå­˜å‚¨çƒ­ç‚¹æ•°æ®å’ŒçŠ¶æ€ä¿¡æ¯
- **Python 3.10+**: ä¸»è¦å¼€å‘è¯­è¨€
- **Docker Compose**: å®¹å™¨ç¼–æ’
- **psycopg2**: PostgreSQL åŒæ­¥å®¢æˆ·ç«¯
- **asyncpg**: PostgreSQL å¼‚æ­¥å®¢æˆ·ç«¯
- **redis-py**: Redis å®¢æˆ·ç«¯

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

#### å®‰è£…ä¾èµ–

```bash
# å®‰è£… Python ä¾èµ–
pip install psycopg2-binary redis asyncpg python-dotenv loguru

# æˆ–ä½¿ç”¨é¡¹ç›®ä¾èµ–æ–‡ä»¶
pip install -r requirements.txt
```

#### ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œç³»ç»Ÿæœ‰é»˜è®¤é…ç½®ï¼‰ï¼š

```bash
# PostgreSQL é…ç½®
DB_HOST=localhost
DB_PORT=15432              # æ³¨æ„ï¼šä½¿ç”¨è‡ªå®šä¹‰ç«¯å£é¿å…å†²çª
DB_NAME=homesystem
DB_USER=homesystem
DB_PASSWORD=homesystem123

# Redis é…ç½®
REDIS_HOST=localhost
REDIS_PORT=16379           # æ³¨æ„ï¼šä½¿ç”¨è‡ªå®šä¹‰ç«¯å£é¿å…å†²çª
REDIS_DB=0
```

### 2. å¯åŠ¨æ•°æ®åº“æœåŠ¡

#### ä½¿ç”¨ Docker Compose

```bash
# å¯åŠ¨æ•°æ®åº“æœåŠ¡ï¼ˆåå°è¿è¡Œï¼‰
docker compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker compose ps

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
docker compose logs postgres
docker compose logs redis
```

#### éªŒè¯æœåŠ¡çŠ¶æ€

```bash
# æ£€æŸ¥ PostgreSQL è¿æ¥
docker exec homesystem-postgres psql -U homesystem -d homesystem -c "\l"

# æ£€æŸ¥ Redis è¿æ¥
docker exec homesystem-redis redis-cli ping
```

### 3. åˆå§‹åŒ–æ•°æ®åº“

æ•°æ®åº“è¡¨ç»“æ„å·²è‡ªåŠ¨åˆ›å»ºï¼ŒåŒ…å«ï¼š

- **arxiv_papers** è¡¨ï¼šå­˜å‚¨è®ºæ–‡ä¿¡æ¯
- **ç´¢å¼•**: arxiv_idã€processing_statusã€categories ç­‰
- **è§¦å‘å™¨**: è‡ªåŠ¨æ›´æ–° updated_at å­—æ®µ

### 4. è¿è¡Œé›†æˆæµ‹è¯•å’Œç¤ºä¾‹

```bash
# è¿è¡Œå®Œæ•´é›†æˆæµ‹è¯•ï¼ˆæ¨èé¦–å…ˆæ‰§è¡Œï¼‰
python test_arxiv_database_integration.py

# æœŸæœ›è¾“å‡ºï¼šæ‰€æœ‰4ä¸ªæµ‹è¯•é€šè¿‡ âœ…
# - æ•°æ®åº“è¿æ¥
# - è¡¨ç»“æ„åˆ›å»º  
# - ArxivDataé›†æˆ
# - æ‰¹é‡å¤„ç†

# è¿è¡Œç®€åŒ–ç‰ˆç¤ºä¾‹
python simple_arxiv_demo.py

# è¿è¡Œå®Œæ•´åŠŸèƒ½ç¤ºä¾‹
python examples/database_usage_example.py
```

## ğŸ“Š æ•°æ®åº“ç»“æ„

### ArXiv è®ºæ–‡è¡¨ (arxiv_papers)

| å­—æ®µå | ç±»å‹ | è¯´æ˜ | ç´¢å¼• |
|--------|------|------|------|
| id | UUID | ä¸»é”®ï¼Œè‡ªåŠ¨ç”Ÿæˆ | PRIMARY |
| arxiv_id | VARCHAR(50) | ArXiv è®ºæ–‡ ID | UNIQUE |
| title | TEXT | è®ºæ–‡æ ‡é¢˜ | - |
| authors | TEXT | ä½œè€…ä¿¡æ¯ | - |
| abstract | TEXT | è®ºæ–‡æ‘˜è¦ | - |
| categories | VARCHAR(255) | è®ºæ–‡åˆ†ç±» | INDEX |
| published_date | VARCHAR(50) | å‘å¸ƒæ—¥æœŸ | INDEX |
| pdf_url | TEXT | PDF ä¸‹è½½é“¾æ¥ | - |
| processing_status | VARCHAR(20) | å¤„ç†çŠ¶æ€ | INDEX |
| tags | JSONB | æ ‡ç­¾æ•°ç»„ | - |
| metadata | JSONB | å…ƒæ•°æ®ï¼ˆå¼•ç”¨æ•°ç­‰ï¼‰ | - |
| **research_background** | TEXT | ç ”ç©¶èƒŒæ™¯ | - |
| **research_objectives** | TEXT | ç ”ç©¶ç›®æ ‡ | INDEX |
| **methods** | TEXT | ç ”ç©¶æ–¹æ³• | - |
| **key_findings** | TEXT | ä¸»è¦å‘ç° | - |
| **conclusions** | TEXT | ç»“è®º | - |
| **limitations** | TEXT | å±€é™æ€§ | - |
| **future_work** | TEXT | æœªæ¥å·¥ä½œ | - |
| **keywords** | TEXT | å…³é”®è¯ | INDEX |
| **task_name** | VARCHAR(255) | ä»»åŠ¡åç§° | INDEX |
| **task_id** | VARCHAR(100) | ä»»åŠ¡æ‰§è¡ŒID | INDEX |
| **full_paper_relevance_score** | DECIMAL(5,3) | å®Œæ•´è®ºæ–‡ç›¸å…³æ€§è¯„åˆ† | INDEX |
| **full_paper_relevance_justification** | TEXT | å®Œæ•´è®ºæ–‡ç›¸å…³æ€§è¯„åˆ†ç†ç”± | - |
| created_at | TIMESTAMP | åˆ›å»ºæ—¶é—´ | INDEX |
| updated_at | TIMESTAMP | æ›´æ–°æ—¶é—´ | - |

### å¤„ç†çŠ¶æ€è¯´æ˜

- `pending`: å¾…å¤„ç†
- `completed`: å·²å®Œæˆ
- `failed`: å¤„ç†å¤±è´¥

### ä»»åŠ¡è¿½è¸ªå­—æ®µè¯´æ˜

ç³»ç»Ÿæ–°å¢äº†ä»»åŠ¡è¿½è¸ªåŠŸèƒ½ï¼Œç”¨äºè®°å½•æ¯ç¯‡è®ºæ–‡çš„æ”¶é›†æ¥æºï¼š

- **task_name**: ä»»åŠ¡ç±»å‹æ ‡è¯†
  - `paper_gather`: Webç•Œé¢å³æ—¶ä»»åŠ¡
  - `paper_gather_scheduled`: Webç•Œé¢å®šæ—¶ä»»åŠ¡
  - `manual_collection`: æ‰‹åŠ¨æ”¶é›†ä»»åŠ¡
  - ç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰ä»»åŠ¡åç§°

- **task_id**: ä»»åŠ¡æ‰§è¡Œçš„å”¯ä¸€æ ‡è¯†ç¬¦
  - æ ¼å¼ï¼šUUID (ç”±Webç•Œé¢ç”Ÿæˆ)
  - æ¯æ¬¡ä»»åŠ¡æ‰§è¡Œéƒ½æœ‰å”¯ä¸€çš„ID
  - å¯ç”¨äºæŸ¥è¯¢ç‰¹å®šä»»åŠ¡æ”¶é›†çš„æ‰€æœ‰è®ºæ–‡

**å‘åå…¼å®¹æ€§**ï¼š
- ç°æœ‰è®ºæ–‡çš„ `task_name` å’Œ `task_id` å­—æ®µä¸º `NULL`
- æ–°æ”¶é›†çš„è®ºæ–‡ä¼šè‡ªåŠ¨å¡«å…¥è¿™äº›å­—æ®µ
- ç³»ç»Ÿå®Œå…¨å…¼å®¹å†å²æ•°æ®

### å®Œæ•´è®ºæ–‡ç›¸å…³æ€§è¯„åˆ†å­—æ®µè¯´æ˜

ç³»ç»Ÿæ–°å¢äº†å®Œæ•´è®ºæ–‡ç›¸å…³æ€§è¯„åˆ†åŠŸèƒ½ï¼Œç”¨äºç²¾ç¡®è¯„ä¼°è®ºæ–‡ä¸ç‰¹å®šä»»åŠ¡çš„ç›¸å…³æ€§ï¼š

- **full_paper_relevance_score**: å®Œæ•´è®ºæ–‡ç›¸å…³æ€§è¯„åˆ†
  - ç±»å‹ï¼šDECIMAL(5,3)ï¼Œå­˜å‚¨ 0.000-1.000 èŒƒå›´çš„è¯„åˆ†
  - ç”¨é€”ï¼šåŸºäºå®Œæ•´è®ºæ–‡å†…å®¹åˆ†æå¾—å‡ºçš„ç›¸å…³æ€§è¯„åˆ†
  - ä¼˜åŠ¿ï¼šæ¯”ä»…åŸºäºæ‘˜è¦çš„è¯„åˆ†æ›´åŠ å‡†ç¡®å’Œå…¨é¢
  - ç´¢å¼•ï¼šå·²ä¼˜åŒ–ï¼Œæ”¯æŒå¿«é€Ÿæ’åºå’ŒèŒƒå›´æŸ¥è¯¢

- **full_paper_relevance_justification**: å®Œæ•´è®ºæ–‡ç›¸å…³æ€§è¯„åˆ†ç†ç”±
  - ç±»å‹ï¼šTEXTï¼Œå­˜å‚¨è¯¦ç»†çš„è¯„åˆ†ç†ç”±è¯´æ˜
  - ç”¨é€”ï¼šè®°å½•ä¸ºä»€ä¹ˆç»™å‡ºè¯¥ç›¸å…³æ€§è¯„åˆ†çš„å…·ä½“åŸå› 
  - å†…å®¹ï¼šåŒ…å«è®ºæ–‡ç›¸å…³æ€§çš„è¯¦ç»†åˆ†æå’Œåˆ¤æ–­ä¾æ®
  - åº”ç”¨ï¼šå¸®åŠ©ç”¨æˆ·ç†è§£è¯„åˆ†ç»“æœï¼Œæé«˜ç³»ç»Ÿé€æ˜åº¦

**æ•°æ®å®Œæ•´æ€§**ï¼š
- ç°æœ‰æ•°æ®çš„ç›¸å…³æ€§å­—æ®µä¸º `NULL`ï¼ˆå†å²åŸå› ï¼‰
- æ–°å¤„ç†çš„è®ºæ–‡ä¼šè‡ªåŠ¨å¡«å…¥è¯„åˆ†å’Œç†ç”±
- è¯„åˆ†æ•°æ®å·²ä»åŸæœ‰ `metadata` å­—æ®µè¿ç§»åˆ°ä¸“é—¨å­—æ®µ
- æä¾›æ›´å¥½çš„æŸ¥è¯¢æ€§èƒ½å’Œæ•°æ®ç»“æ„åŒ–

### æ–°å¢ï¼šç»“æ„åŒ–è®ºæ–‡åˆ†æåŠŸèƒ½

ç³»ç»Ÿç°åœ¨æ”¯æŒè®ºæ–‡çš„æ™ºèƒ½ç»“æ„åŒ–åˆ†æï¼Œè‡ªåŠ¨æå–ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š

```python
# ç»“æ„åŒ–åˆ†æå­—æ®µ
structured_fields = {
    'research_background': 'ç ”ç©¶èƒŒæ™¯',      # ç ”ç©¶çš„èƒŒæ™¯å’ŒåŠ¨æœº
    'research_objectives': 'ç ”ç©¶ç›®æ ‡',      # å…·ä½“çš„ç ”ç©¶ç›®æ ‡å’Œé—®é¢˜
    'methods': 'ç ”ç©¶æ–¹æ³•',                   # ä½¿ç”¨çš„æ–¹æ³•å’ŒæŠ€æœ¯
    'key_findings': 'ä¸»è¦å‘ç°',              # é‡è¦çš„å‘ç°å’Œç»“æœ
    'conclusions': 'ç»“è®º',                   # å¾—å‡ºçš„ç»“è®ºå’Œè§è§£
    'limitations': 'å±€é™æ€§',                 # ç ”ç©¶çš„é™åˆ¶å’Œä¸è¶³
    'future_work': 'æœªæ¥å·¥ä½œ',               # åç»­ç ”ç©¶æ–¹å‘
    'keywords': 'å…³é”®è¯',                    # æ ¸å¿ƒå…³é”®è¯
    'full_paper_relevance_score': 'å®Œæ•´è®ºæ–‡ç›¸å…³æ€§è¯„åˆ†',        # 0.000-1.000 è¯„åˆ†
    'full_paper_relevance_justification': 'å®Œæ•´è®ºæ–‡ç›¸å…³æ€§ç†ç”±'  # è¯„åˆ†è¯¦ç»†è¯´æ˜
}

# ä½¿ç”¨ç¤ºä¾‹
arxiv_data = ArxivData(result)
arxiv_data.research_background = "æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨NLPé¢†åŸŸçš„åº”ç”¨èƒŒæ™¯"
arxiv_data.research_objectives = "æ¢ç´¢å’Œè¯„ä¼°æ·±åº¦å­¦ä¹ åœ¨NLPä»»åŠ¡ä¸­çš„æ•ˆæœ"
arxiv_data.methods = "ä½¿ç”¨Transformeræ¶æ„å’Œé¢„è®­ç»ƒæ¨¡å‹"
arxiv_data.key_findings = "åœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡"
arxiv_data.conclusions = "Transformeræ¶æ„åœ¨NLPé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯"
# æ–°å¢ï¼šå®Œæ•´è®ºæ–‡ç›¸å…³æ€§è¯„åˆ†
arxiv_data.full_paper_relevance_score = 0.85
arxiv_data.full_paper_relevance_justification = "è¯¥è®ºæ–‡ä¸NLPä»»åŠ¡é«˜åº¦ç›¸å…³ï¼Œå› ä¸ºå®ƒè¯¦ç»†æ¢è®¨äº†Transformeræ¶æ„åœ¨å¤šä¸ªNLPä»»åŠ¡ä¸­çš„åº”ç”¨æ•ˆæœï¼Œæä¾›äº†å…¨é¢çš„å®éªŒéªŒè¯å’Œæ·±å…¥çš„åˆ†æï¼Œå¯¹ç›¸å…³ç ”ç©¶å…·æœ‰é‡è¦å‚è€ƒä»·å€¼ã€‚"
```

## ğŸ’» åŸºç¡€ä½¿ç”¨

### 1. æ•°æ®åº“æ“ä½œç¤ºä¾‹

#### ç›´æ¥ SQL æ“ä½œï¼ˆæ¨èç”¨äºå­¦ä¹ ï¼‰

```python
import psycopg2
import psycopg2.extras
import json

# è¿æ¥æ•°æ®åº“
conn = psycopg2.connect(
    host='localhost',
    port=15432,
    database='homesystem',
    user='homesystem',
    password='homesystem123'
)
cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)

# æ’å…¥è®ºæ–‡æ•°æ®
paper_data = {
    'arxiv_id': '2024.12345',
    'title': 'Your Paper Title',
    'authors': 'Author Names',
    'abstract': 'Paper abstract...',
    'categories': 'cs.LG, cs.AI',
    'tags': json.dumps(['machine learning', 'AI']),
    'metadata': json.dumps({'citation_count': 0}),
    'full_paper_relevance_score': 0.78,
    'full_paper_relevance_justification': 'è¯¥è®ºæ–‡åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸå…·æœ‰è¾ƒé«˜ç›¸å…³æ€§ï¼Œæå‡ºçš„æ–¹æ³•å…·æœ‰åˆ›æ–°æ€§å’Œå®ç”¨æ€§ã€‚'
}

cursor.execute("""
    INSERT INTO arxiv_papers (arxiv_id, title, authors, abstract, categories, tags, metadata)
    VALUES (%(arxiv_id)s, %(title)s, %(authors)s, %(abstract)s, %(categories)s, %(tags)s, %(metadata)s)
    ON CONFLICT (arxiv_id) DO NOTHING
""", paper_data)

conn.commit()

# æŸ¥è¯¢è®ºæ–‡
cursor.execute("SELECT * FROM arxiv_papers WHERE arxiv_id = %s", ('2024.12345',))
paper = cursor.fetchone()
print(f"æ‰¾åˆ°è®ºæ–‡: {paper['title']}")

cursor.close()
conn.close()
```

#### ä½¿ç”¨ HomeSystem æ¨¡å‹ï¼ˆé«˜çº§ç”¨æ³•ï¼‰

```python
# æ³¨æ„ï¼šå¦‚æœé‡åˆ°å¯¼å…¥é—®é¢˜ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨ SQL æ“ä½œ
from HomeSystem.integrations.database import DatabaseOperations, ArxivPaperModel

# åˆ›å»ºæ•°æ®åº“æ“ä½œå®ä¾‹
db_ops = DatabaseOperations()

# åˆ›å»ºè®ºæ–‡è®°å½•
paper = ArxivPaperModel(
    arxiv_id="2024.12345",
    title="ç¤ºä¾‹è®ºæ–‡",
    authors="ä½œè€…å§“å",
    abstract="è®ºæ–‡æ‘˜è¦",
    categories="cs.LG",
    tags=["æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "],
    metadata={"conference": "ICML 2024"}
)

# ä¿å­˜åˆ°æ•°æ®åº“
success = db_ops.create(paper)
print(f"ä¿å­˜ç»“æœ: {success}")

# æŸ¥è¯¢è®ºæ–‡
existing_paper = db_ops.get_by_field(ArxivPaperModel, 'arxiv_id', "2024.12345")
if existing_paper:
    print(f"æ‰¾åˆ°è®ºæ–‡: {existing_paper.title}")
```

### 2. Redis ç¼“å­˜æ“ä½œ

```python
import redis

# è¿æ¥ Redis
redis_client = redis.Redis(
    host='localhost', 
    port=16379, 
    decode_responses=True
)

# åŸºç¡€é”®å€¼æ“ä½œ
redis_client.set("paper:status:2024.12345", "completed", ex=3600)  # 1å°æ—¶è¿‡æœŸ
status = redis_client.get("paper:status:2024.12345")

# é›†åˆæ“ä½œ - è·Ÿè¸ªå·²å¤„ç†è®ºæ–‡
redis_client.sadd("processed_papers", "2024.12345", "2024.12346")
is_processed = redis_client.sismember("processed_papers", "2024.12345")

# å“ˆå¸Œæ“ä½œ - ç¼“å­˜è®ºæ–‡å…ƒæ•°æ®
redis_client.hset("paper:meta:2024.12345", mapping={
    "title": "Paper Title",
    "citations": "156",
    "downloads": "2341"
})

meta = redis_client.hgetall("paper:meta:2024.12345")
print(f"è®ºæ–‡å¼•ç”¨æ•°: {meta.get('citations')}")
```

### 3. é«˜çº§æŸ¥è¯¢ç¤ºä¾‹

```python
# å…¨æ–‡æœç´¢ï¼ˆåŒ…å«ç»“æ„åŒ–å­—æ®µï¼‰
cursor.execute("""
    SELECT arxiv_id, title, research_objectives FROM arxiv_papers 
    WHERE title ILIKE %s OR abstract ILIKE %s OR research_objectives ILIKE %s
    LIMIT 10
""", ('%machine learning%', '%machine learning%', '%machine learning%'))

# åŸºäºå…³é”®è¯çš„æ™ºèƒ½æœç´¢
cursor.execute("""
    SELECT arxiv_id, title, keywords, research_objectives FROM arxiv_papers 
    WHERE keywords ILIKE %s OR research_objectives ILIKE %s
    ORDER BY created_at DESC
""", ('%æ·±åº¦å­¦ä¹ %', '%æ·±åº¦å­¦ä¹ %'))

# JSON æ ‡ç­¾æŸ¥è¯¢
cursor.execute("""
    SELECT arxiv_id, title, tags FROM arxiv_papers 
    WHERE tags @> %s
""", (json.dumps(['æ·±åº¦å­¦ä¹ ']),))

# æŒ‰åˆ†ç±»ç»Ÿè®¡ï¼ˆåŒ…å«ç»“æ„åŒ–åˆ†æï¼‰
cursor.execute("""
    SELECT categories, COUNT(*) as count,
           AVG(CAST(metadata->>'citation_count' AS INTEGER)) as avg_citations,
           COUNT(CASE WHEN research_objectives IS NOT NULL THEN 1 END) as structured_count
    FROM arxiv_papers 
    WHERE metadata->>'citation_count' IS NOT NULL
    GROUP BY categories 
    ORDER BY count DESC
""")

# ç»“æ„åŒ–åˆ†æå®Œæ•´æ€§ç»Ÿè®¡
cursor.execute("""
    SELECT 
        COUNT(*) as total_papers,
        COUNT(research_background) as has_background,
        COUNT(research_objectives) as has_objectives,
        COUNT(methods) as has_methods,
        COUNT(key_findings) as has_findings,
        COUNT(conclusions) as has_conclusions,
        COUNT(keywords) as has_keywords,
        COUNT(full_paper_relevance_score) as has_relevance_score,
        AVG(full_paper_relevance_score) as avg_relevance_score
    FROM arxiv_papers
""")

# åŸºäºå®Œæ•´è®ºæ–‡ç›¸å…³æ€§è¯„åˆ†çš„æŸ¥è¯¢
cursor.execute("""
    SELECT arxiv_id, title, full_paper_relevance_score,
           LEFT(full_paper_relevance_justification, 100) as justification_preview
    FROM arxiv_papers 
    WHERE full_paper_relevance_score >= 0.8
    ORDER BY full_paper_relevance_score DESC
    LIMIT 10
""")

# ç›¸å…³æ€§è¯„åˆ†åˆ†å¸ƒç»Ÿè®¡
cursor.execute("""
    SELECT 
        CASE 
            WHEN full_paper_relevance_score >= 0.9 THEN '0.9-1.0 (æé«˜)'
            WHEN full_paper_relevance_score >= 0.8 THEN '0.8-0.9 (é«˜)'
            WHEN full_paper_relevance_score >= 0.7 THEN '0.7-0.8 (ä¸­ç­‰)'
            WHEN full_paper_relevance_score >= 0.6 THEN '0.6-0.7 (ä½)'
            ELSE '0.0-0.6 (å¾ˆä½)'
        END as relevance_range,
        COUNT(*) as count
    FROM arxiv_papers 
    WHERE full_paper_relevance_score IS NOT NULL
    GROUP BY relevance_range
    ORDER BY MIN(full_paper_relevance_score) DESC
""")

# æ—¶é—´èŒƒå›´æŸ¥è¯¢
cursor.execute("""
    SELECT arxiv_id, title, created_at FROM arxiv_papers 
    WHERE created_at >= NOW() - INTERVAL '7 days'
    ORDER BY created_at DESC
""")
```

## ğŸ”§ ç®¡ç†å·¥å…·

### Web ç®¡ç†ç•Œé¢

å¯åŠ¨ Web ç®¡ç†å·¥å…·ï¼ˆå¯é€‰ï¼‰ï¼š

```bash
# å¯åŠ¨ç®¡ç†ç•Œé¢
docker compose --profile tools up -d

# è®¿é—®åœ°å€ï¼š
# pgAdmin: http://localhost:8080
# ç”¨æˆ·å: admin@homesystem.local
# å¯†ç : admin123

# Redis Commander: http://localhost:8081
```

### æ•°æ®åº“å¤‡ä»½ä¸æ¢å¤

```bash
# PostgreSQL å¤‡ä»½
docker exec homesystem-postgres pg_dump -U homesystem homesystem > backup_$(date +%Y%m%d).sql

# PostgreSQL æ¢å¤
docker exec -i homesystem-postgres psql -U homesystem homesystem < backup_$(date +%Y%m%d).sql

# Redis å¤‡ä»½
docker exec homesystem-redis redis-cli BGSAVE

# æŸ¥çœ‹ Redis å¤‡ä»½æ–‡ä»¶
docker exec homesystem-redis ls -la /data/
```

## ğŸ” ArXiv é›†æˆåŠŸèƒ½

### ç»“æ„åŒ–è®ºæ–‡åˆ†æå·¥ä½œæµ

```python
def analyze_paper_structure(arxiv_data):
    """å¯¹è®ºæ–‡è¿›è¡Œç»“æ„åŒ–åˆ†æ"""
    
    # è®¾ç½®ç»“æ„åŒ–åˆ†æå­—æ®µ
    structured_analysis = {
        'research_background': 'åˆ†æè®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº',
        'research_objectives': 'æå–å…·ä½“çš„ç ”ç©¶ç›®æ ‡å’Œè¦è§£å†³çš„é—®é¢˜',
        'methods': 'è¯†åˆ«ä½¿ç”¨çš„ç ”ç©¶æ–¹æ³•ã€ç®—æ³•æˆ–æŠ€æœ¯',
        'key_findings': 'æ€»ç»“é‡è¦çš„å‘ç°ã€ç»“æœæˆ–è´¡çŒ®',
        'conclusions': 'æ¦‚æ‹¬å¾—å‡ºçš„ç»“è®ºå’Œè§è§£',
        'limitations': 'è¯†åˆ«ç ”ç©¶çš„é™åˆ¶ã€ä¸è¶³æˆ–å±€é™æ€§',
        'future_work': 'æå–ä½œè€…æåˆ°çš„åç»­ç ”ç©¶æ–¹å‘',
        'keywords': 'æå–æ ¸å¿ƒå…³é”®è¯å’ŒæŠ€æœ¯æœ¯è¯­'
    }
    
    # å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›å­—æ®µå¯ä»¥é€šè¿‡LLMåˆ†æè®ºæ–‡å†…å®¹è‡ªåŠ¨å¡«å……
    for field, description in structured_analysis.items():
        if hasattr(arxiv_data, field):
            setattr(arxiv_data, field, f"åŸºäº{description}çš„åˆ†æç»“æœ")
    
    return arxiv_data

# ä½¿ç”¨ç¤ºä¾‹
arxiv_data = ArxivData(search_result)
structured_paper = analyze_paper_structure(arxiv_data)
print(f"ç»“æ„åŒ–åˆ†æå®Œæˆ: {structured_paper.has_structured_data()}")
```

### è®ºæ–‡è‡ªåŠ¨ç®¡ç†å·¥ä½œæµ

```python
# å®Œæ•´çš„è®ºæ–‡å¤„ç†å·¥ä½œæµç¤ºä¾‹
import psycopg2
import redis
import json

def arxiv_paper_workflow():
    """ArXiv è®ºæ–‡å¤„ç†å·¥ä½œæµ"""
    
    # æ•°æ®åº“è¿æ¥
    db_conn = psycopg2.connect(
        host='localhost', port=15432, database='homesystem',
        user='homesystem', password='homesystem123'
    )
    redis_client = redis.Redis(host='localhost', port=16379, decode_responses=True)
    
    cursor = db_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
    
    # 1. æ¨¡æ‹Ÿä» ArXiv API è·å–è®ºæ–‡æ•°æ®
    new_papers = [
        {
            'arxiv_id': '2024.01004',
            'title': 'Advances in Neural Network Architectures',
            'authors': 'Research Team',
            'abstract': 'This paper presents new neural network architectures...',
            'categories': 'cs.LG, cs.AI',
            'published_date': '2024-01-30',
            'pdf_url': 'https://arxiv.org/pdf/2024.01004.pdf'
        }
    ]
    
    # 2. æ‰¹é‡æ’å…¥æ–°è®ºæ–‡ï¼ˆå»é‡ï¼‰
    for paper in new_papers:
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        cursor.execute("SELECT arxiv_id FROM arxiv_papers WHERE arxiv_id = %s", (paper['arxiv_id'],))
        if not cursor.fetchone():
            cursor.execute("""
                INSERT INTO arxiv_papers (arxiv_id, title, authors, abstract, categories, published_date, pdf_url)
                VALUES (%(arxiv_id)s, %(title)s, %(authors)s, %(abstract)s, %(categories)s, %(published_date)s, %(pdf_url)s)
            """, paper)
            print(f"âœ… æ–°å¢è®ºæ–‡: {paper['title']}")
        else:
            print(f"âš ï¸  è®ºæ–‡å·²å­˜åœ¨: {paper['arxiv_id']}")
    
    db_conn.commit()
    
    # 3. è·å–å¾…å¤„ç†è®ºæ–‡
    cursor.execute("""
        SELECT arxiv_id, title FROM arxiv_papers 
        WHERE processing_status = 'pending'
        LIMIT 10
    """)
    
    pending_papers = cursor.fetchall()
    print(f"ğŸ“‹ æ‰¾åˆ° {len(pending_papers)} ç¯‡å¾…å¤„ç†è®ºæ–‡")
    
    # 4. å¤„ç†è®ºæ–‡å¹¶æ›´æ–°çŠ¶æ€
    for paper in pending_papers:
        arxiv_id = paper['arxiv_id']
        
        try:
            # æ¨¡æ‹Ÿè®ºæ–‡å¤„ç†ï¼ˆä¸‹è½½ã€åˆ†æç­‰ï¼‰
            print(f"ğŸ”„ æ­£åœ¨å¤„ç†: {paper['title'][:50]}...")
            
            # å¤„ç†å®Œæˆï¼Œæ›´æ–°çŠ¶æ€
            cursor.execute("""
                UPDATE arxiv_papers 
                SET processing_status = 'completed', 
                    updated_at = CURRENT_TIMESTAMP
                WHERE arxiv_id = %s
            """, (arxiv_id,))
            
            # æ·»åŠ åˆ° Redis å·²å¤„ç†é›†åˆ
            redis_client.sadd("processed_papers", arxiv_id)
            
            print(f"âœ… å¤„ç†å®Œæˆ: {arxiv_id}")
            
        except Exception as e:
            # å¤„ç†å¤±è´¥ï¼Œæ ‡è®°çŠ¶æ€
            cursor.execute("""
                UPDATE arxiv_papers 
                SET processing_status = 'failed', 
                    updated_at = CURRENT_TIMESTAMP
                WHERE arxiv_id = %s
            """, (arxiv_id,))
            print(f"âŒ å¤„ç†å¤±è´¥: {arxiv_id}, é”™è¯¯: {e}")
    
    db_conn.commit()
    
    # 5. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    cursor.execute("""
        SELECT 
            processing_status,
            COUNT(*) as count
        FROM arxiv_papers 
        GROUP BY processing_status
    """)
    
    stats = cursor.fetchall()
    print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
    for stat in stats:
        print(f"   {stat['processing_status']}: {stat['count']} ç¯‡")
    
    cursor.close()
    db_conn.close()

# è¿è¡Œå·¥ä½œæµ
if __name__ == "__main__":
    arxiv_paper_workflow()
```

### æ™ºèƒ½å»é‡æœºåˆ¶

```python
def check_duplicate_papers():
    """æ£€æŸ¥é‡å¤è®ºæ–‡çš„å¤šç§ç­–ç•¥"""
    
    conn = psycopg2.connect(
        host='localhost', port=15432, database='homesystem',
        user='homesystem', password='homesystem123'
    )
    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
    
    # 1. åŸºäº arxiv_id çš„ç²¾ç¡®å»é‡ï¼ˆå·²é€šè¿‡æ•°æ®åº“çº¦æŸå®ç°ï¼‰
    print("ğŸ” åŸºäº ArXiv ID çš„å»é‡å·²é€šè¿‡æ•°æ®åº“å”¯ä¸€çº¦æŸå®ç°")
    
    # 2. åŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦çš„æ¨¡ç³Šå»é‡
    cursor.execute("""
        WITH similarity_check AS (
            SELECT 
                a1.arxiv_id as id1,
                a2.arxiv_id as id2,
                a1.title as title1,
                a2.title as title2,
                similarity(a1.title, a2.title) as sim_score
            FROM arxiv_papers a1
            JOIN arxiv_papers a2 ON a1.id < a2.id
            WHERE similarity(a1.title, a2.title) > 0.8
        )
        SELECT * FROM similarity_check ORDER BY sim_score DESC
    """)
    
    similar_papers = cursor.fetchall()
    if similar_papers:
        print(f"âš ï¸  å‘ç° {len(similar_papers)} å¯¹å¯èƒ½é‡å¤çš„è®ºæ–‡:")
        for paper in similar_papers[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   ç›¸ä¼¼åº¦ {paper['sim_score']:.3f}: {paper['id1']} vs {paper['id2']}")
    
    # 3. åŸºäºä½œè€…å’Œå‘å¸ƒæ—¶é—´çš„å»é‡æ£€æŸ¥
    cursor.execute("""
        SELECT authors, published_date, COUNT(*) as count
        FROM arxiv_papers 
        WHERE authors IS NOT NULL AND authors != ''
        GROUP BY authors, published_date
        HAVING COUNT(*) > 1
    """)
    
    author_duplicates = cursor.fetchall()
    if author_duplicates:
        print(f"âš ï¸  å‘ç° {len(author_duplicates)} ç»„ç›¸åŒä½œè€…åŒæ—¥å‘å¸ƒçš„è®ºæ–‡")
    
    cursor.close()
    conn.close()
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### 1. æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–

ç³»ç»Ÿå·²åˆ›å»ºçš„ç´¢å¼•ï¼š

```sql
-- ä¸»è¦ç´¢å¼•ï¼ˆå·²å­˜åœ¨ï¼‰
CREATE INDEX IF NOT EXISTS idx_arxiv_papers_arxiv_id ON arxiv_papers(arxiv_id);
CREATE INDEX IF NOT EXISTS idx_arxiv_papers_status ON arxiv_papers(processing_status);
CREATE INDEX IF NOT EXISTS idx_arxiv_papers_categories ON arxiv_papers(categories);
CREATE INDEX IF NOT EXISTS idx_arxiv_papers_created_at ON arxiv_papers(created_at);

-- å¯é€‰çš„æ€§èƒ½ä¼˜åŒ–ç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_arxiv_papers_published_date ON arxiv_papers(published_date);
CREATE INDEX IF NOT EXISTS idx_arxiv_papers_status_created ON arxiv_papers(processing_status, created_at);

-- ç»“æ„åŒ–åˆ†æå­—æ®µç´¢å¼•ï¼ˆæ–°å¢ï¼‰
CREATE INDEX IF NOT EXISTS idx_arxiv_papers_keywords ON arxiv_papers(keywords);
CREATE INDEX IF NOT EXISTS idx_arxiv_papers_research_objectives ON arxiv_papers(research_objectives);

-- å…¨æ–‡æœç´¢ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
CREATE INDEX IF NOT EXISTS idx_arxiv_papers_title_fts ON arxiv_papers USING gin(to_tsvector('english', title));
CREATE INDEX IF NOT EXISTS idx_arxiv_papers_abstract_fts ON arxiv_papers USING gin(to_tsvector('english', abstract));
```

### 2. æŸ¥è¯¢ä¼˜åŒ–å»ºè®®

```python
# âœ… å¥½çš„æŸ¥è¯¢å®è·µ
def optimized_queries():
    """ä¼˜åŒ–çš„æŸ¥è¯¢ç¤ºä¾‹"""
    
    # 1. ä½¿ç”¨ç´¢å¼•å­—æ®µè¿›è¡ŒæŸ¥è¯¢
    cursor.execute("""
        SELECT arxiv_id, title FROM arxiv_papers 
        WHERE processing_status = 'pending'  -- ä½¿ç”¨ç´¢å¼•
        ORDER BY created_at DESC             -- ä½¿ç”¨ç´¢å¼•
        LIMIT 100
    """)
    
    # 2. é¿å… SELECT *ï¼ŒåªæŸ¥è¯¢éœ€è¦çš„å­—æ®µ
    cursor.execute("""
        SELECT arxiv_id, title, authors FROM arxiv_papers 
        WHERE categories LIKE 'cs.LG%'
    """)
    
    # 3. ä½¿ç”¨ EXPLAIN åˆ†ææŸ¥è¯¢è®¡åˆ’
    cursor.execute("EXPLAIN ANALYZE SELECT * FROM arxiv_papers WHERE arxiv_id = '2024.01001'")
    plan = cursor.fetchall()
    print("æŸ¥è¯¢è®¡åˆ’:", plan)

# âŒ é¿å…çš„æŸ¥è¯¢æ¨¡å¼
def slow_queries():
    """åº”è¯¥é¿å…çš„æ…¢æŸ¥è¯¢"""
    
    # 1. é¿å…åœ¨éç´¢å¼•å­—æ®µä¸Šä½¿ç”¨ LIKE
    # cursor.execute("SELECT * FROM arxiv_papers WHERE abstract LIKE '%machine learning%'")
    
    # 2. é¿å…ä¸å¿…è¦çš„ ORDER BY
    # cursor.execute("SELECT * FROM arxiv_papers ORDER BY abstract")
    
    # 3. é¿å…åœ¨å¤§è¡¨ä¸Šä½¿ç”¨ COUNT(*) æ— æ¡ä»¶ç»Ÿè®¡
    # cursor.execute("SELECT COUNT(*) FROM arxiv_papers")
```

### 3. Redis ç¼“å­˜ç­–ç•¥

```python
import redis
import json
import time
from functools import wraps

redis_client = redis.Redis(host='localhost', port=16379, decode_responses=True)

def cache_result(expire_time=600):
    """Redis ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"cache:{func.__name__}:{hash(str(args) + str(sorted(kwargs.items())))}"
            
            # æ£€æŸ¥ç¼“å­˜
            cached_result = redis_client.get(cache_key)
            if cached_result:
                return json.loads(cached_result)
            
            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = func(*args, **kwargs)
            redis_client.setex(cache_key, expire_time, json.dumps(result, default=str))
            
            return result
        return wrapper
    return decorator

@cache_result(expire_time=1800)  # 30åˆ†é’Ÿç¼“å­˜
def get_popular_categories():
    """è·å–çƒ­é—¨åˆ†ç±»ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    cursor.execute("""
        SELECT categories, COUNT(*) as count
        FROM arxiv_papers 
        GROUP BY categories
        ORDER BY count DESC
        LIMIT 10
    """)
    return cursor.fetchall()
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. æ•°æ®åº“è¿æ¥å¤±è´¥

**é—®é¢˜ç°è±¡**ï¼š
```
psycopg2.OperationalError: could not connect to server
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker compose ps

# æ£€æŸ¥ç«¯å£å ç”¨
netstat -an | grep 15432

# é‡å¯æ•°æ®åº“æœåŠ¡
docker compose restart postgres

# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
docker compose logs postgres
```

#### 2. Redis è¿æ¥é—®é¢˜

**é—®é¢˜ç°è±¡**ï¼š
```
redis.exceptions.ConnectionError: Error connecting to Redis
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ£€æŸ¥ Redis æœåŠ¡
docker exec homesystem-redis redis-cli ping

# æ£€æŸ¥ Redis é…ç½®
docker exec homesystem-redis redis-cli CONFIG GET "*"

# é‡å¯ Redis æœåŠ¡
docker compose restart redis
```

#### 3. è¡¨ä¸å­˜åœ¨é”™è¯¯

**é—®é¢˜ç°è±¡**ï¼š
```
psycopg2.errors.UndefinedTable: relation "arxiv_papers" does not exist
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ‰‹åŠ¨åˆ›å»ºè¡¨ç»“æ„
docker exec homesystem-postgres psql -U homesystem -d homesystem -c "
CREATE TABLE IF NOT EXISTS arxiv_papers (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    arxiv_id VARCHAR(50) UNIQUE NOT NULL,
    title TEXT NOT NULL,
    -- ... å…¶ä»–å­—æ®µ
);
"

# æˆ–è¿è¡Œåˆå§‹åŒ–è„šæœ¬
python -c "
import psycopg2
# æ‰§è¡Œè¡¨åˆ›å»º SQL
"
```

#### 4. æƒé™é—®é¢˜

**é—®é¢˜ç°è±¡**ï¼š
```
permission denied for table arxiv_papers
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ£€æŸ¥ç”¨æˆ·æƒé™
docker exec homesystem-postgres psql -U homesystem -d homesystem -c "\dp arxiv_papers"

# æˆæƒï¼ˆå¦‚æœéœ€è¦ï¼‰
docker exec homesystem-postgres psql -U homesystem -d homesystem -c "GRANT ALL ON arxiv_papers TO homesystem;"
```

### æ€§èƒ½è°ƒä¼˜

#### ç›‘æ§æŸ¥è¯¢æ€§èƒ½

```python
import time
import psycopg2

def monitor_query_performance(query, params=None):
    """æŸ¥è¯¢æ€§èƒ½ç›‘æ§"""
    conn = psycopg2.connect(
        host='localhost', port=15432, database='homesystem',
        user='homesystem', password='homesystem123'
    )
    cursor = conn.cursor()
    
    start_time = time.time()
    cursor.execute(query, params)
    results = cursor.fetchall()
    end_time = time.time()
    
    execution_time = end_time - start_time
    print(f"æŸ¥è¯¢æ‰§è¡Œæ—¶é—´: {execution_time:.3f} ç§’")
    print(f"è¿”å›è®°å½•æ•°: {len(results)}")
    
    # åˆ†ææŸ¥è¯¢è®¡åˆ’
    explain_query = f"EXPLAIN ANALYZE {query}"
    cursor.execute(explain_query, params)
    plan = cursor.fetchall()
    
    print("æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’:")
    for row in plan:
        print(f"  {row[0]}")
    
    cursor.close()
    conn.close()
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
monitor_query_performance("""
    SELECT arxiv_id, title FROM arxiv_papers 
    WHERE processing_status = %s 
    ORDER BY created_at DESC 
    LIMIT 100
""", ('pending',))
```

## ğŸ“ˆ æ‰©å±•å¼€å‘

### 1. æ·»åŠ è‡ªå®šä¹‰æ•°æ®æ¨¡å‹

```python
# æ‰©å±•ç”¨æˆ·ç®¡ç†åŠŸèƒ½
def create_user_table():
    """åˆ›å»ºç”¨æˆ·è¡¨"""
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS users (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        username VARCHAR(100) UNIQUE NOT NULL,
        email VARCHAR(255) UNIQUE NOT NULL,
        preferences JSONB DEFAULT '{}',
        favorite_papers JSONB DEFAULT '[]',
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    
    CREATE INDEX IF NOT EXISTS idx_users_username ON users(username);
    CREATE INDEX IF NOT EXISTS idx_users_email ON users(email);
    
    -- åˆ›å»ºç”¨æˆ·-è®ºæ–‡æ”¶è—å…³ç³»è¡¨
    CREATE TABLE IF NOT EXISTS user_favorite_papers (
        user_id UUID REFERENCES users(id) ON DELETE CASCADE,
        paper_id UUID REFERENCES arxiv_papers(id) ON DELETE CASCADE,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        PRIMARY KEY (user_id, paper_id)
    );
    """
    
    conn = psycopg2.connect(
        host='localhost', port=15432, database='homesystem',
        user='homesystem', password='homesystem123'
    )
    cursor = conn.cursor()
    cursor.execute(create_table_sql)
    conn.commit()
    cursor.close()
    conn.close()
    
    print("âœ… ç”¨æˆ·è¡¨åˆ›å»ºå®Œæˆ")
```

### 2. å®ç°è®ºæ–‡æ¨èç³»ç»Ÿ

```python
def recommend_papers(user_id, limit=10):
    """åŸºäºç”¨æˆ·åå¥½æ¨èè®ºæ–‡"""
    conn = psycopg2.connect(
        host='localhost', port=15432, database='homesystem',
        user='homesystem', password='homesystem123'
    )
    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
    
    # 1. è·å–ç”¨æˆ·åå¥½åˆ†ç±»
    cursor.execute("""
        SELECT DISTINCT ap.categories
        FROM user_favorite_papers ufp
        JOIN arxiv_papers ap ON ufp.paper_id = ap.id
        WHERE ufp.user_id = %s
    """, (user_id,))
    
    user_categories = [row['categories'] for row in cursor.fetchall()]
    
    if not user_categories:
        # ç”¨æˆ·æ²¡æœ‰æ”¶è—ï¼Œæ¨èçƒ­é—¨è®ºæ–‡
        cursor.execute("""
            SELECT arxiv_id, title, categories,
                   CAST(metadata->>'citation_count' AS INTEGER) as citations
            FROM arxiv_papers 
            WHERE metadata->>'citation_count' IS NOT NULL
            ORDER BY CAST(metadata->>'citation_count' AS INTEGER) DESC
            LIMIT %s
        """, (limit,))
    else:
        # åŸºäºç”¨æˆ·åå¥½åˆ†ç±»æ¨è
        category_patterns = [f"%{cat}%" for cat in user_categories]
        placeholders = ','.join(['%s'] * len(category_patterns))
        
        cursor.execute(f"""
            SELECT arxiv_id, title, categories,
                   CAST(metadata->>'citation_count' AS INTEGER) as citations
            FROM arxiv_papers 
            WHERE categories SIMILAR TO '({"|".join(category_patterns)})'
            AND id NOT IN (
                SELECT paper_id FROM user_favorite_papers WHERE user_id = %s
            )
            ORDER BY created_at DESC
            LIMIT %s
        """, [user_id] + [limit])
    
    recommendations = cursor.fetchall()
    cursor.close()
    conn.close()
    
    return recommendations
```

### 3. å®ç°æ•°æ®åˆ†æAPI

```python
def generate_analytics_report():
    """ç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Š"""
    conn = psycopg2.connect(
        host='localhost', port=15432, database='homesystem',
        user='homesystem', password='homesystem123'
    )
    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
    
    report = {}
    
    # 1. åŸºç¡€ç»Ÿè®¡
    cursor.execute("""
        SELECT 
            COUNT(*) as total_papers,
            COUNT(CASE WHEN processing_status = 'completed' THEN 1 END) as completed,
            COUNT(CASE WHEN processing_status = 'pending' THEN 1 END) as pending,
            COUNT(CASE WHEN processing_status = 'failed' THEN 1 END) as failed
        FROM arxiv_papers
    """)
    report['basic_stats'] = cursor.fetchone()
    
    # 2. æœˆåº¦å¢é•¿è¶‹åŠ¿
    cursor.execute("""
        SELECT 
            DATE_TRUNC('month', created_at) as month,
            COUNT(*) as paper_count
        FROM arxiv_papers 
        WHERE created_at >= NOW() - INTERVAL '12 months'
        GROUP BY DATE_TRUNC('month', created_at)
        ORDER BY month
    """)
    report['monthly_trend'] = cursor.fetchall()
    
    # 3. çƒ­é—¨åˆ†ç±»æ’è¡Œ
    cursor.execute("""
        SELECT 
            categories,
            COUNT(*) as count,
            AVG(CAST(metadata->>'citation_count' AS INTEGER)) as avg_citations
        FROM arxiv_papers 
        WHERE metadata->>'citation_count' IS NOT NULL
        GROUP BY categories
        ORDER BY count DESC
        LIMIT 20
    """)
    report['popular_categories'] = cursor.fetchall()
    
    # 4. é«˜å¼•ç”¨è®ºæ–‡
    cursor.execute("""
        SELECT 
            arxiv_id, title, authors,
            CAST(metadata->>'citation_count' AS INTEGER) as citations
        FROM arxiv_papers 
        WHERE metadata->>'citation_count' IS NOT NULL
        ORDER BY CAST(metadata->>'citation_count' AS INTEGER) DESC
        LIMIT 10
    """)
    report['top_cited_papers'] = cursor.fetchall()
    
    cursor.close()
    conn.close()
    
    return report

# ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
def save_analytics_report():
    """ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶"""
    report = generate_analytics_report()
    
    import json
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"analytics_report_{timestamp}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"ğŸ“Š åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
    return filename
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ•°æ®å®Œæ•´æ€§

```python
def ensure_data_integrity():
    """ç¡®ä¿æ•°æ®å®Œæ•´æ€§çš„æ£€æŸ¥"""
    conn = psycopg2.connect(
        host='localhost', port=15432, database='homesystem',
        user='homesystem', password='homesystem123'
    )
    cursor = conn.cursor()
    
    integrity_checks = []
    
    # æ£€æŸ¥å¿…å¡«å­—æ®µ
    cursor.execute("""
        SELECT COUNT(*) FROM arxiv_papers 
        WHERE arxiv_id IS NULL OR arxiv_id = '' OR title IS NULL OR title = ''
    """)
    missing_required = cursor.fetchone()[0]
    integrity_checks.append(f"ç¼ºå¤±å¿…å¡«å­—æ®µçš„è®°å½•: {missing_required}")
    
    # æ£€æŸ¥é‡å¤è®°å½•
    cursor.execute("""
        SELECT arxiv_id, COUNT(*) FROM arxiv_papers 
        GROUP BY arxiv_id HAVING COUNT(*) > 1
    """)
    duplicates = cursor.fetchall()
    integrity_checks.append(f"é‡å¤çš„ arxiv_id: {len(duplicates)}")
    
    # æ£€æŸ¥å¼‚å¸¸çŠ¶æ€
    cursor.execute("""
        SELECT COUNT(*) FROM arxiv_papers 
        WHERE processing_status NOT IN ('pending', 'completed', 'failed')
    """)
    invalid_status = cursor.fetchone()[0]
    integrity_checks.append(f"æ— æ•ˆçŠ¶æ€çš„è®°å½•: {invalid_status}")
    
    cursor.close()
    conn.close()
    
    print("ğŸ” æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ç»“æœ:")
    for check in integrity_checks:
        print(f"  - {check}")
    
    return integrity_checks
```

### 2. è‡ªåŠ¨åŒ–ç»´æŠ¤ä»»åŠ¡

```python
import schedule
import time
from datetime import datetime, timedelta

def cleanup_old_data():
    """æ¸…ç†æ—§æ•°æ®"""
    conn = psycopg2.connect(
        host='localhost', port=15432, database='homesystem',
        user='homesystem', password='homesystem123'
    )
    cursor = conn.cursor()
    
    # åˆ é™¤30å¤©å‰çš„å¤±è´¥è®°å½•
    cursor.execute("""
        DELETE FROM arxiv_papers 
        WHERE processing_status = 'failed' 
        AND created_at < NOW() - INTERVAL '30 days'
    """)
    
    deleted_count = cursor.rowcount
    conn.commit()
    cursor.close()
    conn.close()
    
    print(f"ğŸ§¹ æ¸…ç†äº† {deleted_count} æ¡è¿‡æœŸçš„å¤±è´¥è®°å½•")

def backup_database():
    """è‡ªåŠ¨å¤‡ä»½æ•°æ®åº“"""
    import subprocess
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = f"backup_homesystem_{timestamp}.sql"
    
    try:
        subprocess.run([
            'docker', 'exec', 'homesystem-postgres',
            'pg_dump', '-U', 'homesystem', 'homesystem'
        ], stdout=open(backup_file, 'w'), check=True)
        
        print(f"ğŸ’¾ æ•°æ®åº“å¤‡ä»½å®Œæˆ: {backup_file}")
    except subprocess.CalledProcessError as e:
        print(f"âŒ å¤‡ä»½å¤±è´¥: {e}")

# è®¾ç½®å®šæ—¶ä»»åŠ¡
def setup_maintenance_schedule():
    """è®¾ç½®ç»´æŠ¤è®¡åˆ’"""
    schedule.every().day.at("02:00").do(cleanup_old_data)
    schedule.every().day.at("03:00").do(backup_database)
    schedule.every().hour.do(ensure_data_integrity)
    
    print("â° ç»´æŠ¤è®¡åˆ’å·²è®¾ç½®:")
    print("  - æ¯æ—¥ 02:00: æ¸…ç†æ—§æ•°æ®")
    print("  - æ¯æ—¥ 03:00: å¤‡ä»½æ•°æ®åº“")
    print("  - æ¯å°æ—¶: æ•°æ®å®Œæ•´æ€§æ£€æŸ¥")
    
    # è¿è¡Œè°ƒåº¦å™¨
    while True:
        schedule.run_pending()
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
```

## ğŸ“š æ€»ç»“

Home System æ•°æ®åº“é›†æˆæä¾›äº†å®Œæ•´çš„ ArXiv è®ºæ–‡ç®¡ç†è§£å†³æ–¹æ¡ˆï¼Œå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒä¼˜åŠ¿ï¼š

### âœ… æ ¸å¿ƒåŠŸèƒ½
- **åŒæ•°æ®åº“æ¶æ„**: PostgreSQL + Redis é«˜æ€§èƒ½ç»„åˆ
- **æ™ºèƒ½å»é‡**: åŸºäº arxiv_id çš„ç²¾ç¡®å»é‡æœºåˆ¶
- **ç»“æ„åŒ–åˆ†æ**: è®ºæ–‡çš„æ™ºèƒ½æ‘˜è¦å’Œå…³é”®ä¿¡æ¯æå–
- **çŠ¶æ€ç®¡ç†**: å®Œæ•´çš„è®ºæ–‡å¤„ç†çŠ¶æ€è·Ÿè¸ª
- **é«˜æ€§èƒ½æŸ¥è¯¢**: ä¼˜åŒ–çš„ç´¢å¼•å’ŒæŸ¥è¯¢ç­–ç•¥
- **å®¹å™¨åŒ–éƒ¨ç½²**: Docker Compose ä¸€é”®éƒ¨ç½²
- **é›†æˆæµ‹è¯•**: å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ä¿è¯åŠŸèƒ½ç¨³å®š

### ğŸš€ æŠ€æœ¯ç‰¹æ€§
- **è¿æ¥æ± ç®¡ç†**: é«˜æ•ˆçš„æ•°æ®åº“è¿æ¥å¤ç”¨
- **äº‹åŠ¡æ”¯æŒ**: è‡ªåŠ¨äº‹åŠ¡ç®¡ç†å’Œå›æ»š
- **ç¼“å­˜ç­–ç•¥**: Redis å¤šå±‚ç¼“å­˜ä¼˜åŒ–
- **æ‰¹é‡æ“ä½œ**: é«˜æ•ˆçš„æ‰¹é‡æ•°æ®å¤„ç†
- **ç»“æ„åŒ–å­˜å‚¨**: 8ä¸ªä¸“ç”¨å­—æ®µå­˜å‚¨è®ºæ–‡åˆ†æç»“æœ
- **æ™ºèƒ½ç´¢å¼•**: é’ˆå¯¹ç»“æ„åŒ–å­—æ®µçš„æŸ¥è¯¢ä¼˜åŒ–
- **ç›‘æ§æŒ‡æ ‡**: å®Œæ•´çš„æ€§èƒ½ç›‘æ§ä½“ç³»

### ğŸ“ˆ æ‰©å±•èƒ½åŠ›
- **æ¨¡å—åŒ–è®¾è®¡**: æ˜“äºæ‰©å±•æ–°åŠŸèƒ½
- **API å‹å¥½**: æ”¯æŒ REST API é›†æˆ
- **æ™ºèƒ½åˆ†æ**: åŸºäºLLMçš„è®ºæ–‡å†…å®¹åˆ†æ
- **å¤šç»´æŸ¥è¯¢**: æ”¯æŒæ ‡é¢˜ã€æ‘˜è¦ã€ç»“æ„åŒ–å­—æ®µçš„ç»¼åˆæœç´¢
- **ç”¨æˆ·ç³»ç»Ÿ**: æ”¯æŒå¤šç”¨æˆ·å’Œæƒé™ç®¡ç†
- **æ¨èç®—æ³•**: åŸºäºç»“æ„åŒ–åˆ†æçš„æ™ºèƒ½è®ºæ–‡æ¨è
- **è¶‹åŠ¿åˆ†æ**: åŸºäºå…³é”®è¯å’Œç ”ç©¶ç›®æ ‡çš„è¶‹åŠ¿è¯†åˆ«

### ğŸ¯ ä½¿ç”¨å»ºè®®

1. **ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²**: 
   - ä½¿ç”¨ä¸“ç”¨çš„æ•°æ®åº“æœåŠ¡å™¨
   - é…ç½®æ•°æ®åº“å¤‡ä»½ç­–ç•¥
   - è®¾ç½®ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ

2. **æ€§èƒ½ä¼˜åŒ–**:
   - å®šæœŸåˆ†ææ…¢æŸ¥è¯¢å¹¶ä¼˜åŒ–
   - åˆç†ä½¿ç”¨ Redis ç¼“å­˜
   - ç›‘æ§æ•°æ®åº“è¿æ¥æ± çŠ¶æ€

3. **å®‰å…¨è€ƒè™‘**:
   - ä½¿ç”¨å¼ºå¯†ç å’ŒåŠ å¯†è¿æ¥
   - å®šæœŸæ›´æ–°æ•°æ®åº“è½¯ä»¶
   - é™åˆ¶æ•°æ®åº“è®¿é—®æƒé™

4. **æ•°æ®ç®¡ç†**:
   - å®šæœŸæ¸…ç†è¿‡æœŸæ•°æ®
   - å®æ–½æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
   - å»ºç«‹æ•°æ®æ¢å¤æµç¨‹

é€šè¿‡æœ¬æŒ‡å—ï¼Œæ‚¨å·²ç»æŒæ¡äº† Home System æ•°æ®åº“é›†æˆçš„å®Œæ•´ä½¿ç”¨æ–¹æ³•ã€‚ç³»ç»Ÿç°å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹æ‚¨çš„ ArXiv è®ºæ–‡ç®¡ç†é¡¹ç›®å¼€å‘ï¼

## ğŸ”— ç›¸å…³èµ„æº

- **é›†æˆæµ‹è¯•**: `test_arxiv_database_integration.py` - å®Œæ•´çš„é›†æˆæµ‹è¯•å¥—ä»¶
- **ç¤ºä¾‹ä»£ç **: `simple_arxiv_demo.py` - å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹
- **Docker é…ç½®**: `docker-compose.yml` - å®¹å™¨ç¼–æ’é…ç½®
- **æ•°æ®åº“æ¶æ„**: æœ¬æ–‡æ¡£ç¬¬4èŠ‚ - è¯¦ç»†çš„è¡¨ç»“æ„è¯´æ˜ï¼ˆåŒ…å«ç»“æ„åŒ–å­—æ®µï¼‰
- **æ€§èƒ½ä¼˜åŒ–**: æœ¬æ–‡æ¡£ç¬¬7èŠ‚ - æ€§èƒ½è°ƒä¼˜æŒ‡å—
- **æ‰©å±•å¼€å‘**: æœ¬æ–‡æ¡£ç¬¬9èŠ‚ - è‡ªå®šä¹‰å¼€å‘æŒ‡å—
- **ç»“æ„åŒ–åˆ†æ**: æœ¬æ–‡æ¡£ç¬¬5èŠ‚ - æ™ºèƒ½è®ºæ–‡åˆ†æåŠŸèƒ½

---

ğŸ“ **æ–‡æ¡£ç‰ˆæœ¬**: v2.1 | **æ›´æ–°æ—¶é—´**: 2025-07-28 | **é€‚ç”¨ç‰ˆæœ¬**: Home System v1.0+ | **æ–°å¢**: ç»“æ„åŒ–è®ºæ–‡åˆ†æåŠŸèƒ½